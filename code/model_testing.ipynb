{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c9f5ad59-0355-4cba-8ead-a36a4ac8f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "db3f182c-2e66-442d-a977-26c91f321624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path=\"C:/Users/benjc/Documents/models/qwen2-0_5b-instruct-fp16.gguf\"\n",
    "model_path = \"C:/Users/benjc/Documents/models/Qwen2-7B-Instruct-Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e46c615f-de63-44a8-bc79-cb327acc65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Given the sentence, generate at least twenty paraphrased versions that maintain the original semantic meaning and style. \n",
    "Return the rephrased sentences in a Python list format, ensuring the output starts with '[' and ends with ']'. \n",
    "Include no additional text or notes in the output.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\"sentence\": \"DBC Pierre, Booker Prize-winner and author of Vernon God Little, has moved to Ireland.\",\n",
    "     \"rephrased_sentences\": [\"Booker Prize-winner DBC Pierre, known for writing Vernon God Little, has moved to Ireland.\", \n",
    "                             \"The author of Vernon God Little and Booker Prize-winner, DBC Pierre, has settled in Ireland.\", \n",
    "                             \"DBC Pierre, who won the Booker Prize for Vernon God Little, has now moved to Ireland.\", \n",
    "                             \"DBC Pierre, celebrated for his Booker Prize-winning novel Vernon God Little, has moved to Ireland.\", \n",
    "                             \"Winner of the Booker Prize and author of Vernon God Little, DBC Pierre, has relocated to Ireland.\", \n",
    "                             \"The Booker Prize-winning author of Vernon God Little, DBC Pierre, has taken up residence in Ireland.\", \n",
    "                             \"DBC Pierre, the acclaimed Booker Prize-winner for Vernon God Little, has moved to Ireland.\", \n",
    "                             \"Booker Prize laureate and Vernon God Little's author, DBC Pierre, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, the author of the Booker Prize-winning novel Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, recognized for his Booker Prize win with Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, famous for Vernon God Little and a Booker Prize-winner, has moved to Ireland.\", \n",
    "                             \"Booker Prize-winning author DBC Pierre, known for Vernon God Little, has now moved to Ireland.\", \n",
    "                             \"DBC Pierre, who won the Booker Prize for his novel Vernon God Little, has relocated to Ireland.\", \n",
    "                             \"The author DBC Pierre, who won the Booker Prize for Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, who penned the Booker Prize-winning Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, a Booker Prize winner for Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, recognized for his Booker Prize-winning book Vernon God Little, has moved to Ireland.\", \n",
    "                             \"Booker Prize-winning author DBC Pierre, famous for Vernon God Little, has moved to Ireland.\", \n",
    "                             \"DBC Pierre, the Booker Prize-winning novelist of Vernon God Little, has moved to Ireland.\"]\n",
    "    },\n",
    "    {\"sentence\": \"Known for being very delicate, the skill could take a lifetime to master.\",\n",
    "     \"rephrased_sentences\": [\"Famous for being delicate, this skill might require a lifetime to master.\", \n",
    "                             \"This skill, known for its delicacy, could take a lifetime to master.\", \n",
    "                             \"The skill's reputation for delicacy means it could take a lifetime to master.\", \n",
    "                             \"Mastering this skill, which is known for its delicacy, might take a lifetime.\", \n",
    "                             \"The delicacy of this skill suggests it could take a lifetime to master.\", \n",
    "                             \"Renowned for its delicateness, the skill might take a lifetime to master.\", \n",
    "                             \"Due to its delicate nature, the skill could take a lifetime to master.\", \n",
    "                             \"The skill, famed for its delicacy, might require a lifetime to master.\", \n",
    "                             \"Known for its delicate nature, this skill could take a lifetime to master.\", \n",
    "                             \"Because of its delicateness, the skill could take a lifetime to master.\", \n",
    "                             \"The skill, recognized for its delicacy, could take a lifetime to master.\", \n",
    "                             \"This skill, due to its delicate nature, might take a lifetime to master.\", \n",
    "                             \"The skill, famous for being very delicate, could take a lifetime to master.\", \n",
    "                             \"The delicate nature of the skill means it might take a lifetime to master.\", \n",
    "                             \"Given its delicacy, the skill could take a lifetime to master.\", \n",
    "                             \"The skill's delicate nature suggests it could take a lifetime to master.\", \n",
    "                             \"Known for its very delicate nature, the skill could take a lifetime to master.\", \n",
    "                             \"Mastering this delicate skill could require a lifetime.\", \n",
    "                             \"This skill, noted for its delicacy, might take a lifetime to master.\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{sentence}\"),\n",
    "        (\"ai\", \"{rephrased_sentences}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{sentence}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0ca7a843-9dc2-4bd5-b924-a8f0703d8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 339 tensors from C:/Users/benjc/Documents/models/Qwen2-7B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen2-7B-Instruct\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  21:                      quantize.imatrix.file str              = /models/Qwen2-7B-Instruct-GGUF/Qwen2-...\n",
      "llama_model_loader: - kv  22:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n",
      "llama_model_loader: - kv  23:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  24:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 421/152064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2-7B-Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.16 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4460.45 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 1\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 875\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'qwen2.attention.head_count': '28', 'general.name': 'Qwen2-7B-Instruct', 'general.architecture': 'qwen2', 'qwen2.block_count': '28', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '4', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'qwen2.embedding_length': '3584', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '18944', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models/Qwen2-7B-Instruct-GGUF/Qwen2-7B-Instruct.imatrix', 'quantize.imatrix.entries_count': '196'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=12, # Maximum I have is 12\n",
    "    n_gpu_layers=-1, # The number of layers to offload to GPU, if you have GPU acceleration available.\n",
    "    # verbose=False,\n",
    "    flash_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f0553895-3157-4d0e-be26-b37a34929d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_qwen(original_sentence, prompt_input=final_prompt):\n",
    "    \"\"\"Function to convert the prompt into a Qwen formatted prompt\"\"\"\n",
    "\n",
    "    # Qwen2 has the format\n",
    "    \n",
    "    # <|im_start|>system\n",
    "    # {system_prompt}<|im_end|>\n",
    "    # <|im_start|>user\n",
    "    # {prompt}<|im_end|>\n",
    "    # <|im_start|>assistant\n",
    "\n",
    "    # Get the messages part of the prompt and initialise an empty string\n",
    "    messages = prompt_input.messages\n",
    "    formatted_messages = \"\"\n",
    "\n",
    "    # For each item follow certain rules. If few shot example loop through all examples and append the \n",
    "    # input and output to the string as you do.\n",
    "    for message in messages:\n",
    "        \n",
    "        if isinstance(message, SystemMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|im_start|>system\\n{message.prompt.template.replace('\\n', '')}<|im_end|>\\n\"  \n",
    "        elif isinstance(message, FewShotChatMessagePromptTemplate):\n",
    "            for i in range(0, len(message.examples)):\n",
    "                \n",
    "                formatted_messages += f\"<|im_start|>user\\n{message.examples[i]['sentence'].replace('\\n', '')}<|im_end|>\\n\"\n",
    "                formatted_messages += f\"<|im_start|>assistant\\n{message.examples[i]['rephrased_sentences']}<|im_end|>\\n\"\n",
    "                \n",
    "        elif isinstance(message, HumanMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|im_start|>user\\n{message.prompt.template.replace('\\n', '')}<|im_end|>\\n\"\n",
    "\n",
    "    # Append the final tag for the beginning of the assistant message\n",
    "    formatted_messages += f\"<|im_start|>assistant\"\n",
    "\n",
    "    # I'm not using a chain so just replace the tag for the users' sentence with the actual sentence.\n",
    "    formatted_prompt = formatted_messages.replace(\"<|im_start|>user\\n{sentence}<|im_end|>\",\n",
    "                                                  f\"<|im_start|>user\\n{original_sentence}<|im_end|>\")\n",
    "    \n",
    "    return formatted_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7dbd7289-1497-4e99-b4a4-4a4145bcf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"Usain Bolt, winner of the 100m sprint, was known for his lightning bolt celebration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "67f59ee6-d774-45cd-b41d-6db2b3ea86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = convert_to_qwen(original_sentence, prompt_input=final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d9e9e4c6-63d4-4102-88b8-52297666ee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nGiven the sentence, generate at least twenty paraphrased versions that maintain the original semantic meaning and style. Return the rephrased sentences in a Python list format, ensuring the output starts with \\'[\\' and ends with \\']\\'. Include no additional text or notes in the output.<|im_end|>\\n<|im_start|>user\\nDBC Pierre, Booker Prize-winner and author of Vernon God Little, has moved to Ireland.<|im_end|>\\n<|im_start|>assistant\\n[\\'Booker Prize-winner DBC Pierre, known for writing Vernon God Little, has moved to Ireland.\\', \\'The author of Vernon God Little and Booker Prize-winner, DBC Pierre, has settled in Ireland.\\', \\'DBC Pierre, who won the Booker Prize for Vernon God Little, has now moved to Ireland.\\', \\'DBC Pierre, celebrated for his Booker Prize-winning novel Vernon God Little, has moved to Ireland.\\', \\'Winner of the Booker Prize and author of Vernon God Little, DBC Pierre, has relocated to Ireland.\\', \\'The Booker Prize-winning author of Vernon God Little, DBC Pierre, has taken up residence in Ireland.\\', \\'DBC Pierre, the acclaimed Booker Prize-winner for Vernon God Little, has moved to Ireland.\\', \"Booker Prize laureate and Vernon God Little\\'s author, DBC Pierre, has moved to Ireland.\", \\'DBC Pierre, the author of the Booker Prize-winning novel Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, recognized for his Booker Prize win with Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, famous for Vernon God Little and a Booker Prize-winner, has moved to Ireland.\\', \\'Booker Prize-winning author DBC Pierre, known for Vernon God Little, has now moved to Ireland.\\', \\'DBC Pierre, who won the Booker Prize for his novel Vernon God Little, has relocated to Ireland.\\', \\'The author DBC Pierre, who won the Booker Prize for Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, who penned the Booker Prize-winning Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, a Booker Prize winner for Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, recognized for his Booker Prize-winning book Vernon God Little, has moved to Ireland.\\', \\'Booker Prize-winning author DBC Pierre, famous for Vernon God Little, has moved to Ireland.\\', \\'DBC Pierre, the Booker Prize-winning novelist of Vernon God Little, has moved to Ireland.\\']<|im_end|>\\n<|im_start|>user\\nKnown for being very delicate, the skill could take a lifetime to master.<|im_end|>\\n<|im_start|>assistant\\n[\\'Famous for being delicate, this skill might require a lifetime to master.\\', \\'This skill, known for its delicacy, could take a lifetime to master.\\', \"The skill\\'s reputation for delicacy means it could take a lifetime to master.\", \\'Mastering this skill, which is known for its delicacy, might take a lifetime.\\', \\'The delicacy of this skill suggests it could take a lifetime to master.\\', \\'Renowned for its delicateness, the skill might take a lifetime to master.\\', \\'Due to its delicate nature, the skill could take a lifetime to master.\\', \\'The skill, famed for its delicacy, might require a lifetime to master.\\', \\'Known for its delicate nature, this skill could take a lifetime to master.\\', \\'Because of its delicateness, the skill could take a lifetime to master.\\', \\'The skill, recognized for its delicacy, could take a lifetime to master.\\', \\'This skill, due to its delicate nature, might take a lifetime to master.\\', \\'The skill, famous for being very delicate, could take a lifetime to master.\\', \\'The delicate nature of the skill means it might take a lifetime to master.\\', \\'Given its delicacy, the skill could take a lifetime to master.\\', \"The skill\\'s delicate nature suggests it could take a lifetime to master.\", \\'Known for its very delicate nature, the skill could take a lifetime to master.\\', \\'Mastering this delicate skill could require a lifetime.\\', \\'This skill, noted for its delicacy, might take a lifetime to master.\\']<|im_end|>\\n<|im_start|>user\\nUsain Bolt, winner of the 100m sprint, was known for his lightning bolt celebration.<|im_end|>\\n<|im_start|>assistant'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a8c01c36-d2d1-4653-8fe8-3c83bc7600a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   30753.65 ms\n",
      "llama_print_timings:      sample time =     468.33 ms /   431 runs   (    1.09 ms per token,   920.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55418.88 ms /   849 tokens (   65.28 ms per token,    15.32 tokens per second)\n",
      "llama_print_timings:        eval time =   91709.76 ms /   430 runs   (  213.28 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =  153647.48 ms /  1279 tokens\n"
     ]
    }
   ],
   "source": [
    "output_str = llm(formatted_prompt, max_tokens=1000, stop=[\"<|endoftext|>\"], temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f05df278-0039-42f2-9ed0-ea23b2757849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-43361007-c0d5-4abd-b73d-3f1d5a8d1de9',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1717718999,\n",
       " 'model': 'C:/Users/benjc/Documents/models/Qwen2-7B-Instruct-Q4_K_M.gguf',\n",
       " 'choices': [{'text': '\\n[\\'Usain Bolt, the champion in the 100-meter dash, was famous for executing the lightning bolt pose.\\', \\'The title holder of the 100m sprint, Usain Bolt, was renowned for his lightning bolt gesture.\\', \\'Usain Bolt, who triumphed in the 100m sprint, was recognized for his distinctive lighting bolt celebration.\\', \"Usain Bolt\\'s victory in the 100m race was characterized by his iconic lightning bolt salute.\", \\'The person victorious in the 100-meter dash, Usain Bolt, was known for his signature lighting bolt dance move.\\', \"For being known for his lighting bolt pose, Usain Bolt won the 100 meter sprint.\", \\'Usain Bolt, the winner of the 100m race, is renowned for his lightning bolt gesture.\\', \\'The champion in the 100m sprint, Usain Bolt, was distinguished by his lightning bolt celebration.\\', \\'Winning the 100m sprint, Usain Bolt was noted for his lighting bolt salute.\\', \\'Usain Bolt, the winner of the 100-meter race, was famous for his lighting bolt pose.\\', \"For winning the 100 meters sprint, Usain Bolt was known for his lightning bolt gesture.\", \\'The title holder of the 100-meter dash, Usain Bolt, was recognized for his lightning bolt move.\\', \\'Usain Bolt, who won the 100m sprint, is famous for his lightning bolt celebration.\\', \\'Usain Bolt, who achieved victory in the 100 meter race, was known for his lighting bolt salute.\\', \"For executing the lighting bolt gesture, Usain Bolt was victorious in the 100 meter sprint.\", \\'The champion of the 100-meter dash, Usain Bolt, is famous for his lightning bolt celebration.\\', \"Winning the 100m sprint, Usain Bolt\\'s signature move was his lightning bolt pose.\", \\'Usain Bolt, who won the 100 meters race, was known for his lighting bolt salute.\\']',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 849,\n",
       "  'completion_tokens': 430,\n",
       "  'total_tokens': 1279}}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ed7455c9-b7e6-494a-9f6e-66e0bfa5d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = output_str['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "07a59982-8503-4301-8d6d-648e5e075435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Usain Bolt, the champion in the 100-meter dash, was famous for executing the lightning bolt pose.', 'The title holder of the 100m sprint, Usain Bolt, was renowned for his lightning bolt gesture.', 'Usain Bolt, who triumphed in the 100m sprint, was recognized for his distinctive lighting bolt celebration.', \"Usain Bolt's victory in the 100m race was characterized by his iconic lightning bolt salute.\", 'The person victorious in the 100-meter dash, Usain Bolt, was known for his signature lighting bolt dance move.', \"For being known for his lighting bolt pose, Usain Bolt won the 100 meter sprint.\", 'Usain Bolt, the winner of the 100m race, is renowned for his lightning bolt gesture.', 'The champion in the 100m sprint, Usain Bolt, was distinguished by his lightning bolt celebration.', 'Winning the 100m sprint, Usain Bolt was noted for his lighting bolt salute.', 'Usain Bolt, the winner of the 100-meter race, was famous for his lighting bolt pose.', \"For winning the 100 meters sprint, Usain Bolt was known for his lightning bolt gesture.\", 'The title holder of the 100-meter dash, Usain Bolt, was recognized for his lightning bolt move.', 'Usain Bolt, who won the 100m sprint, is famous for his lightning bolt celebration.', 'Usain Bolt, who achieved victory in the 100 meter race, was known for his lighting bolt salute.', \"For executing the lighting bolt gesture, Usain Bolt was victorious in the 100 meter sprint.\", 'The champion of the 100-meter dash, Usain Bolt, is famous for his lightning bolt celebration.', \"Winning the 100m sprint, Usain Bolt's signature move was his lightning bolt pose.\", 'Usain Bolt, who won the 100 meters race, was known for his lighting bolt salute.']\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b2d3e-8fb9-44d2-a6e6-00642246e1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para_llm",
   "language": "python",
   "name": "para_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
