{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4419d9b-0a56-4f2e-ae87-a209e70da11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd5fa2-1bce-4e62-ae77-ccc7c594f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"read_and_write_docs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b5749-8cf0-4cd7-a257-e159736ae438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./google_drive.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1537ac-0484-4090-ad53-e4552f305f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_id(service, main_folder_name):\n",
    "    \"\"\"Function to get the id of the raw folder within the main folder on Google Drive\"\"\"\n",
    "    \n",
    "    # Get the files info from within the main folder\n",
    "    main_folder_id = search_file_by_name(service, main_folder_name)['id']\n",
    "    files_data = get_files_in_folder_recursive(service, main_folder_id, main_folder_name)\n",
    "    files_data = split_name_column(files_data)\n",
    "\n",
    "    # Get the folder data from within the file data\n",
    "    folders = files_data[files_data['is_folder'] == True]\n",
    "\n",
    "    # Split the data into the raw, error, and processed data\n",
    "    raw_id = folders[folders['name'] == 'raw'].iloc[0,2]\n",
    "\n",
    "    return raw_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93383216-875d-49f0-b1f3-e7864b7a089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Given the sentence, generate as many paraphrased sentences as possible while preserving the original semantic meaning and style. \n",
    "Return the rephrased sentences in a python list format. Aim for AT LEAST TWENTY sentences. DO NOT INCLUDE ANY NOTES OR ADDITIONAL TEXT IN THE OUTPUT.\n",
    "\n",
    "An example is below:\n",
    "--------\n",
    "Sentence: ```\"Known for being very delicate, the skill could take a lifetime to master.\"```\n",
    "\n",
    "Rephrased Sentences: ```[\"The skill is well known for its delicacy and could require a lifetime to perfect.\", \"The skill's reputation for delicateness suggests that it could take a whole lifetime to master.\", \"It may take a lifetime to master the skill, which is renowned for its delicacy.\", \"The delicacy of the skill means it could take a lifetime to master.\"]```\n",
    "--------\n",
    "Sentence: ```{original_user_supplied_sentence}```\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{original_user_supplied_sentence}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f41564-b276-4313-ad1e-bea604eac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_phi(original_sentence,\n",
    "                   prompt_input=final_prompt):\n",
    "\n",
    "    messages = prompt_input.messages\n",
    "    \n",
    "    formatted_messages = \"\"\n",
    "\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|assistant|>\\n{message.prompt.template.replace('\\n', '')} <|end|>\\n\"\n",
    "        elif isinstance(message, FewShotChatMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|user|>\\n{message.examples[0]['original_user_supplied_sentence'].replace('\\n', '')} <|end|>\\n\"\n",
    "            formatted_messages += f\"<|assistant|>\\n{message.examples[0]} <|end|>\\n\"\n",
    "        elif isinstance(message, HumanMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|user|>\\n{message.prompt.template.replace('\\n', '')} <|end|>\\n\"\n",
    "    \n",
    "    formatted_messages += f\"<|assistant|>\"\n",
    "\n",
    "    formatted_prompt = formatted_messages.replace(\"<|user|>\\n{original_user_supplied_sentence} <|end|>\", f\"<|user|>\\n{original_sentence} <|end|>\")\n",
    "    \n",
    "    return formatted_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508c090-9918-4157-a354-2f45673f065b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path=\"C:/Users/benjc/Documents/models/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=10, # Maximum I have is 12\n",
    "    n_gpu_layers=-1, # The number of layers to offload to GPU, if you have GPU acceleration available.\n",
    "    verbose=False,\n",
    "    flash_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323244b-eba6-4eda-a003-618e9536cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_paraphrase(original_sentence,\n",
    "                   n_runs=10,\n",
    "                   llm=llm,\n",
    "                   prompt_input=final_prompt):\n",
    "\n",
    "\n",
    "    formatted_prompt = convert_to_phi(original_sentence, prompt_input=final_prompt)\n",
    "\n",
    "    sentences = [original_sentence]\n",
    "    new_sentence_amount = 1\n",
    "    \n",
    "    for i in range(1, n_runs + 1):\n",
    "\n",
    "        if new_sentence_amount == 0:\n",
    "            break\n",
    "        print(f'  Iteration: {i}')\n",
    "        attempts = 1\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                output_str = llm(formatted_prompt, max_tokens=1000, stop=[\"<|end|>\"],\n",
    "                                temperature=1)\n",
    "                output_text = output_str['choices'][0]['text']\n",
    "\n",
    "                # Find the index of the first '[' and the last ']'\n",
    "                start_index = output_text.find('[')\n",
    "                end_index = output_text.rfind(']')\n",
    "\n",
    "                # Extract the content between the first '[' and the last ']'\n",
    "                content_str = output_text[start_index+1:end_index]\n",
    "\n",
    "                # Evaluate the content string as a Python expression to convert it into a list\n",
    "                result_list = eval('[' + content_str + ']')\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'    Attempt {attempts} failed: {str(e)}')\n",
    "                attempts += 1  # Increment the number of attempts\n",
    "\n",
    "                if attempts == 4:\n",
    "                    print(\"3 Attempts Exceeded, Next Iteration.\")\n",
    "                    result_list = []\n",
    "                    break\n",
    "\n",
    "        new_sentence_amount = 0\n",
    "\n",
    "        for result in result_list:\n",
    "            if result not in sentences:\n",
    "                sentences.append(result)\n",
    "                new_sentence_amount += 1\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff83e1-660e-4428-bf02-f760c78401e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_df(df, *args):\n",
    "    doc_ids = []\n",
    "    chunks = []\n",
    "    rephrased_sentences = []\n",
    "    \n",
    "    n_rows = df['id'].count()\n",
    "\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_num = index + 1\n",
    "        print(f'Row {row_num} out of {n_rows}')\n",
    "        doc_id = row['id']\n",
    "        chunk = row['chunk_id']\n",
    "        sentence = row['text']\n",
    "        \n",
    "        rephrased = phi_paraphrase(sentence, *args)\n",
    "        num_sent = len(rephrased)\n",
    "        \n",
    "        # Extend lists with repeated doc_id and chunk_id\n",
    "        doc_ids.extend([doc_id] * num_sent)\n",
    "        chunks.extend([chunk] * num_sent)\n",
    "        \n",
    "        rephrased_sentences.extend(rephrased)\n",
    "\n",
    "    # Construct DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'doc_id': doc_ids,\n",
    "        'chunk_id': chunks,\n",
    "        'sentence': rephrased_sentences\n",
    "    })\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9468080-5a8c-4cfe-b578-e16dced6127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl_file('../data/guardian_preprocessed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d8fca-d758-41c4-bc01-f7b73119b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['id'] == 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941d0f5-27fd-47a0-a45f-cf62ac61e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[(df['id'] == 2) & (df['chunk_id'] == 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570ee92-5230-41db-bbdd-a786fec57089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_df_save(df, base_save_loc, google_drive_main_folder, *args):\n",
    "\n",
    "    # Connect to my Google Drive \n",
    "    service = connect_to_drive()\n",
    "\n",
    "    # Get the id of the raw save location in chosen folder\n",
    "    raw_folder_id = get_raw_id(service, google_drive_main_folder)\n",
    "    \n",
    "    docs = df['id'].unique().tolist()\n",
    "    \n",
    "    if 'subchunk_id' in df.columns:\n",
    "        subchunk = True\n",
    "    else:\n",
    "        subchunk = False\n",
    "\n",
    "    for doc in docs:\n",
    "        filtered_df = df[df['id'] == doc]\n",
    "        n_rows = filtered_df['id'].count()\n",
    "\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            doc_ids = []\n",
    "            chunks = []\n",
    "            subchunks = []\n",
    "            rephrased_sentences = []\n",
    "            \n",
    "            doc_id = row['id']\n",
    "            chunk = row['chunk_id']\n",
    "            max_chunk = filtered_df['chunk_id'].max()\n",
    "            sentence = row['text']\n",
    "                \n",
    "            print(f'Doc: {doc_id} - Chunk: {chunk + 1} out of {max_chunk + 1}')\n",
    "\n",
    "            rephrased = phi_paraphrase(sentence, *args)\n",
    "            num_sent = len(rephrased)\n",
    "        \n",
    "            # Extend lists with repeated doc_id and chunk_id\n",
    "            doc_ids.extend([doc_id] * num_sent)\n",
    "            chunks.extend([chunk] * num_sent)\n",
    "            rephrased_sentences.extend(rephrased)\n",
    "            \n",
    "            if subchunk:\n",
    "                s_chunk = row['subchunk_id']\n",
    "                subchunks.extend([s_chunk] * num_sent)\n",
    "\n",
    "                raw_df = pd.DataFrame({\n",
    "                    'doc_id': doc_ids,\n",
    "                    'chunk_id': chunks,\n",
    "                    'subchunk_id': subchunks,\n",
    "                    'rephrased': rephrased_sentences\n",
    "                })\n",
    "\n",
    "                filtered_raw_df = raw_df[(raw_df['doc_id'] == doc_id)  &\n",
    "                    (raw_df['chunk_id'] == chunk) &\n",
    "                    (raw_df['subchunk_id'] == s_chunk)]\n",
    "                \n",
    "                temp_loc = f\"{base_save_loc}temp.jsonl\"\n",
    "                google_drive_name = f\"doc_{doc}_chunk_{chunk}_subchunk_{s_chunk}.jsonl\"\n",
    "\n",
    "            else:\n",
    "                raw_df = pd.DataFrame({\n",
    "                    'doc_id': doc_ids,\n",
    "                    'chunk_id': chunks,\n",
    "                    'rephrased': rephrased_sentences\n",
    "                })\n",
    "\n",
    "                filtered_raw_df = raw_df[(raw_df['doc_id'] == doc_id)  &\n",
    "                    (raw_df['chunk_id'] == chunk)]\n",
    "                \n",
    "                temp_loc = f\"{base_save_loc}temp.jsonl\"\n",
    "                google_drive_name = f\"doc_{doc}_chunk_{chunk}.jsonl\"\n",
    "\n",
    "            try:\n",
    "                save_as_jsonl(filtered_raw_df, temp_loc)\n",
    "                upload_file(service, google_drive_name, temp_loc, parent_folder_id=\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ebb0c-920c-4883-a2cd-aa14dcf44d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase_df_save(df, base_save_loc = \"../data/guardian_phi/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
