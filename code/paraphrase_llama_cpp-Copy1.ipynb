{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4419d9b-0a56-4f2e-ae87-a209e70da11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffd5fa2-1bce-4e62-ae77-ccc7c594f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"read_and_write_docs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1537ac-0484-4090-ad53-e4552f305f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_id(service, main_folder_name):\n",
    "    \"\"\"Function to get the id of the raw folder within the main folder on Google Drive\"\"\"\n",
    "    \n",
    "    # Get the files info from within the main folder\n",
    "    main_folder_id = search_file_by_name(service, main_folder_name)['id']\n",
    "    files_data = get_files_in_folder_recursive(service, main_folder_id, main_folder_name)\n",
    "    files_data = split_name_column(files_data)\n",
    "\n",
    "    # Get the folder data from within the file data\n",
    "    folders = files_data[files_data['is_folder'] == True]\n",
    "\n",
    "    # Split the data into the raw, error, and processed data\n",
    "    raw_id = folders[folders['name'] == 'raw'].iloc[0,2]\n",
    "\n",
    "    return raw_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93383216-875d-49f0-b1f3-e7864b7a089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Given the sentence, generate as many paraphrased sentences as possible while preserving the original semantic meaning and style. \n",
    "Return the rephrased sentences in a python list format. Aim for AT LEAST TWENTY sentences. DO NOT INCLUDE ANY NOTES OR ADDITIONAL TEXT IN THE OUTPUT.\n",
    "\n",
    "An example is below:\n",
    "--------\n",
    "Sentence: ```\"Known for being very delicate, the skill could take a lifetime to master.\"```\n",
    "\n",
    "Rephrased Sentences: ```[\"The skill is well known for its delicacy and could require a lifetime to perfect.\", \"The skill's reputation for delicateness suggests that it could take a whole lifetime to master.\", \"It may take a lifetime to master the skill, which is renowned for its delicacy.\", \"The delicacy of the skill means it could take a lifetime to master.\"]```\n",
    "--------\n",
    "Sentence: ```{original_user_supplied_sentence}```\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{original_user_supplied_sentence}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f41564-b276-4313-ad1e-bea604eac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_phi(original_sentence,\n",
    "                   prompt_input=final_prompt):\n",
    "\n",
    "    messages = prompt_input.messages\n",
    "    \n",
    "    formatted_messages = \"\"\n",
    "\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|assistant|>\\n{message.prompt.template.replace('\\n', '')} <|end|>\\n\"\n",
    "        elif isinstance(message, FewShotChatMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|user|>\\n{message.examples[0]['original_user_supplied_sentence'].replace('\\n', '')} <|end|>\\n\"\n",
    "            formatted_messages += f\"<|assistant|>\\n{message.examples[0]} <|end|>\\n\"\n",
    "        elif isinstance(message, HumanMessagePromptTemplate):\n",
    "            formatted_messages += f\"<|user|>\\n{message.prompt.template.replace('\\n', '')} <|end|>\\n\"\n",
    "    \n",
    "    formatted_messages += f\"<|assistant|>\"\n",
    "\n",
    "    formatted_prompt = formatted_messages.replace(\"<|user|>\\n{original_user_supplied_sentence} <|end|>\", f\"<|user|>\\n{original_sentence} <|end|>\")\n",
    "    \n",
    "    return formatted_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144c46a-4bfa-489f-8b31-23b0cee6e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(formatted_prompt, max_tokens=1000, stop=[\"<|end|>\"],temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4508c090-9918-4157-a354-2f45673f065b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Llama' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m      4\u001b[0m \trepo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-72B-Instruct-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m \tfilename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2.5-72b-instruct-fp16-00001-of-00042.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m llm\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[1;32m      9\u001b[0m \tmessages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m \t\t{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \t]\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Llama' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"Qwen/Qwen2.5-72B-Instruct-GGUF\",\n",
    "\tfilename=\"qwen2.5-72b-instruct-fp16-00001-of-00042.gguf\",\n",
    ")\n",
    "\n",
    "llm.create_chat_completion(\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": \"What is the capital of France?\"\n",
    "\t\t}\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d76584-d63e-4c92-88a3-0bc866f24202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f1439-a88a-4b98-85e0-811533845107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323244b-eba6-4eda-a003-618e9536cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_paraphrase(original_sentence,\n",
    "                   n_runs=10,\n",
    "                   llm=llm,\n",
    "                   prompt_input=final_prompt):\n",
    "\n",
    "\n",
    "    formatted_prompt = convert_to_phi(original_sentence, prompt_input=final_prompt)\n",
    "\n",
    "    sentences = [original_sentence]\n",
    "    new_sentence_amount = 1\n",
    "    \n",
    "    for i in range(1, n_runs + 1):\n",
    "\n",
    "        if new_sentence_amount == 0:\n",
    "            break\n",
    "        print(f'  Iteration: {i}')\n",
    "        attempts = 1\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                output_str = llm(formatted_prompt, max_tokens=1000, stop=[\"<|end|>\"],\n",
    "                                temperature=1)\n",
    "                output_text = output_str['choices'][0]['text']\n",
    "\n",
    "                # Find the index of the first '[' and the last ']'\n",
    "                start_index = output_text.find('[')\n",
    "                end_index = output_text.rfind(']')\n",
    "\n",
    "                # Extract the content between the first '[' and the last ']'\n",
    "                content_str = output_text[start_index+1:end_index]\n",
    "\n",
    "                # Evaluate the content string as a Python expression to convert it into a list\n",
    "                result_list = eval('[' + content_str + ']')\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'    Attempt {attempts} failed: {str(e)}')\n",
    "                attempts += 1  # Increment the number of attempts\n",
    "\n",
    "                if attempts == 4:\n",
    "                    print(\"3 Attempts Exceeded, Next Iteration.\")\n",
    "                    result_list = []\n",
    "                    break\n",
    "\n",
    "        new_sentence_amount = 0\n",
    "\n",
    "        for result in result_list:\n",
    "            if result not in sentences:\n",
    "                sentences.append(result)\n",
    "                new_sentence_amount += 1\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff83e1-660e-4428-bf02-f760c78401e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_df(df, *args):\n",
    "    doc_ids = []\n",
    "    chunks = []\n",
    "    rephrased_sentences = []\n",
    "    \n",
    "    n_rows = df['id'].count()\n",
    "\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        row_num = index + 1\n",
    "        print(f'Row {row_num} out of {n_rows}')\n",
    "        doc_id = row['id']\n",
    "        chunk = row['chunk_id']\n",
    "        sentence = row['text']\n",
    "        \n",
    "        rephrased = phi_paraphrase(sentence, *args)\n",
    "        num_sent = len(rephrased)\n",
    "        \n",
    "        # Extend lists with repeated doc_id and chunk_id\n",
    "        doc_ids.extend([doc_id] * num_sent)\n",
    "        chunks.extend([chunk] * num_sent)\n",
    "        \n",
    "        rephrased_sentences.extend(rephrased)\n",
    "\n",
    "    # Construct DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'doc_id': doc_ids,\n",
    "        'chunk_id': chunks,\n",
    "        'sentence': rephrased_sentences\n",
    "    })\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9468080-5a8c-4cfe-b578-e16dced6127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl_file('../data/guardian_chunked_impostor.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d8fca-d758-41c4-bc01-f7b73119b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[(df['id'] >= 19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941d0f5-27fd-47a0-a45f-cf62ac61e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[(df['id'] == 18) & (df['chunk_id'] >= 46)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731b63a-8df9-4fbc-a51f-f6725858d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d38183-bc6c-4c00-98d7-2eeaee1719a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('id').size().reset_index(name='count')\n",
    "grouped_df = grouped_df.sort_values(by='count', ascending=True)\n",
    "grouped_df = grouped_df.iloc[(len(grouped_df)//2 - 5):(len(grouped_df)//2 + 5)]\n",
    "sample_list = grouped_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dceac6-502c-4f3b-be8b-e217d9865520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c1fac-f76f-4ebc-aa22-04f79b14fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['id'].isin(sample_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd27c4-7067-46f6-ba38-df5b0888e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da260a2-c5d4-4aae-822e-126d5874cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.iloc[0,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ae999-fa78-4f54-96da-93652521815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.iloc[1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570ee92-5230-41db-bbdd-a786fec57089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_df_save(df, base_save_loc, google_drive_main_folder, *args):\n",
    "\n",
    "    # Connect to my Google Drive \n",
    "    service = connect_to_drive()\n",
    "\n",
    "    # Get the id of the raw save location in chosen folder\n",
    "    raw_folder_id = get_raw_id(service, google_drive_main_folder)\n",
    "    \n",
    "    docs = df['id'].unique().tolist()\n",
    "    \n",
    "    if 'subchunk_id' in df.columns:\n",
    "        subchunk = True\n",
    "    else:\n",
    "        subchunk = False\n",
    "\n",
    "    for doc in docs:\n",
    "        filtered_df = df[df['id'] == doc]\n",
    "        n_rows = filtered_df['id'].count()\n",
    "\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            doc_ids = []\n",
    "            chunks = []\n",
    "            subchunks = []\n",
    "            rephrased_sentences = []\n",
    "            \n",
    "            doc_id = row['id']\n",
    "            chunk = row['chunk_id']\n",
    "            max_chunk = filtered_df['chunk_id'].max()\n",
    "            sentence = row['text']\n",
    "                \n",
    "            print(f'Doc: {doc_id} - Chunk: {chunk + 1} out of {max_chunk + 1}')\n",
    "\n",
    "            rephrased = phi_paraphrase(sentence, *args)\n",
    "            num_sent = len(rephrased)\n",
    "        \n",
    "            # Extend lists with repeated doc_id and chunk_id\n",
    "            doc_ids.extend([doc_id] * num_sent)\n",
    "            chunks.extend([chunk] * num_sent)\n",
    "            rephrased_sentences.extend(rephrased)\n",
    "            \n",
    "            if subchunk:\n",
    "                s_chunk = row['subchunk_id']\n",
    "                subchunks.extend([s_chunk] * num_sent)\n",
    "\n",
    "                raw_df = pd.DataFrame({\n",
    "                    'doc_id': doc_ids,\n",
    "                    'chunk_id': chunks,\n",
    "                    'subchunk_id': subchunks,\n",
    "                    'rephrased': rephrased_sentences\n",
    "                })\n",
    "\n",
    "                filtered_raw_df = raw_df[(raw_df['doc_id'] == doc_id)  &\n",
    "                    (raw_df['chunk_id'] == chunk) &\n",
    "                    (raw_df['subchunk_id'] == s_chunk)]\n",
    "                \n",
    "                temp_loc = f\"{base_save_loc}temp.jsonl\"\n",
    "                google_drive_name = f\"doc_{doc}_chunk_{chunk}_subchunk_{s_chunk}.jsonl\"\n",
    "\n",
    "            else:\n",
    "                raw_df = pd.DataFrame({\n",
    "                    'doc_id': doc_ids,\n",
    "                    'chunk_id': chunks,\n",
    "                    'rephrased': rephrased_sentences\n",
    "                })\n",
    "\n",
    "                filtered_raw_df = raw_df[(raw_df['doc_id'] == doc_id)  &\n",
    "                    (raw_df['chunk_id'] == chunk)]\n",
    "                \n",
    "                temp_loc = f\"{base_save_loc}temp.jsonl\"\n",
    "                google_drive_name = f\"doc_{doc}_chunk_{chunk}.jsonl\"\n",
    "\n",
    "            try:\n",
    "                save_as_jsonl(filtered_raw_df, temp_loc)\n",
    "                upload_file(service, google_drive_name, temp_loc, parent_folder_id=raw_folder_id)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ebb0c-920c-4883-a2cd-aa14dcf44d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase_df_save(filtered_df, base_save_loc = \"../data/guardian_phi/\", google_drive_main_folder=\"guardian_phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f234b7-96e5-4369-8305-3fcccc230861",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_df_save(filtered_df, base_save_loc=\"../data/guardian_phi_chunked\", google_drive_main_folder=\"guardian_phi_chunked\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
