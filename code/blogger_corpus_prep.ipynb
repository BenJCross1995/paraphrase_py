{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a333bdd0-9f58-480c-996e-81f7ac523d01",
   "metadata": {},
   "source": [
    "This notebook completes all of the steps to create a sample of the blogger corpus with even same and different authors, then it preprocesses the text, chunks it and gathers the metadata. The script saves at each point. I have not functionised it yet but could be done for a larger sample or for the PAN data which is effectively the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e88e50ea-de94-4d08-8829-3be42b30646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import read_and_write_docs\n",
    "import preprocessing\n",
    "import combine_sentences\n",
    "import combine_rephrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ee6cd884-01d4-4e6a-adaa-0785e5988086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words in text\n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3a11296e-1051-4245-9b92-15b29319bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loc = \"/Users/user/Downloads/blogtext.csv\"\n",
    "save_base_loc = \"/Users/user/Documents/GitHub/paraphrase_py/data/blogger_new_algorithm\"\n",
    "\n",
    "# Raw for split data, then preprocessed is converted to sentences and combined is\n",
    "# chunked by word count\n",
    "known_raw_loc = f\"{save_base_loc}/known_raw.jsonl\"\n",
    "known_preprocessed_loc = f\"{save_base_loc}/known_preprocessed.jsonl\"\n",
    "known_combined_loc = f\"{save_base_loc}/known_combined.jsonl\"\n",
    "known_final_loc = f\"{save_base_loc}/known_final.jsonl\"\n",
    "\n",
    "unknown_raw_loc = f\"{save_base_loc}/unknown_raw.jsonl\"\n",
    "unknown_preprocessed_loc = f\"{save_base_loc}/unknown_preprocessed.jsonl\"\n",
    "unknown_combined_loc = f\"{save_base_loc}/unknown_combined.jsonl\"\n",
    "unknown_final_loc = f\"{save_base_loc}/unknown_final.jsonl\"\n",
    "\n",
    "rephrased_preprocessed_loc = f\"{save_base_loc}/rephrased_preprocessed.jsonl\"\n",
    "\n",
    "metadata_loc = f\"{save_base_loc}/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "96de2b31-3efe-445c-b9fe-68551cc3c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_loc)\n",
    "# Remove any whitespace from the column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "80de9667-0670-441f-8457-ef012b0cfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(count_words)\n",
    "df['author_id'] = df['id']\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'author_id'] + [col for col in df.columns if col not in ['id', 'author_id']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e3631f9b-ace0-4f62-9332-d671d7160ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['word_count'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3b65dcca-1395-4bf0-9e7b-fc99c4f0ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter IDs with more than one row\n",
    "multi_row_ids = filtered_df['author_id'].value_counts()\n",
    "multi_row_ids = multi_row_ids[multi_row_ids > 1].index\n",
    "\n",
    "# Sample 5 IDs that have more than one row\n",
    "common_ids = multi_row_ids.to_series().sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "450ff2b3-8e42-431d-9bf4-f2477ebb1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/hy496x3x5sn4hy9gy1fk19lw0000gp/T/ipykernel_3092/913058778.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_common = common_rows.groupby('author_id').apply(lambda group: group.sample(1, random_state=2)).reset_index(drop=True)\n",
      "/var/folders/xx/hy496x3x5sn4hy9gy1fk19lw0000gp/T/ipykernel_3092/913058778.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_common = common_rows[~common_rows.index.isin(x_common.index)].groupby('author_id').apply(lambda group: group.sample(1, random_state=5)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have at least 2 rows for each common_id in the main dataframe\n",
    "common_rows = filtered_df[filtered_df['author_id'].isin(common_ids)]\n",
    "\n",
    "# Separate rows for common IDs into x_common and y_common ensuring different rows for each\n",
    "x_common = common_rows.groupby('author_id').apply(lambda group: group.sample(1, random_state=2)).reset_index(drop=True)\n",
    "\n",
    "# Ensure remaining rows for common IDs are used in y_common\n",
    "y_common = common_rows[~common_rows.index.isin(x_common.index)].groupby('author_id').apply(lambda group: group.sample(1, random_state=5)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "78e304e5-9ea1-431d-bd92-62c7234967a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/hy496x3x5sn4hy9gy1fk19lw0000gp/T/ipykernel_3092/3626867923.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_unique = filtered_df[filtered_df['author_id'].isin(x_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=6)).reset_index(drop=True)\n",
      "/var/folders/xx/hy496x3x5sn4hy9gy1fk19lw0000gp/T/ipykernel_3092/3626867923.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_unique = filtered_df[filtered_df['author_id'].isin(y_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=7)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Sample remaining unique IDs for x and y\n",
    "remaining_ids = filtered_df[~filtered_df['author_id'].isin(common_ids)]['author_id'].drop_duplicates()\n",
    "x_unique_ids = remaining_ids.sample(10, random_state=4)\n",
    "y_unique_ids = remaining_ids[~remaining_ids.isin(x_unique_ids)].sample(10, random_state=5)\n",
    "\n",
    "# Extract a random row for each unique ID for x and y\n",
    "x_unique = filtered_df[filtered_df['author_id'].isin(x_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=6)).reset_index(drop=True)\n",
    "y_unique = filtered_df[filtered_df['author_id'].isin(y_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=7)).reset_index(drop=True)\n",
    "\n",
    "# Combine common and unique rows\n",
    "x = pd.concat([x_common, x_unique]).reset_index(drop=True)\n",
    "y = pd.concat([y_common, y_unique]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "27f2d139-138a-4dcf-825c-afe51691608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "723e987a-1d63-4e11-ab3b-2cf4f1f0d51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = y.drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6afd7a12-bae9-4dff-b55b-29cce51bcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the blogger docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0f60636e-ce51-4c08-a832-f83f12bc708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(x, known_raw_loc)\n",
    "read_and_write_docs.save_as_jsonl(y, unknown_raw_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a7f58da2-40b4-480e-beec-63d415d141aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "be944d04-3afa-4378-98e3-b921bac4fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = preprocessing.apply_sentence_split(x)\n",
    "known = preprocessing.split_rows_by_word_count(known, num_words=250)\n",
    "\n",
    "unknown = preprocessing.apply_sentence_split(y)\n",
    "unknown = preprocessing.split_rows_by_word_count(unknown, num_words=250)\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known, known_preprocessed_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown, unknown_preprocessed_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "eed046e9-49a2-4f53-bc78-0a16aa5ec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d7bda1b3-995e-4a8e-9ff4-1b35e15f11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_combined = combine_sentences.concatenate_sentences(known, length_threshold=500, threshold_type='word')\n",
    "unknown_combined = combine_sentences.concatenate_sentences(unknown, length_threshold=500, threshold_type='word')\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known_combined, known_combined_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown_combined, unknown_combined_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b1505f7c-c7bb-495d-b70b-10e4265c1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, filter_type):\n",
    "    \"\"\"\n",
    "    Process the dataframe to add row numbers and chunk_count from the first row within each doc_id group.\n",
    "    Then filter based on the filter_type.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with columns 'id', 'chunk_id', 'subchunk_id', and 'chunk_count'.\n",
    "    filter_type (str): The filter criteria, can be 'known', 'unknown', or 'rephrased'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed and filtered dataframe.\n",
    "    \"\"\"\n",
    "    # Sort the dataframe by id, chunk_id, subchunk_id\n",
    "    sorted_df = df.sort_values(by=['id', 'chunk_id', 'subchunk_id'])\n",
    "\n",
    "    # Assign row numbers within each id group\n",
    "    sorted_df['row_number'] = sorted_df.groupby('id').cumcount() + 1\n",
    "\n",
    "    # Find the chunk_count where row_number == 1 within each group\n",
    "    chunk_count_first_row = sorted_df[sorted_df['row_number'] == 1][['id', 'chunk_count']]\n",
    "    chunk_count_first_row = chunk_count_first_row.rename(columns={'chunk_count': 'chunk_count_first_row'})\n",
    "\n",
    "    # Merge the chunk_count_first_row back into the original dataframe\n",
    "    result = pd.merge(sorted_df, chunk_count_first_row, on='id', how='left')\n",
    "\n",
    "    # Apply the filter based on filter_type\n",
    "    if filter_type in ['known', 'unknown']:\n",
    "        columns_to_drop = ['index', 'chunk_id', 'subchunk_id', 'input_length', 'chunk_count',\n",
    "                           'original_sentence', 'row_number', 'chunk_count_first_row']\n",
    "        result = result[result['row_number'] == 1]\n",
    "        result = result.drop(columns=columns_to_drop)\n",
    "        result = result.rename(columns={'id': 'doc_id'})\n",
    "    elif filter_type == 'rephrased':\n",
    "        columns_to_drop = ['index','word_count', 'chunk_id', 'subchunk_id', 'input_length',\n",
    "                           'chunk_count', 'text', 'chunk_count_first_row']\n",
    "        result = result[result['row_number'] <= result['chunk_count_first_row']]\n",
    "        result = result.drop(columns=columns_to_drop)\n",
    "        result = result.rename(columns={'row_number': 'chunk_id', 'original_sentence': 'text',\n",
    "                                       'id': 'doc_id'})\n",
    "        result = result.reset_index(drop=True)\n",
    "        # Adjust the index to start from 1\n",
    "        result.index = result.index + 1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid filter_type. Must be 'known', 'unknown', or 'rephrased'.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd5b0ee1-cccb-4a2c-a98e-969220faf986",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_final = process_dataframe(unknown_combined, 'unknown')\n",
    "known_final = process_dataframe(known_combined, 'known')\n",
    "rephrased_final = process_dataframe(unknown_combined, 'rephrased')\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known_final, known_final_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown_final, unknown_final_loc)\n",
    "read_and_write_docs.save_as_jsonl(rephrased_final, rephrased_preprocessed_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ba2cdb3b-d9ff-4358-ad43-002fc8250a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in x and y\n",
    "known_final.rename(columns={'author_id': 'author_known', 'topic': 'topic_known', 'doc_id': 'doc_id_known'}, inplace=True)\n",
    "unknown_final.rename(columns={'author_id': 'author_unknown', 'topic': 'topic_unknown', 'doc_id': 'doc_id_unknown'}, inplace=True)\n",
    "\n",
    "author_id_x = x['author_id'].tolist()\n",
    "author_id_y = y['author_id'].tolist()\n",
    "\n",
    "known_data = known_final.sort_values(by=['author_known'],\n",
    "                                    key=lambda col: col.map({val: i for i, val in enumerate(author_id_x)}))\n",
    "\n",
    "unknown_data = unknown_final.sort_values(by=['author_unknown'],\n",
    "                                        key=lambda col: col.map({val: i for i, val in enumerate(author_id_y)}))\n",
    "\n",
    "known_data['sample_id'] = range(1,len(known_data) + 1)\n",
    "unknown_data['sample_id'] = range(1,len(unknown_data) + 1)\n",
    "\n",
    "# Merge x and y on sample_id\n",
    "metadata = pd.merge(known_data, unknown_data, on='sample_id', how='inner')\n",
    "\n",
    "metadata['same_author'] = metadata['author_known'] == metadata['author_unknown']\n",
    "metadata = metadata[['sample_id', 'doc_id_known', 'doc_id_unknown', 'author_known',\n",
    "                     'author_unknown', 'topic_known', 'topic_unknown', 'same_author']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "467c6303-7b8b-476c-bc62-40687d0e267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(metadata, metadata_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bd834765-bd54-40ec-af7e-1af1cb892b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>doc_id_known</th>\n",
       "      <th>doc_id_unknown</th>\n",
       "      <th>author_known</th>\n",
       "      <th>author_unknown</th>\n",
       "      <th>topic_known</th>\n",
       "      <th>topic_unknown</th>\n",
       "      <th>same_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>114606</td>\n",
       "      <td>114599</td>\n",
       "      <td>27603</td>\n",
       "      <td>27603</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>509664</td>\n",
       "      <td>509577</td>\n",
       "      <td>993945</td>\n",
       "      <td>993945</td>\n",
       "      <td>HumanResources</td>\n",
       "      <td>HumanResources</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>402493</td>\n",
       "      <td>402496</td>\n",
       "      <td>1796990</td>\n",
       "      <td>1796990</td>\n",
       "      <td>Sports-Recreation</td>\n",
       "      <td>Sports-Recreation</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>357508</td>\n",
       "      <td>357437</td>\n",
       "      <td>2534568</td>\n",
       "      <td>2534568</td>\n",
       "      <td>Education</td>\n",
       "      <td>Education</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>676576</td>\n",
       "      <td>676573</td>\n",
       "      <td>2876684</td>\n",
       "      <td>2876684</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Technology</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>349669</td>\n",
       "      <td>349665</td>\n",
       "      <td>3152540</td>\n",
       "      <td>3152540</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Arts</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>463281</td>\n",
       "      <td>463289</td>\n",
       "      <td>3492066</td>\n",
       "      <td>3492066</td>\n",
       "      <td>Student</td>\n",
       "      <td>Student</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>490838</td>\n",
       "      <td>490831</td>\n",
       "      <td>3835771</td>\n",
       "      <td>3835771</td>\n",
       "      <td>Student</td>\n",
       "      <td>Student</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>288777</td>\n",
       "      <td>288775</td>\n",
       "      <td>3911836</td>\n",
       "      <td>3911836</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>18515</td>\n",
       "      <td>18516</td>\n",
       "      <td>4160528</td>\n",
       "      <td>4160528</td>\n",
       "      <td>Student</td>\n",
       "      <td>Student</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>178616</td>\n",
       "      <td>569896</td>\n",
       "      <td>1365227</td>\n",
       "      <td>554681</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>578879</td>\n",
       "      <td>585623</td>\n",
       "      <td>2117806</td>\n",
       "      <td>3189607</td>\n",
       "      <td>Student</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>255148</td>\n",
       "      <td>401638</td>\n",
       "      <td>3441279</td>\n",
       "      <td>3213695</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>247547</td>\n",
       "      <td>299096</td>\n",
       "      <td>3451090</td>\n",
       "      <td>3372901</td>\n",
       "      <td>Education</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>404625</td>\n",
       "      <td>546828</td>\n",
       "      <td>3840627</td>\n",
       "      <td>3683175</td>\n",
       "      <td>Student</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15111</td>\n",
       "      <td>213621</td>\n",
       "      <td>3898365</td>\n",
       "      <td>3698710</td>\n",
       "      <td>Student</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>582213</td>\n",
       "      <td>501802</td>\n",
       "      <td>4091880</td>\n",
       "      <td>3758011</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Student</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>41306</td>\n",
       "      <td>415925</td>\n",
       "      <td>4212782</td>\n",
       "      <td>4054452</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Education</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>240645</td>\n",
       "      <td>253553</td>\n",
       "      <td>4231260</td>\n",
       "      <td>4114873</td>\n",
       "      <td>Technology</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>209855</td>\n",
       "      <td>435880</td>\n",
       "      <td>4289870</td>\n",
       "      <td>4138502</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id  doc_id_known  doc_id_unknown  author_known  author_unknown  \\\n",
       "0           1        114606          114599         27603           27603   \n",
       "1           2        509664          509577        993945          993945   \n",
       "2           3        402493          402496       1796990         1796990   \n",
       "3           4        357508          357437       2534568         2534568   \n",
       "4           5        676576          676573       2876684         2876684   \n",
       "5           6        349669          349665       3152540         3152540   \n",
       "6           7        463281          463289       3492066         3492066   \n",
       "7           8        490838          490831       3835771         3835771   \n",
       "8           9        288777          288775       3911836         3911836   \n",
       "9          10         18515           18516       4160528         4160528   \n",
       "10         11        178616          569896       1365227          554681   \n",
       "11         12        578879          585623       2117806         3189607   \n",
       "12         13        255148          401638       3441279         3213695   \n",
       "13         14        247547          299096       3451090         3372901   \n",
       "14         15        404625          546828       3840627         3683175   \n",
       "15         16         15111          213621       3898365         3698710   \n",
       "16         17        582213          501802       4091880         3758011   \n",
       "17         18         41306          415925       4212782         4054452   \n",
       "18         19        240645          253553       4231260         4114873   \n",
       "19         20        209855          435880       4289870         4138502   \n",
       "\n",
       "          topic_known      topic_unknown  same_author  \n",
       "0         Advertising        Advertising         True  \n",
       "1      HumanResources     HumanResources         True  \n",
       "2   Sports-Recreation  Sports-Recreation         True  \n",
       "3           Education          Education         True  \n",
       "4          Technology         Technology         True  \n",
       "5                Arts               Arts         True  \n",
       "6             Student            Student         True  \n",
       "7             Student            Student         True  \n",
       "8              indUnk             indUnk         True  \n",
       "9             Student            Student         True  \n",
       "10             indUnk             indUnk        False  \n",
       "11            Student             indUnk        False  \n",
       "12             indUnk             indUnk        False  \n",
       "13          Education            Tourism        False  \n",
       "14            Student             indUnk        False  \n",
       "15            Student             indUnk        False  \n",
       "16     Transportation            Student        False  \n",
       "17             indUnk          Education        False  \n",
       "18         Technology             indUnk        False  \n",
       "19             indUnk             indUnk        False  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
