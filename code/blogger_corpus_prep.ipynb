{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a333bdd0-9f58-480c-996e-81f7ac523d01",
   "metadata": {},
   "source": [
    "This notebook completes all of the steps to create a sample of the blogger corpus with even same and different authors, then it preprocesses the text, chunks it and gathers the metadata. The script saves at each point. I have not functionised it yet but could be done for a larger sample or for the PAN data which is effectively the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88e50ea-de94-4d08-8829-3be42b30646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import read_and_write_docs\n",
    "import preprocessing\n",
    "import combine_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6cd884-01d4-4e6a-adaa-0785e5988086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words in text\n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a11296e-1051-4245-9b92-15b29319bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loc = \"/Users/user/Downloads/blogtext.csv\"\n",
    "save_base_loc = \"/Users/user/Documents/GitHub/paraphrase_py/data/blogger\"\n",
    "\n",
    "# Raw for split data, then preprocessed is converted to sentences and combined is\n",
    "# chunked by word count\n",
    "known_raw_loc = f\"{save_base_loc}/known_raw.jsonl\"\n",
    "known_preprocessed_loc = f\"{save_base_loc}/known_preprocessed.jsonl\"\n",
    "known_combined_loc = f\"{save_base_loc}/known_combined.jsonl\"\n",
    "\n",
    "unknown_raw_loc = f\"{save_base_loc}/unknown_raw.jsonl\"\n",
    "unknown_preprocessed_loc = f\"{save_base_loc}/unknown_preprocessed.jsonl\"\n",
    "unknown_combined_loc = f\"{save_base_loc}/unknown_combined.jsonl\"\n",
    "\n",
    "metadata_loc = f\"{save_base_loc}/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96de2b31-3efe-445c-b9fe-68551cc3c731",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/user/Downloads/blogtext.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Remove any whitespace from the column names\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/user/Downloads/blogtext.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(raw_loc)\n",
    "# Remove any whitespace from the column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de9667-0670-441f-8457-ef012b0cfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(count_words)\n",
    "df['author_id'] = df['id']\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'author_id'] + [col for col in df.columns if col not in ['id', 'author_id']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3631f9b-ace0-4f62-9332-d671d7160ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['word_count'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b2d92-3dee-4870-9b85-19c934fa0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65dcca-1395-4bf0-9e7b-fc99c4f0ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter IDs with more than one row\n",
    "multi_row_ids = filtered_df['author_id'].value_counts()\n",
    "multi_row_ids = multi_row_ids[multi_row_ids > 1].index\n",
    "\n",
    "# Sample 5 IDs that have more than one row\n",
    "common_ids = multi_row_ids.to_series().sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ff2b3-8e42-431d-9bf4-f2477ebb1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have at least 2 rows for each common_id in the main dataframe\n",
    "common_rows = filtered_df[filtered_df['author_id'].isin(common_ids)]\n",
    "\n",
    "# Separate rows for common IDs into x_common and y_common ensuring different rows for each\n",
    "x_common = common_rows.groupby('author_id').apply(lambda group: group.sample(1, random_state=2)).reset_index(drop=True)\n",
    "\n",
    "# Ensure remaining rows for common IDs are used in y_common\n",
    "y_common = common_rows[~common_rows.index.isin(x_common.index)].groupby('author_id').apply(lambda group: group.sample(1, random_state=5)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a813a9-4ad3-4edd-98e4-7105107d05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50c6d9-35c3-4205-b7a5-e454a8fcfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e304e5-9ea1-431d-bd92-62c7234967a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample remaining unique IDs for x and y\n",
    "remaining_ids = filtered_df[~filtered_df['author_id'].isin(common_ids)]['author_id'].drop_duplicates()\n",
    "x_unique_ids = remaining_ids.sample(5, random_state=4)\n",
    "y_unique_ids = remaining_ids[~remaining_ids.isin(x_unique_ids)].sample(5, random_state=5)\n",
    "\n",
    "# Extract a random row for each unique ID for x and y\n",
    "x_unique = filtered_df[filtered_df['author_id'].isin(x_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=6)).reset_index(drop=True)\n",
    "y_unique = filtered_df[filtered_df['author_id'].isin(y_unique_ids)].groupby('author_id').apply(lambda group: group.sample(1, random_state=7)).reset_index(drop=True)\n",
    "\n",
    "# Combine common and unique rows\n",
    "x = pd.concat([x_common, x_unique]).reset_index(drop=True)\n",
    "y = pd.concat([y_common, y_unique]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2d139-138a-4dcf-825c-afe51691608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e987a-1d63-4e11-ab3b-2cf4f1f0d51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = y.drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67449e8-b107-4920-98e6-302a20778319",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b54bc-ed13-4763-b93a-a185baa51dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd7a12-bae9-4dff-b55b-29cce51bcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the blogger docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60636e-ce51-4c08-a832-f83f12bc708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(x, known_raw_loc)\n",
    "read_and_write_docs.save_as_jsonl(y, unknown_raw_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f58da2-40b4-480e-beec-63d415d141aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be944d04-3afa-4378-98e3-b921bac4fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = preprocessing.apply_sentence_split(x)\n",
    "known = preprocessing.split_rows_by_word_count(known, num_words=250)\n",
    "\n",
    "unknown = preprocessing.apply_sentence_split(y)\n",
    "unknown = preprocessing.split_rows_by_word_count(unknown, num_words=250)\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known, known_preprocessed_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown, unknown_preprocessed_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed046e9-49a2-4f53-bc78-0a16aa5ec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bda1b3-995e-4a8e-9ff4-1b35e15f11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_combined = combine_sentences.concatenate_sentences(known, length_threshold=250, threshold_type='word')\n",
    "unknown_combined = combine_sentences.concatenate_sentences(unknown, length_threshold=250, threshold_type='word')\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known_combined, known_combined_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown_combined, unknown_combined_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cdb3b-d9ff-4358-ad43-002fc8250a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate row counts for x and y by sample_id, author_id, and topic\n",
    "known_counts = known_combined.groupby(['author_id', 'topic']).size().reset_index(name='row_count_known')\n",
    "unknown_counts = unknown_combined.groupby(['author_id', 'topic']).size().reset_index(name='row_count_unknown')\n",
    "\n",
    "# Rename columns in x and y\n",
    "known_counts.rename(columns={'author_id': 'author_known', 'topic': 'topic_known'}, inplace=True)\n",
    "unknown_counts.rename(columns={'author_id': 'author_unknown', 'topic': 'topic_unknown'}, inplace=True)\n",
    "\n",
    "\n",
    "author_id_x = x['author_id'].tolist()\n",
    "author_id_y = y['author_id'].tolist()\n",
    "\n",
    "known_counts = known_counts.sort_values(by=['author_known'],\n",
    "                                        key=lambda col: col.map({val: i for i, val in enumerate(author_id_x)}))\n",
    "\n",
    "unknown_counts = unknown_counts.sort_values(by=['author_unknown'],\n",
    "                                            key=lambda col: col.map({val: i for i, val in enumerate(author_id_y)}))\n",
    "\n",
    "known_counts['sample_id'] = range(1,len(known_counts) + 1)\n",
    "unknown_counts['sample_id'] = range(1,len(unknown_counts) + 1)\n",
    "\n",
    "# Merge x and y on sample_id\n",
    "metadata = pd.merge(known_counts, unknown_counts, on='sample_id', how='inner')\n",
    "\n",
    "metadata['same_author'] = metadata['author_known'] == metadata['author_unknown']\n",
    "metadata['total_comparisons'] = metadata['row_count_known'] * metadata['row_count_unknown']\n",
    "metadata = metadata[['sample_id', 'author_known', 'author_unknown', 'same_author',\n",
    "                     'topic_known', 'topic_unknown', 'row_count_known', 'row_count_unknown',\n",
    "                     'total_comparisons']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccfbe2-61e9-43c7-8d10-34684beec080",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c6303-7b8b-476c-bc62-40687d0e267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(metadata, metadata_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para_llm",
   "language": "python",
   "name": "para_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
