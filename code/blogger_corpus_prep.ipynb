{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a333bdd0-9f58-480c-996e-81f7ac523d01",
   "metadata": {},
   "source": [
    "This notebook completes all of the steps to create a sample of the blogger corpus with even same and different authors, then it preprocesses the text, chunks it and gathers the metadata. The script saves at each point. I have not functionised it yet but could be done for a larger sample or for the PAN data which is effectively the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88e50ea-de94-4d08-8829-3be42b30646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import read_and_write_docs\n",
    "import preprocessing\n",
    "import combine_sentences\n",
    "import combine_rephrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6cd884-01d4-4e6a-adaa-0785e5988086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count words in text\n",
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0dcf9f-d5a3-47a8-b8e5-9b50a91d1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataframe(filtered_df, sample_num):\n",
    "    # Step 1: Group by 'author_id' and 'topic'\n",
    "    grouped = filtered_df.groupby(['author_id', 'topic'])\n",
    "\n",
    "    # Step 2: Filter groups that have at least two different documents\n",
    "    eligible_groups = [group for _, group in grouped if group['id'].nunique() > 1]\n",
    "\n",
    "    # Flatten the list of eligible groups into a single DataFrame\n",
    "    eligible_df = pd.concat(eligible_groups)\n",
    "\n",
    "    # Initialize lists to collect rows for x and y\n",
    "    x_rows = []\n",
    "    y_rows = []\n",
    "\n",
    "    # Sample pairs ensuring different documents from the same author with the same topic\n",
    "    authors_sampled = set()\n",
    "    pairs_count = 0\n",
    "\n",
    "    for (author_id, topic), group in eligible_df.groupby(['author_id', 'topic']):\n",
    "        unique_docs = group['id'].unique()\n",
    "        if len(unique_docs) >= 2:\n",
    "            # Get two different documents\n",
    "            doc1, doc2 = np.random.choice(unique_docs, 2, replace=False)\n",
    "            sample1 = group[group['id'] == doc1].sample(1)\n",
    "            sample2 = group[group['id'] == doc2].sample(1)\n",
    "            authors_sampled.add(author_id)\n",
    "            x_rows.append(sample1)\n",
    "            y_rows.append(sample2)\n",
    "            pairs_count += 1\n",
    "            if pairs_count >= sample_num // 2:\n",
    "                break\n",
    "\n",
    "    # Step 3: Concatenate the sampled rows into DataFrames\n",
    "    x = pd.concat(x_rows).reset_index(drop=True)\n",
    "    y = pd.concat(y_rows).reset_index(drop=True)\n",
    "\n",
    "    # Step 4: Exclude authors already sampled\n",
    "    remaining_df = filtered_df[~filtered_df['author_id'].isin(authors_sampled)]\n",
    "\n",
    "    # Step 5: Sample pairs from different authors with the same topic\n",
    "    remaining_grouped = remaining_df.groupby('topic')\n",
    "\n",
    "    different_author_pairs_count = 0\n",
    "\n",
    "    for topic, group in remaining_grouped:\n",
    "        unique_authors = group['author_id'].unique()\n",
    "        while different_author_pairs_count < sample_num // 2:\n",
    "            if len(unique_authors) < 2:\n",
    "                break\n",
    "            author1, author2 = np.random.choice(unique_authors, 2, replace=False)\n",
    "            sample1 = group[group['author_id'] == author1].sample(1)\n",
    "            sample2 = group[group['author_id'] == author2].sample(1)\n",
    "            x = pd.concat([x, sample1]).reset_index(drop=True)\n",
    "            y = pd.concat([y, sample2]).reset_index(drop=True)\n",
    "            different_author_pairs_count += 1\n",
    "            unique_authors = unique_authors[unique_authors != author1]\n",
    "            unique_authors = unique_authors[unique_authors != author2]\n",
    "            if different_author_pairs_count >= sample_num // 2:\n",
    "                break\n",
    "\n",
    "    # Ensure we have exactly sample_num rows in x and y\n",
    "    x = x.head(sample_num)\n",
    "    y = y.head(sample_num)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def create_metadata(x, y):\n",
    "    # Step 1: Add a new index called sample_id to x and y, and ensure it is the first column\n",
    "    x['sample_id'] = range(1, len(x) + 1)\n",
    "    y['sample_id'] = range(1, len(y) + 1)\n",
    "\n",
    "    # Move sample_id to the first column\n",
    "    x = x[['sample_id'] + [col for col in x.columns if col != 'sample_id']]\n",
    "    y = y[['sample_id'] + [col for col in y.columns if col != 'sample_id']]\n",
    "\n",
    "    x.rename(columns={'id': 'doc_id'}, inplace=True)\n",
    "    y.rename(columns={'id': 'doc_id'}, inplace=True)\n",
    "\n",
    "    # Step 2: Rename columns in x and y to add _x and _y suffixes respectively\n",
    "    x = x.add_suffix('_x')\n",
    "    y = y.add_suffix('_y')\n",
    "\n",
    "    # Rename sample_id columns back to sample_id (they were also suffixed)\n",
    "    x.rename(columns={'sample_id_x': 'sample_id'}, inplace=True)\n",
    "    y.rename(columns={'sample_id_y': 'sample_id'}, inplace=True)\n",
    "\n",
    "    # Step 3: Join the two tables on sample_id\n",
    "    metadata = pd.merge(x, y, on='sample_id')\n",
    "\n",
    "    # Step 4: Create new columns same_author and same_topic\n",
    "    metadata['same_author'] = metadata['author_id_x'] == metadata['author_id_y']\n",
    "    metadata['same_topic'] = metadata['topic_x'] == metadata['topic_y']\n",
    "\n",
    "    # Step 5: Keep only the required columns\n",
    "    metadata = metadata[['sample_id', 'doc_id_x', 'doc_id_y', 'author_id_x',\n",
    "                         'author_id_y', 'topic_x', 'topic_y', 'same_author', 'same_topic']]\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a11296e-1051-4245-9b92-15b29319bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loc = \"/Users/user/Downloads/blogtext.csv\"\n",
    "save_base_loc = \"/Users/user/Documents/datasets/blogger\"\n",
    "\n",
    "# Raw for split data, then preprocessed is converted to sentences and combined is\n",
    "# chunked by word count\n",
    "raw_lsave_loc = f\"{save_base_loc}/raw.jsonl\"\n",
    "\n",
    "known_raw_loc = f\"{save_base_loc}/known_raw.jsonl\"\n",
    "known_preprocessed_loc = f\"{save_base_loc}/known_preprocessed.jsonl\"\n",
    "known_combined_loc = f\"{save_base_loc}/known_combined.jsonl\"\n",
    "known_final_loc = f\"{save_base_loc}/known_final.jsonl\"\n",
    "\n",
    "unknown_raw_loc = f\"{save_base_loc}/unknown_raw.jsonl\"\n",
    "unknown_preprocessed_loc = f\"{save_base_loc}/unknown_preprocessed.jsonl\"\n",
    "unknown_combined_loc = f\"{save_base_loc}/unknown_combined.jsonl\"\n",
    "unknown_final_loc = f\"{save_base_loc}/unknown_final.jsonl\"\n",
    "\n",
    "rephrased_preprocessed_loc = f\"{save_base_loc}/rephrased_preprocessed.jsonl\"\n",
    "\n",
    "metadata_loc = f\"{save_base_loc}/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96de2b31-3efe-445c-b9fe-68551cc3c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_loc)\n",
    "# Remove any whitespace from the column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80de9667-0670-441f-8457-ef012b0cfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(count_words)\n",
    "df['author_id'] = df['id']\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'author_id'] + [col for col in df.columns if col not in ['id', 'author_id']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3631f9b-ace0-4f62-9332-d671d7160ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['word_count'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bededa4-14aa-4d07-b4bf-c3215582eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sample_dataframe(filtered_df, 200)\n",
    "metadata = create_metadata(x, y)\n",
    "read_and_write_docs.save_as_jsonl(metadata, metadata_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f2d139-138a-4dcf-825c-afe51691608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(columns=\"word_count\")\n",
    "y = y.drop(columns=\"word_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6afd7a12-bae9-4dff-b55b-29cce51bcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the blogger docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f60636e-ce51-4c08-a832-f83f12bc708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(x, known_raw_loc)\n",
    "read_and_write_docs.save_as_jsonl(y, unknown_raw_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7f58da2-40b4-480e-beec-63d415d141aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be944d04-3afa-4378-98e3-b921bac4fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = preprocessing.apply_sentence_split(x)\n",
    "known = preprocessing.split_rows_by_word_count(known, num_words=250)\n",
    "\n",
    "unknown = preprocessing.apply_sentence_split(y)\n",
    "unknown = preprocessing.split_rows_by_word_count(unknown, num_words=250)\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known, known_preprocessed_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown, unknown_preprocessed_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed046e9-49a2-4f53-bc78-0a16aa5ec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7bda1b3-995e-4a8e-9ff4-1b35e15f11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_combined = combine_sentences.concatenate_sentences(known, length_threshold=500, threshold_type='word')\n",
    "unknown_combined = combine_sentences.concatenate_sentences(unknown, length_threshold=500, threshold_type='word')\n",
    "\n",
    "read_and_write_docs.save_as_jsonl(known_combined, known_combined_loc)\n",
    "read_and_write_docs.save_as_jsonl(unknown_combined, unknown_combined_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
