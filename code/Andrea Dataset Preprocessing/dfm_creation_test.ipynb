{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f95d820-5c47-4d91-86bb-e6afaf83c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77586a14-5e77-4c2a-b0c0-0e02b45ffe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counts(df, text_column, n_char=4, remove_punct_first=True):\n",
    "    \"\"\"\n",
    "    Compute global feature counts from the text in a pandas DataFrame.\n",
    "    \n",
    "    For each document, two types of tokens are generated:\n",
    "      1. Character n-grams from the text (with spaces replaced by underscores) \n",
    "         where n-grams from within longer words are unpadded.\n",
    "      2. Word tokens for words with length <= n_char, which are padded with underscores \n",
    "         on both sides (e.g. \"bye\" becomes \"_bye_\").\n",
    "    \n",
    "    If a short word token qualifies (length <= n_char), only the padded version is retained.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing text data.\n",
    "    text_column : str\n",
    "        Name of the column with text.\n",
    "    n_char : int, optional\n",
    "        Number of characters in the n-grams (default is 4).\n",
    "    remove_punct_first : bool, optional\n",
    "        If True, remove punctuation from the text (default is True).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    feature_counts : dict\n",
    "        A dictionary mapping feature tokens to their global counts.\n",
    "    \"\"\"\n",
    "    # Compile regex patterns.\n",
    "    token_pattern = re.compile(r'^(?:[a-z0-9]+|_[a-z0-9]+_)$')\n",
    "    word_pattern = re.compile(r'\\b[a-z0-9]+\\b')\n",
    "    \n",
    "    global_counts = Counter()\n",
    "    \n",
    "    # Process each document.\n",
    "    for text in df[text_column].astype(str):\n",
    "        if remove_punct_first:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # --- Character n-gram extraction ---\n",
    "        text_mod = text_lower.replace(\" \", \"_\")\n",
    "        char_tokens = []\n",
    "        for i in range(len(text_mod) - n_char + 1):\n",
    "            gram = text_mod[i:i+n_char]\n",
    "            if token_pattern.match(gram):\n",
    "                # If the token is a boundary token (e.g., _bye_), strip the underscores for the char token.\n",
    "                if gram.startswith(\"_\") and gram.endswith(\"_\"):\n",
    "                    token = gram.strip(\"_\")\n",
    "                    if token:  # avoid empty tokens\n",
    "                        char_tokens.append(token)\n",
    "                else:\n",
    "                    char_tokens.append(gram)\n",
    "        char_counts = Counter(char_tokens)\n",
    "        \n",
    "        # --- Word token extraction ---\n",
    "        words = word_pattern.findall(text_lower)\n",
    "        # Pad words with length <= n_char.\n",
    "        word_tokens = [f\"_{word}_\" for word in words if len(word) <= n_char]\n",
    "        word_counts = Counter(word_tokens)\n",
    "        \n",
    "        # --- Filter out unpadded tokens when a padded version exists ---\n",
    "        filtered_char_counts = Counter()\n",
    "        for token, count in char_counts.items():\n",
    "            if len(token) <= n_char and f\"_{token}_\" in word_counts:\n",
    "                continue\n",
    "            filtered_char_counts[token] = count\n",
    "        \n",
    "        # --- Combine counts for the document ---\n",
    "        combined_counts = filtered_char_counts + word_counts\n",
    "        global_counts.update(combined_counts)\n",
    "    \n",
    "    return dict(global_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "395260c1-16d4-4aae-8e92-3ba67d9a0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "def create_temp_doc_id(input_text):\n",
    "    # Extract everything between the brackets\n",
    "    match = re.search(r'\\[(.*?)\\]', input_text)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        # Replace all punctuation and spaces with \"_\"\n",
    "        cleaned_text = re.sub(r'[^\\w]', '_', extracted_text)\n",
    "        # Replace multiple underscores with a single \"_\"\n",
    "        final_text = re.sub(r'_{2,}', '_', cleaned_text)\n",
    "        return final_text.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87c4cd5a-848a-4322-95d3-29b09c94c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"training\"\n",
    "corpus = \"Enron\"\n",
    "\n",
    "raw_data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b3affb1-ba67-46d9-9e0c-a686bb3ae699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/BCross/datasets/author_verification/training/Enron/known_raw.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4b0fad9-0707-4007-81d2-f150f18bf43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(raw_data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5384a7d1-bdda-439a-a7f4-1b42d90d9998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>corpus</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>known [Andy.zipper - Mail_1].txt</td>\n",
       "      <td>And I guess we simply weren't prepared for thi...</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>known [Andy.zipper - Mail_3].txt</td>\n",
       "      <td>Does that mean yes to tax increases as long as...</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>known [Andy.zipper - Mail_4].txt</td>\n",
       "      <td>Go ahead and get set up and coordinate documen...</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>known [Andy.zipper - Mail_5].txt</td>\n",
       "      <td>In addition he will be pursuing the channel pa...</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>known [Barry.tycholiz - Mail_1].txt</td>\n",
       "      <td>the fact it may be the only thing I get out of...</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                doc_id  \\\n",
       "0     known [Andy.zipper - Mail_1].txt   \n",
       "1     known [Andy.zipper - Mail_3].txt   \n",
       "2     known [Andy.zipper - Mail_4].txt   \n",
       "3     known [Andy.zipper - Mail_5].txt   \n",
       "4  known [Barry.tycholiz - Mail_1].txt   \n",
       "\n",
       "                                                text corpus          author  \\\n",
       "0  And I guess we simply weren't prepared for thi...  Enron     Andy.zipper   \n",
       "1  Does that mean yes to tax increases as long as...  Enron     Andy.zipper   \n",
       "2  Go ahead and get set up and coordinate documen...  Enron     Andy.zipper   \n",
       "3  In addition he will be pursuing the channel pa...  Enron     Andy.zipper   \n",
       "4  the fact it may be the only thing I get out of...  Enron  Barry.tycholiz   \n",
       "\n",
       "  texttype  \n",
       "0    known  \n",
       "1    known  \n",
       "2    known  \n",
       "3    known  \n",
       "4    known  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8404983-1c95-4395-9ee1-e294693ee82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = get_feature_counts(df, 'text', n_char=4, remove_punct_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c53f39d-3cda-42e9-a8d0-be968577f955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gues': 20,\n",
       " 'uess': 16,\n",
       " 'simp': 10,\n",
       " 'impl': 16,\n",
       " 'mply': 7,\n",
       " 'eren': 80,\n",
       " 'rent': 118,\n",
       " 'prep': 25,\n",
       " 'repa': 30,\n",
       " 'epar': 54,\n",
       " 'pare': 43,\n",
       " 'ared': 32,\n",
       " 'leve': 24,\n",
       " 'evel': 43,\n",
       " 'hyst': 1,\n",
       " 'yste': 52,\n",
       " 'ster': 91,\n",
       " 'teri': 22,\n",
       " 'eria': 15,\n",
       " 'befo': 73,\n",
       " 'efor': 82,\n",
       " 'fore': 96,\n",
       " 'mili': 10,\n",
       " 'ilit': 99,\n",
       " 'lita': 5,\n",
       " 'itar': 14,\n",
       " 'tary': 9,\n",
       " 'poli': 18,\n",
       " 'olic': 24,\n",
       " 'lice': 6,\n",
       " 'rest': 84,\n",
       " 'esto': 19,\n",
       " 'stor': 53,\n",
       " 'tore': 15,\n",
       " 'ored': 7,\n",
       " 'orde': 45,\n",
       " 'rder': 43,\n",
       " 'thou': 94,\n",
       " 'hous': 131,\n",
       " 'ousa': 1,\n",
       " 'usan': 10,\n",
       " 'sand': 13,\n",
       " 'ands': 26,\n",
       " 'fran': 13,\n",
       " 'rant': 57,\n",
       " 'anti': 36,\n",
       " 'ntic': 11,\n",
       " 'tica': 27,\n",
       " 'ical': 87,\n",
       " 'ally': 208,\n",
       " 'spec': 90,\n",
       " 'pecu': 2,\n",
       " 'ecul': 2,\n",
       " 'cula': 28,\n",
       " 'ulat': 60,\n",
       " 'lati': 78,\n",
       " 'atin': 67,\n",
       " 'ting': 417,\n",
       " 'yout': 10,\n",
       " 'outh': 15,\n",
       " 'uths': 1,\n",
       " 'drov': 2,\n",
       " 'rove': 33,\n",
       " 'nikk': 1,\n",
       " 'ikke': 1,\n",
       " 'kkei': 1,\n",
       " 'aver': 14,\n",
       " 'vera': 53,\n",
       " 'erag': 17,\n",
       " 'rage': 39,\n",
       " '1600': 1,\n",
       " '6000': 2,\n",
       " 'erro': 9,\n",
       " 'rrol': 4,\n",
       " '4000': 4,\n",
       " '000d': 1,\n",
       " 'apro': 2,\n",
       " 'proc': 63,\n",
       " 'roct': 2,\n",
       " 'plea': 329,\n",
       " 'leas': 368,\n",
       " 'ease': 377,\n",
       " 'digi': 2,\n",
       " 'igit': 2,\n",
       " 'gita': 1,\n",
       " 'ital': 22,\n",
       " 'mean': 20,\n",
       " 'eant': 6,\n",
       " 'hedg': 7,\n",
       " 'edge': 26,\n",
       " 'real': 58,\n",
       " 'eall': 58,\n",
       " 'othe': 204,\n",
       " 'ther': 591,\n",
       " 'fami': 24,\n",
       " 'amil': 25,\n",
       " 'mily': 20,\n",
       " 'goin': 148,\n",
       " 'oing': 185,\n",
       " 'tryi': 40,\n",
       " 'ryin': 45,\n",
       " 'ying': 134,\n",
       " 'arou': 43,\n",
       " 'roun': 56,\n",
       " 'ound': 130,\n",
       " 'rall': 21,\n",
       " 'grou': 106,\n",
       " 'roup': 98,\n",
       " 'long': 37,\n",
       " 'onge': 24,\n",
       " 'nger': 44,\n",
       " 'inun': 1,\n",
       " 'nund': 1,\n",
       " 'unda': 14,\n",
       " 'ndat': 14,\n",
       " 'date': 53,\n",
       " 'ated': 169,\n",
       " 'ereq': 1,\n",
       " 'requ': 87,\n",
       " 'eque': 54,\n",
       " 'ques': 190,\n",
       " 'uest': 189,\n",
       " 'ests': 22,\n",
       " 'addi': 52,\n",
       " 'ddit': 51,\n",
       " 'diti': 70,\n",
       " 'itio': 136,\n",
       " 'tion': 1279,\n",
       " 'iona': 85,\n",
       " 'onal': 93,\n",
       " 'mail': 85,\n",
       " 'ailb': 3,\n",
       " 'ilbo': 3,\n",
       " 'lbox': 4,\n",
       " 'spac': 12,\n",
       " 'pace': 12,\n",
       " 'caro': 15,\n",
       " 'arol': 17,\n",
       " 'sist': 61,\n",
       " 'iste': 43,\n",
       " 'name': 22,\n",
       " 'amed': 10,\n",
       " 'marr': 13,\n",
       " 'arri': 38,\n",
       " 'rrie': 19,\n",
       " 'ried': 30,\n",
       " 'stev': 12,\n",
       " 'teve': 20,\n",
       " 'oust': 90,\n",
       " 'usto': 127,\n",
       " 'ston': 106,\n",
       " 'frie': 35,\n",
       " 'rien': 59,\n",
       " 'iend': 35,\n",
       " 'woma': 15,\n",
       " 'oman': 15,\n",
       " 'soun': 29,\n",
       " 'unde': 156,\n",
       " 'nded': 45,\n",
       " 'rols': 8,\n",
       " 'unds': 28,\n",
       " 'comp': 262,\n",
       " 'ompl': 65,\n",
       " 'mpli': 11,\n",
       " 'plic': 34,\n",
       " 'lica': 43,\n",
       " 'icat': 77,\n",
       " 'cate': 48,\n",
       " 'fari': 2,\n",
       " 'arin': 30,\n",
       " 'ring': 152,\n",
       " 'rece': 113,\n",
       " 'ecen': 28,\n",
       " 'cent': 81,\n",
       " 'even': 43,\n",
       " 'vent': 67,\n",
       " 'ents': 188,\n",
       " 'norm': 17,\n",
       " 'orma': 111,\n",
       " 'rmal': 16,\n",
       " 'rese': 84,\n",
       " 'eset': 2,\n",
       " 'pric': 119,\n",
       " 'rice': 116,\n",
       " 'unle': 12,\n",
       " 'nles': 12,\n",
       " 'less': 26,\n",
       " 'part': 117,\n",
       " 'arti': 71,\n",
       " 'rtic': 36,\n",
       " 'ticu': 18,\n",
       " 'icul': 38,\n",
       " 'ular': 29,\n",
       " 'refe': 36,\n",
       " 'efer': 41,\n",
       " 'fere': 77,\n",
       " 'renc': 67,\n",
       " 'ence': 110,\n",
       " 'mont': 91,\n",
       " 'onth': 86,\n",
       " 'sett': 32,\n",
       " 'ettl': 22,\n",
       " 'ttle': 86,\n",
       " 'tles': 9,\n",
       " 'abov': 23,\n",
       " 'bove': 23,\n",
       " 'trig': 12,\n",
       " 'rigg': 8,\n",
       " 'igge': 16,\n",
       " 'gger': 11,\n",
       " 'ment': 501,\n",
       " 'enti': 130,\n",
       " 'ntio': 37,\n",
       " 'ione': 19,\n",
       " 'oned': 14,\n",
       " 'spea': 27,\n",
       " 'peak': 29,\n",
       " 'eaki': 12,\n",
       " 'akin': 72,\n",
       " 'king': 295,\n",
       " 'rega': 71,\n",
       " 'egar': 68,\n",
       " 'gard': 69,\n",
       " 'ards': 44,\n",
       " 'esta': 20,\n",
       " 'stat': 114,\n",
       " 'tate': 98,\n",
       " 'want': 38,\n",
       " 'ante': 76,\n",
       " 'nted': 94,\n",
       " 'opin': 25,\n",
       " 'pini': 14,\n",
       " 'inio': 14,\n",
       " 'nion': 17,\n",
       " 'extr': 19,\n",
       " 'xtre': 8,\n",
       " 'trem': 11,\n",
       " 'reme': 57,\n",
       " 'emel': 7,\n",
       " 'mely': 12,\n",
       " 'shar': 43,\n",
       " 'harp': 3,\n",
       " 'capa': 25,\n",
       " 'apab': 8,\n",
       " 'pabl': 6,\n",
       " 'able': 81,\n",
       " 'trad': 147,\n",
       " 'rade': 94,\n",
       " 'ader': 48,\n",
       " 'ders': 100,\n",
       " 'deal': 28,\n",
       " 'ealt': 7,\n",
       " 'nont': 2,\n",
       " 'ontr': 128,\n",
       " 'ntra': 100,\n",
       " 'radi': 62,\n",
       " 'adin': 81,\n",
       " 'ding': 309,\n",
       " 'comm': 177,\n",
       " 'omme': 94,\n",
       " 'mmer': 43,\n",
       " 'merc': 27,\n",
       " 'erci': 24,\n",
       " 'rcia': 17,\n",
       " 'cial': 105,\n",
       " 'situ': 9,\n",
       " 'itua': 12,\n",
       " 'tuat': 10,\n",
       " 'uati': 19,\n",
       " 'atio': 588,\n",
       " 'ions': 427,\n",
       " 'orke': 16,\n",
       " 'rked': 17,\n",
       " 'enro': 175,\n",
       " 'nron': 165,\n",
       " 'rono': 14,\n",
       " 'onon': 14,\n",
       " 'nonl': 14,\n",
       " 'onli': 23,\n",
       " 'nlin': 25,\n",
       " 'line': 64,\n",
       " 'alwa': 20,\n",
       " 'lway': 21,\n",
       " 'ways': 18,\n",
       " 'view': 91,\n",
       " 'iewe': 18,\n",
       " 'ewed': 19,\n",
       " 'folk': 10,\n",
       " 'olks': 10,\n",
       " 'unso': 4,\n",
       " 'nsol': 9,\n",
       " 'soli': 12,\n",
       " 'lici': 14,\n",
       " 'icit': 37,\n",
       " 'cite': 7,\n",
       " 'ited': 38,\n",
       " 'houg': 66,\n",
       " 'ough': 185,\n",
       " 'ught': 75,\n",
       " 'thin': 410,\n",
       " 'hink': 187,\n",
       " 'rela': 58,\n",
       " 'elat': 51,\n",
       " 'late': 84,\n",
       " 'ates': 98,\n",
       " 'gett': 54,\n",
       " 'etti': 70,\n",
       " 'ttin': 93,\n",
       " 'thei': 140,\n",
       " 'heir': 140,\n",
       " 'unre': 9,\n",
       " 'nreg': 2,\n",
       " 'regu': 42,\n",
       " 'egul': 42,\n",
       " 'gula': 43,\n",
       " 'shoc': 4,\n",
       " 'hock': 4,\n",
       " 'ockl': 1,\n",
       " 'ckle': 3,\n",
       " 'kley': 3,\n",
       " 'vand': 4,\n",
       " 'ande': 23,\n",
       " 'nder': 226,\n",
       " 'derw': 10,\n",
       " 'erwa': 9,\n",
       " 'rwal': 1,\n",
       " 'wald': 1,\n",
       " 'repo': 57,\n",
       " 'epor': 57,\n",
       " 'port': 213,\n",
       " 'pres': 105,\n",
       " 'ress': 99,\n",
       " 'rele': 30,\n",
       " 'elea': 21,\n",
       " 'akes': 18,\n",
       " 'read': 92,\n",
       " 'eadi': 24,\n",
       " 'inte': 199,\n",
       " 'nter': 222,\n",
       " 'tere': 85,\n",
       " 'eres': 77,\n",
       " 'este': 84,\n",
       " 'sted': 75,\n",
       " 'wher': 70,\n",
       " 'cont': 237,\n",
       " 'ntri': 16,\n",
       " 'trib': 34,\n",
       " 'ribu': 35,\n",
       " 'ibut': 34,\n",
       " 'buti': 25,\n",
       " 'utio': 45,\n",
       " 'john': 4,\n",
       " 'ohna': 2,\n",
       " 'hnat': 2,\n",
       " 'nath': 3,\n",
       " 'atha': 3,\n",
       " 'hans': 2,\n",
       " 'deat': 3,\n",
       " 'eath': 24,\n",
       " 'cele': 4,\n",
       " 'eleb': 1,\n",
       " 'lebr': 1,\n",
       " 'ebra': 6,\n",
       " 'brat': 1,\n",
       " 'rati': 97,\n",
       " 'jona': 1,\n",
       " 'onat': 5,\n",
       " 'mmen': 76,\n",
       " 'brou': 6,\n",
       " 'roug': 89,\n",
       " 'alls': 13,\n",
       " 'resi': 28,\n",
       " 'esig': 25,\n",
       " 'sign': 103,\n",
       " 'igna': 18,\n",
       " 'gnat': 16,\n",
       " 'nati': 96,\n",
       " 'expl': 18,\n",
       " 'xpla': 8,\n",
       " 'plan': 59,\n",
       " 'lana': 4,\n",
       " 'anat': 4,\n",
       " 'goeh': 1,\n",
       " 'oehl': 1,\n",
       " 'ehle': 1,\n",
       " 'hler': 1,\n",
       " 'onte': 14,\n",
       " 'ntex': 2,\n",
       " 'text': 1,\n",
       " 'atem': 19,\n",
       " 'teme': 18,\n",
       " 'emen': 224,\n",
       " 'atta': 85,\n",
       " 'ttac': 82,\n",
       " 'tack': 5,\n",
       " 'acks': 9,\n",
       " 'aime': 6,\n",
       " 'imed': 4,\n",
       " 'symb': 4,\n",
       " 'ymbo': 4,\n",
       " 'mbol': 4,\n",
       " 'boli': 3,\n",
       " 'hear': 39,\n",
       " 'eart': 18,\n",
       " 'amer': 24,\n",
       " 'meri': 24,\n",
       " 'rica': 25,\n",
       " 'towe': 1,\n",
       " 'ower': 109,\n",
       " 'wers': 6,\n",
       " 'phal': 1,\n",
       " 'hall': 67,\n",
       " 'alli': 11,\n",
       " 'llic': 1,\n",
       " 'bols': 2,\n",
       " 'answ': 24,\n",
       " 'nswe': 24,\n",
       " 'swer': 23,\n",
       " 'esti': 202,\n",
       " 'stio': 151,\n",
       " 'imag': 6,\n",
       " 'mage': 20,\n",
       " 'ages': 35,\n",
       " 'remi': 14,\n",
       " 'emin': 7,\n",
       " 'mind': 8,\n",
       " 'coul': 149,\n",
       " 'ould': 656,\n",
       " 'ardi': 60,\n",
       " 'rdin': 83,\n",
       " 'week': 38,\n",
       " 'eeks': 38,\n",
       " 'behi': 5,\n",
       " 'ehin': 5,\n",
       " 'hind': 6,\n",
       " 'indt': 1,\n",
       " 'ndth': 1,\n",
       " 'dthe': 1,\n",
       " 'theh': 2,\n",
       " 'hehe': 1,\n",
       " 'ehea': 1,\n",
       " 'head': 25,\n",
       " 'guit': 2,\n",
       " 'uita': 2,\n",
       " 'wick': 5,\n",
       " 'icke': 54,\n",
       " 'cked': 19,\n",
       " 'jimm': 1,\n",
       " 'immy': 1,\n",
       " 'hetz': 1,\n",
       " 'etze': 1,\n",
       " 'tzel': 1,\n",
       " 'chil': 9,\n",
       " 'hild': 9,\n",
       " 'ildr': 7,\n",
       " 'ldre': 7,\n",
       " 'dren': 7,\n",
       " 'blow': 1,\n",
       " 'lown': 2,\n",
       " 'fret': 1,\n",
       " 'retb': 1,\n",
       " 'etbo': 2,\n",
       " 'tboa': 1,\n",
       " 'boar': 22,\n",
       " 'oard': 23,\n",
       " 'wiza': 1,\n",
       " 'izar': 1,\n",
       " 'zard': 1,\n",
       " 'ardr': 1,\n",
       " 'rdry': 1,\n",
       " 'hing': 233,\n",
       " 'betw': 39,\n",
       " 'etwe': 39,\n",
       " 'twee': 40,\n",
       " 'ween': 41,\n",
       " 'play': 19,\n",
       " 'laye': 14,\n",
       " 'ayed': 13,\n",
       " 'sorr': 18,\n",
       " 'orry': 23,\n",
       " 'didn': 23,\n",
       " 'idnt': 23,\n",
       " 'mess': 42,\n",
       " 'essa': 60,\n",
       " 'ssag': 40,\n",
       " 'sage': 48,\n",
       " 'lets': 1,\n",
       " 'etss': 1,\n",
       " 'dinn': 25,\n",
       " 'inne': 29,\n",
       " 'nner': 37,\n",
       " 'tues': 23,\n",
       " 'uesd': 22,\n",
       " 'esda': 54,\n",
       " 'sday': 73,\n",
       " 'orks': 37,\n",
       " 'moni': 8,\n",
       " 'onic': 6,\n",
       " 'nica': 32,\n",
       " 'ings': 131,\n",
       " 'litt': 52,\n",
       " 'ittl': 51,\n",
       " 'unse': 28,\n",
       " 'nset': 1,\n",
       " 'tled': 9,\n",
       " 'righ': 72,\n",
       " 'ight': 241,\n",
       " 'orki': 50,\n",
       " 'rkin': 59,\n",
       " 'omet': 62,\n",
       " 'meth': 52,\n",
       " 'ethi': 49,\n",
       " 'shor': 43,\n",
       " 'hort': 41,\n",
       " 'ortl': 21,\n",
       " 'rtly': 9,\n",
       " 'coup': 29,\n",
       " 'oupl': 28,\n",
       " 'uple': 28,\n",
       " 'prob': 91,\n",
       " 'roba': 42,\n",
       " 'obab': 40,\n",
       " 'babl': 40,\n",
       " 'abli': 10,\n",
       " 'blil': 1,\n",
       " 'lili': 1,\n",
       " 'lity': 98,\n",
       " 'firs': 67,\n",
       " 'irst': 68,\n",
       " 'quar': 26,\n",
       " 'uart': 26,\n",
       " 'arte': 44,\n",
       " 'rter': 29,\n",
       " 'brea': 25,\n",
       " 'reak': 24,\n",
       " 'anyt': 52,\n",
       " 'nyth': 50,\n",
       " 'ythi': 73,\n",
       " 'spok': 31,\n",
       " 'poke': 30,\n",
       " 'stuc': 1,\n",
       " 'tuck': 2,\n",
       " 'ucke': 4,\n",
       " 'ckey': 1,\n",
       " 'advi': 21,\n",
       " 'dvis': 20,\n",
       " 'vise': 39,\n",
       " 'ised': 34,\n",
       " 'seve': 30,\n",
       " 'rriv': 13,\n",
       " 'rivi': 13,\n",
       " 'ivin': 33,\n",
       " 'ving': 111,\n",
       " 'toda': 77,\n",
       " 'oday': 75,\n",
       " 'woul': 368,\n",
       " 'heav': 5,\n",
       " 'eave': 48,\n",
       " 'peci': 49,\n",
       " 'ecif': 25,\n",
       " 'cifi': 32,\n",
       " 'ific': 64,\n",
       " 'disc': 117,\n",
       " 'iscu': 89,\n",
       " 'scus': 90,\n",
       " 'cuss': 91,\n",
       " 'ussi': 32,\n",
       " 'ssio': 94,\n",
       " 'sion': 174,\n",
       " 'form': 111,\n",
       " 'tech': 21,\n",
       " 'echn': 17,\n",
       " 'chno': 9,\n",
       " 'hnol': 9,\n",
       " 'nolo': 10,\n",
       " 'olog': 19,\n",
       " 'logy': 11,\n",
       " 'star': 56,\n",
       " 'tart': 53,\n",
       " 'rtin': 23,\n",
       " 'mond': 57,\n",
       " 'onda': 56,\n",
       " 'nday': 62,\n",
       " 'morn': 67,\n",
       " 'orni': 114,\n",
       " 'rnin': 95,\n",
       " 'ning': 205,\n",
       " 'adiv': 1,\n",
       " 'divs': 1,\n",
       " 'ivse': 1,\n",
       " 'vsed': 1,\n",
       " 'amen': 35,\n",
       " 'mend': 61,\n",
       " 'endi': 41,\n",
       " 'ndin': 96,\n",
       " 'orig': 28,\n",
       " 'rigi': 30,\n",
       " 'igin': 30,\n",
       " 'gina': 36,\n",
       " 'inal': 53,\n",
       " 'shee': 34,\n",
       " 'heet': 33,\n",
       " 'disu': 1,\n",
       " 'isuc': 1,\n",
       " 'sucs': 1,\n",
       " 'ucss': 1,\n",
       " 'csse': 1,\n",
       " 'ssed': 41,\n",
       " 'satu': 36,\n",
       " 'atur': 84,\n",
       " 'turd': 37,\n",
       " 'urda': 38,\n",
       " 'rday': 68,\n",
       " 'nigh': 91,\n",
       " '_and_': 1975,\n",
       " '_i_': 1852,\n",
       " '_we_': 682,\n",
       " '_for_': 1041,\n",
       " '_this_': 770,\n",
       " '_of_': 1543,\n",
       " '_fan_': 3,\n",
       " '_the_': 3810,\n",
       " '_past_': 26,\n",
       " '_andy_': 10,\n",
       " '_bot_': 1,\n",
       " '_02_': 1,\n",
       " '_2_': 39,\n",
       " '_put_': 46,\n",
       " '_in_': 1163,\n",
       " '_same_': 38,\n",
       " '_book_': 53,\n",
       " '_as_': 440,\n",
       " '_it_': 555,\n",
       " '_is_': 907,\n",
       " '_to_': 2872,\n",
       " '_dont_': 114,\n",
       " '_know_': 336,\n",
       " '_than_': 87,\n",
       " '_he_': 276,\n",
       " '_was_': 335,\n",
       " '_out_': 194,\n",
       " '_some_': 168,\n",
       " '_time_': 157,\n",
       " '_its_': 123,\n",
       " '_okay_': 9,\n",
       " '_not_': 358,\n",
       " '_give_': 121,\n",
       " '_back_': 111,\n",
       " '_too_': 43,\n",
       " '_much_': 76,\n",
       " '_chop_': 1,\n",
       " '_work_': 114,\n",
       " '_with_': 626,\n",
       " '_so_': 223,\n",
       " '_that_': 927,\n",
       " '_am_': 224,\n",
       " '_no_': 98,\n",
       " '_does_': 52,\n",
       " '_have_': 755,\n",
       " '_a_': 1524,\n",
       " '_mimi_': 1,\n",
       " '_who_': 102,\n",
       " '_guy_': 10,\n",
       " '_here_': 94,\n",
       " '_guys_': 42,\n",
       " '_best_': 33,\n",
       " '_like_': 189,\n",
       " '_hope_': 56,\n",
       " '_you_': 1369,\n",
       " '_are_': 506,\n",
       " '_swap_': 8,\n",
       " '_at_': 417,\n",
       " '_or_': 330,\n",
       " '_dave_': 38,\n",
       " '_mike_': 24,\n",
       " '_just_': 162,\n",
       " '_let_': 222,\n",
       " '_very_': 71,\n",
       " '_high_': 26,\n",
       " '_him_': 76,\n",
       " '_one_': 143,\n",
       " '_more_': 127,\n",
       " '_ive_': 30,\n",
       " '_when_': 175,\n",
       " '_me_': 566,\n",
       " '_my_': 292,\n",
       " '_do_': 209,\n",
       " '_what_': 196,\n",
       " '_will_': 570,\n",
       " '_but_': 317,\n",
       " '_if_': 555,\n",
       " '_paul_': 11,\n",
       " '_top_': 14,\n",
       " '_job_': 24,\n",
       " '_soon_': 52,\n",
       " '_be_': 677,\n",
       " '_spun_': 1,\n",
       " '_off_': 58,\n",
       " '_unit_': 18,\n",
       " '_went_': 28,\n",
       " '_eric_': 8,\n",
       " '_fun_': 12,\n",
       " '_greg_': 17,\n",
       " '_make_': 100,\n",
       " '_wake_': 2,\n",
       " '_uman_': 1,\n",
       " '_j_': 4,\n",
       " '_her_': 80,\n",
       " '_also_': 154,\n",
       " '_an_': 237,\n",
       " '_from_': 314,\n",
       " '_were_': 134,\n",
       " '_made_': 60,\n",
       " '_she_': 56,\n",
       " '_said_': 65,\n",
       " '_sort_': 7,\n",
       " '_did_': 51,\n",
       " '_jeff_': 44,\n",
       " '_call_': 165,\n",
       " '_last_': 90,\n",
       " '_his_': 138,\n",
       " '_solo_': 1,\n",
       " '_11_': 8,\n",
       " '_six_': 9,\n",
       " '_away_': 27,\n",
       " '_by_': 240,\n",
       " '_vais_': 1,\n",
       " '_held_': 8,\n",
       " '_legs_': 4,\n",
       " '_bow_': 1,\n",
       " '_get_': 251,\n",
       " '_next_': 95,\n",
       " '_bit_': 11,\n",
       " '_now_': 81,\n",
       " '_on_': 733,\n",
       " '_them_': 136,\n",
       " '_told_': 38,\n",
       " '_your_': 361,\n",
       " '_mom_': 6,\n",
       " '_ago_': 18,\n",
       " '_they_': 210,\n",
       " '_file_': 26,\n",
       " '_chpt_': 1,\n",
       " '_end_': 51,\n",
       " '_well_': 78,\n",
       " '_rick_': 7,\n",
       " '_pro_': 1,\n",
       " '_term_': 25,\n",
       " 'incr': 35,\n",
       " 'ncre': 35,\n",
       " 'crea': 66,\n",
       " 'reas': 84,\n",
       " 'ases': 25,\n",
       " 'aliv': 4,\n",
       " 'live': 46,\n",
       " 'grat': 27,\n",
       " 'atif': 1,\n",
       " 'tifi': 23,\n",
       " 'ifie': 25,\n",
       " 'fied': 27,\n",
       " 'orpo': 24,\n",
       " 'rpor': 30,\n",
       " 'orta': 30,\n",
       " 'rtae': 1,\n",
       " 'cust': 37,\n",
       " 'stom': 37,\n",
       " 'tome': 34,\n",
       " 'omer': 36,\n",
       " 'mers': 35,\n",
       " 'talk': 25,\n",
       " 'alke': 24,\n",
       " 'lked': 24,\n",
       " 'abou': 198,\n",
       " 'bout': 199,\n",
       " 'find': 6,\n",
       " 'indi': 52,\n",
       " 'valu': 53,\n",
       " 'alue': 40,\n",
       " 'prod': 73,\n",
       " 'rodu': 79,\n",
       " 'oduc': 79,\n",
       " 'duct': 72,\n",
       " 'serv': 84,\n",
       " 'ervi': 65,\n",
       " 'rvic': 38,\n",
       " 'vice': 38,\n",
       " 'idea': 10,\n",
       " 'oppo': 37,\n",
       " 'ppor': 70,\n",
       " 'ortu': 53,\n",
       " 'rtun': 53,\n",
       " 'tuni': 33,\n",
       " 'unit': 48,\n",
       " 'nity': 34,\n",
       " 'merg': 15,\n",
       " 'erge': 17,\n",
       " 'offi': 85,\n",
       " 'ffic': 114,\n",
       " 'fice': 71,\n",
       " 'supp': 80,\n",
       " 'uppo': 38,\n",
       " 'plat': 14,\n",
       " 'latf': 5,\n",
       " 'atfo': 5,\n",
       " 'tfor': 6,\n",
       " 'orms': 11,\n",
       " 'usse': 22,\n",
       " 'imes': 24,\n",
       " 'ronn': 1,\n",
       " 'onne': 15,\n",
       " 'nnet': 3,\n",
       " 'netw': 14,\n",
       " 'etwo': 14,\n",
       " 'twor': 15,\n",
       " 'prac': 20,\n",
       " 'ract': 118,\n",
       " 'acti': 168,\n",
       " 'ctic': 24,\n",
       " 'ommo': 19,\n",
       " 'mmod': 14,\n",
       " 'modi': 19,\n",
       " 'odit': 13,\n",
       " 'dity': 13,\n",
       " 'ityl': 1,\n",
       " 'tylo': 1,\n",
       " 'ylog': 1,\n",
       " 'logi': 28,\n",
       " 'ogic': 7,\n",
       " 'resu': 50,\n",
       " 'esum': 16,\n",
       " 'sump': 8,\n",
       " 'umpt': 8,\n",
       " 'mptu': 2,\n",
       " 'ptuo': 2,\n",
       " 'tuou': 2,\n",
       " 'uous': 3,\n",
       " 'hadn': 7,\n",
       " 'adnt': 7,\n",
       " 'whil': 61,\n",
       " 'hile': 61,\n",
       " 'ches': 10,\n",
       " 'hest': 4,\n",
       " 'moon': 1,\n",
       " 'oone': 7,\n",
       " 'oney': 33,\n",
       " 'neys': 5,\n",
       " 'grea': 77,\n",
       " 'reat': 124,\n",
       " 'lane': 9,\n",
       " 'anes': 3,\n",
       " 'buil': 33,\n",
       " 'uilt': 8,\n",
       " 'inci': 17,\n",
       " 'ncid': 9,\n",
       " 'cide': 33,\n",
       " 'iden': 68,\n",
       " 'dent': 87,\n",
       " 'infl': 7,\n",
       " 'nfli': 9,\n",
       " 'flig': 3,\n",
       " 'ligh': 22,\n",
       " 'airf': 3,\n",
       " 'irfr': 1,\n",
       " 'rfra': 1,\n",
       " 'fram': 8,\n",
       " 'rame': 12,\n",
       " 'fail': 9,\n",
       " 'ailu': 3,\n",
       " 'ilur': 3,\n",
       " 'lure': 4,\n",
       " 'draw': 8,\n",
       " 'rawb': 2,\n",
       " 'awba': 2,\n",
       " 'wbac': 2,\n",
       " 'quit': 22,\n",
       " 'uite': 16,\n",
       " 'smal': 24,\n",
       " 'mall': 28,\n",
       " 'here': 147,\n",
       " 'clou': 3,\n",
       " 'loud': 3,\n",
       " 'coma': 3,\n",
       " 'omap': 2,\n",
       " 'mapn': 1,\n",
       " 'apny': 1,\n",
       " 'bank': 11,\n",
       " 'ankr': 7,\n",
       " 'nkru': 7,\n",
       " 'krup': 7,\n",
       " 'rupt': 11,\n",
       " 'uptc': 8,\n",
       " 'ptcy': 8,\n",
       " 'whet': 19,\n",
       " 'heth': 19,\n",
       " 'ethe': 70,\n",
       " 'uppl': 40,\n",
       " 'pply': 35,\n",
       " 'arts': 15,\n",
       " 'avai': 31,\n",
       " 'vail': 33,\n",
       " 'aila': 30,\n",
       " 'ilab': 30,\n",
       " 'labl': 25,\n",
       " 'fowa': 1,\n",
       " 'owar': 10,\n",
       " 'ward': 108,\n",
       " 'ably': 45,\n",
       " 'bein': 48,\n",
       " 'eing': 62,\n",
       " 'isco': 19,\n",
       " 'scou': 8,\n",
       " 'coun': 137,\n",
       " 'ount': 141,\n",
       " 'unte': 39,\n",
       " 'whic': 167,\n",
       " 'hich': 167,\n",
       " 'idon': 1,\n",
       " 'issu': 84,\n",
       " 'ssue': 84,\n",
       " 'some': 27,\n",
       " 'omeo': 39,\n",
       " 'meon': 38,\n",
       " 'eone': 38,\n",
       " 'manu': 7,\n",
       " 'anuf': 3,\n",
       " 'nufa': 3,\n",
       " 'ufac': 3,\n",
       " 'actu': 41,\n",
       " 'ctur': 46,\n",
       " 'turi': 4,\n",
       " 'urin': 49,\n",
       " 'focu': 9,\n",
       " 'ocus': 9,\n",
       " 'clos': 55,\n",
       " 'losi': 12,\n",
       " 'osin': 12,\n",
       " 'sing': 86,\n",
       " 'igne': 39,\n",
       " 'gned': 37,\n",
       " 'ecei': 81,\n",
       " 'ceiv': 72,\n",
       " 'eive': 67,\n",
       " 'ived': 46,\n",
       " 'mult': 10,\n",
       " 'ulti': 29,\n",
       " 'limi': 23,\n",
       " 'imit': 21,\n",
       " 'plac': 51,\n",
       " 'lace': 51,\n",
       " 'aced': 3,\n",
       " 'afte': 130,\n",
       " 'fter': 130,\n",
       " 'frid': 53,\n",
       " 'rida': 56,\n",
       " 'iday': 65,\n",
       " 'migh': 34,\n",
       " 'wipe': 2,\n",
       " 'iped': 2,\n",
       " 'duir': 1,\n",
       " 'uirn': 1,\n",
       " 'irng': 1,\n",
       " 'main': 43,\n",
       " 'aint': 32,\n",
       " 'nten': 35,\n",
       " 'tena': 10,\n",
       " 'enan': 10,\n",
       " 'nanc': 65,\n",
       " 'ance': 136,\n",
       " 'aler': 4,\n",
       " 'lert': 2,\n",
       " 'reco': 55,\n",
       " 'ecom': 58,\n",
       " 'allr': 2,\n",
       " 'llri': 1,\n",
       " 'lrig': 1,\n",
       " 'aven': 29,\n",
       " 'oken': 6,\n",
       " 'awhi': 5,\n",
       " 'chan': 157,\n",
       " 'hanc': 17,\n",
       " 'catc': 10,\n",
       " 'atch': 41,\n",
       " 'wedn': 33,\n",
       " 'edne': 33,\n",
       " 'dnes': 35,\n",
       " 'nesd': 32,\n",
       " 'reac': 22,\n",
       " 'acha': 3,\n",
       " 'chab': 1,\n",
       " 'habl': 1,\n",
       " 'thro': 82,\n",
       " 'hrou': 76,\n",
       " 'enou': 15,\n",
       " 'noug': 15,\n",
       " 'seem': 20,\n",
       " 'eems': 16,\n",
       " 'drac': 1,\n",
       " 'raco': 1,\n",
       " 'acon': 1,\n",
       " 'coni': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac0f24a7-44c8-4190-ac06-212098c39e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'othe': 204, 'ther': 591, 'thei': 140, 'dthe': 1, 'theh': 2, '_the_': 3810, '_them_': 136, '_they_': 210, 'ethe': 70, 'thes': 122, '_then_': 65, 'athe': 56, 'rthe': 41, 'ithe': 23, 'nthe': 5, 'thet': 1, 'they': 2, 'tthe': 1, 'thea': 1, 'thel': 6, 'uthe': 8, 'thew': 1, 'thed': 7, 'them': 1, 'theo': 1}\n"
     ]
    }
   ],
   "source": [
    "# Assume feature_counts is your dictionary of feature counts\n",
    "filtered_feature_counts = {token: count for token, count in feature_counts.items() if \"the\" in token}\n",
    "\n",
    "print(filtered_feature_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fe102f7-a8ca-49ed-b03e-1037e75cdb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1306"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['dfm'].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12ae5bc2-b736-4ca6-8917-58ad09f79cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1306"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56932b8a-75f5-4619-bfb4-7dd1176eb9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
