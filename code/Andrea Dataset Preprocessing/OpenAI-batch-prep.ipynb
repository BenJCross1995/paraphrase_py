{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e1645378-31c9-4c10-a4f8-d251feaaa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c64df5ef-b734-41fd-ab48-f6f766b8cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../read_and_write_docs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "71eaff22-2b8b-4e8b-b1dd-527c5d9978bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(location, exact_name=None):\n",
    "    \"\"\"\n",
    "    Lists all files in the specified location, optionally filtering by file type.\n",
    "\n",
    "    Parameters:\n",
    "    - location (str): The directory to search in.\n",
    "    - file_type (str, optional): The file extension to filter by (e.g., \".jsonl\").\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of full file paths that match the file type.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store file paths\n",
    "    file_list = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file_name in files:\n",
    "            # Match exact file name if specified\n",
    "            if exact_name and file_name == exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "            # If no exact_name is provided, include all files\n",
    "            elif not exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c186247-4a75-468a-89fa-9c71493f8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a paraphrasing assistant. Your task is to generate paraphrased sentences that retain the original meaning, tone, and style but demonstrate maximum lexical and structural variety.\n",
    "Each paraphrase should use distinct vocabulary and sentence structures, prioritizing as much lexical difference as possible.\n",
    "\n",
    "Guidelines:\n",
    "- Create AT LEAST TWENTY unique paraphrases.\n",
    "- Avoid repeating words or phrases across paraphrases, unless they are critical to meaning (e.g., names or specific technical terms).\n",
    "- Use varied synonyms, alter phrasing, and experiment with different sentence structures to ensure each paraphrase feels fresh and unique.\n",
    "- Examples of strategies to achieve this include: using metaphors or idioms, reordering clauses, shifting perspectives, and exploring different grammatical constructions.\n",
    "- Preserve the original intent and style without adding new information or altering names.\n",
    "\n",
    "DO NOT INCLUDE ANY NOTES OR ADDITIONAL TEXT IN THE OUTPUT.\n",
    "\n",
    "Example in JSON format:\n",
    "\n",
    "input: \"Although the skill appears easy at first, it can take a long time to master.\"\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"original\": \"Although the skill appears easy at first, it can take a long time to master.\",\n",
    "  \"paraphrase_1\": \"Initially, the skill may seem effortless, yet true mastery demands a lengthy commitment.\",\n",
    "  \"paraphrase_2\": \"What begins as a simple-looking skill often turns into a time-consuming mastery process.\",\n",
    "  \"paraphrase_3\": \"While appearing simple at the outset, mastering this skill typically requires extended effort.\",\n",
    "  \"paraphrase_4\": \"Despite an easy start, reaching mastery in this skill can be a prolonged journey.\",\n",
    "  \"paraphrase_5\": \"This skill, while seemingly straightforward at first glance, requires considerable time to excel in.\",\n",
    "  \"paraphrase_6\": \"Even if it looks easy at the beginning, achieving expertise in this skill may be time-intensive.\",\n",
    "  \"paraphrase_7\": \"Though simple in appearance, the skill demands time and practice to truly master.\",\n",
    "  \"paraphrase_8\": \"Achieving proficiency in this skill can take substantial time, even if it seems easy initially.\",\n",
    "  \"paraphrase_9\": \"While the skill might look easy at the start, honing it to perfection can require considerable time.\",\n",
    "  \"paraphrase_10\": \"It might seem straightforward to pick up, yet mastering this skill is often a slow process.\",\n",
    "  \"paraphrase_11\": \"Perfecting this seemingly easy skill can actually be a long and demanding task.\",\n",
    "  \"paraphrase_12\": \"Though it appears simple to learn, achieving mastery in this skill often takes a significant amount of time.\",\n",
    "  \"paraphrase_13\": \"Initially, the skill may come across as effortless, but true proficiency is typically time-consuming.\",\n",
    "  \"paraphrase_14\": \"Mastering this skill is a lengthy pursuit, despite its initial simplicity.\",\n",
    "  \"paraphrase_15\": \"While it looks uncomplicated at first, gaining full mastery of this skill can be a long journey.\",\n",
    "  \"paraphrase_16\": \"Even though this skill seems straightforward, becoming proficient usually takes an extended period.\",\n",
    "  \"paraphrase_17\": \"Mastery of this seemingly simple skill often requires more time than one might expect.\",\n",
    "  \"paraphrase_18\": \"Though it may appear easy at first glance, mastering this skill can be a drawn-out process.\",\n",
    "  \"paraphrase_19\": \"Although appearing effortless at first, this skill demands time and patience for true mastery.\",\n",
    "  \"paraphrase_20\": \"While this skill may look easy initially, true expertise often requires a great deal of time to develop.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "635ffdd1-eed1-435e-803e-3a0e9e5ee8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_doc_id(input_text):\n",
    "    \n",
    "    # Extract everything between the brackets\n",
    "    match = re.search(r'\\[(.*)\\]', input_text)\n",
    "\n",
    "    if match:\n",
    "        extracted_text = extracted_text = match.group(1)\n",
    "    else:\n",
    "        extracted_text = input_text\n",
    "        \n",
    "    # Replace punctuation with \"_\" and spaces with \"_\", then reduce multiple underscores to one\n",
    "    cleaned_text = re.sub(r'[^\\w]', '_', extracted_text)\n",
    "    cleaned_text = re.sub(r'_+', '_', cleaned_text)\n",
    "\n",
    "    # Remove any leading or training '_'\n",
    "    final_text = cleaned_text.strip('_')\n",
    "    \n",
    "    return final_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "898c3435-8e21-443b-9fa7-7bbd3fbc03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_jsonl(row):\n",
    "    custom_id = row['custom_id']\n",
    "    user_text = row['sentence']\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_text}\n",
    "            ],\n",
    "            \"max_tokens\": 5000,\n",
    "            \"temperature\": 1,\n",
    "            \"top_p\": 1,\n",
    "            \"response_format\": { \"type\": 'json_object' }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7ac1e230-52af-4a73-b547-17433ca800f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch_jsonl(df, batch_loc):\n",
    "    unique_doc_ids = df['temp_doc_id'].unique()\n",
    "    \n",
    "    for doc_id in unique_doc_ids:\n",
    "        filtered_df = df[df['temp_doc_id'] == doc_id]\n",
    "        jsonl_data = [row_to_jsonl(row) for _, row in filtered_df.iterrows()]\n",
    "        file_name = f\"batch_{doc_id}.jsonl\"\n",
    "        file_path = os.path.join(batch_loc, file_name)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            for item in jsonl_data:\n",
    "                f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83405597-8844-4bbe-933e-7442cb631c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"test\"\n",
    "data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d1c3ea2-6aae-4505-89f7-8c7c58db7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list_files(data_loc, \"known_processed.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a61f8597-c543-4a0f-b9bf-616a70942752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/BCross/datasets/author_verification/test/StackExchange/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Amazon/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/The Telegraph/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Yelp/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Wiki/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/All-the-news/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/IMDB/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Reddit/known_processed.jsonl',\n",
       " \"/Volumes/BCross/datasets/author_verification/test/Koppel's Blogs/known_processed.jsonl\",\n",
       " '/Volumes/BCross/datasets/author_verification/test/Perverted Justice/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/TripAdvisor/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/ACL/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/The Apricity/known_processed.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Enron/known_processed.jsonl']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2fd1aaed-3ec1-4885-93c4-19b521366db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: StackExchange Batch Preprocessing Complete!\n",
      "Corpus: Amazon Batch Preprocessing Complete!\n",
      "Corpus: The Telegraph Batch Preprocessing Complete!\n",
      "Corpus: Yelp Batch Preprocessing Complete!\n",
      "Corpus: Wiki Batch Preprocessing Complete!\n",
      "Corpus: All-the-news Batch Preprocessing Complete!\n",
      "Corpus: IMDB Batch Preprocessing Complete!\n",
      "Corpus: Reddit Batch Preprocessing Complete!\n",
      "Corpus: Koppel's Blogs Batch Preprocessing Complete!\n",
      "Corpus: Perverted Justice Batch Preprocessing Complete!\n",
      "Corpus: TripAdvisor Batch Preprocessing Complete!\n",
      "Corpus: ACL Batch Preprocessing Complete!\n",
      "Corpus: The Apricity Batch Preprocessing Complete!\n",
      "Corpus: Enron Batch Preprocessing Complete!\n"
     ]
    }
   ],
   "source": [
    "for file in file_list:\n",
    "\n",
    "    # Get the folder name for saving\n",
    "    folder_name = Path(file).parent.name\n",
    "\n",
    "    # temp_doc_id something like StackExchange_12345\n",
    "    df = read_jsonl(file)\n",
    "\n",
    "    df['temp_doc_id'] = df['doc_id'].apply(create_temp_doc_id)\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    # Repeat each row 10 times\n",
    "    df_repeated = df.loc[np.repeat(df.index, 10)].reset_index()\n",
    "    \n",
    "    # Add a column for the repetition number\n",
    "    df_repeated['repetition'] = df_repeated.groupby('index').cumcount() + 1\n",
    "    \n",
    "    # Drop the old index column as it's no longer needed\n",
    "    df_repeated = df_repeated.drop(columns='index')\n",
    "    \n",
    "    # Create the custom_id column\n",
    "    df_repeated['custom_id'] = (\n",
    "        current_date + '_' +\n",
    "        'doc_' + df_repeated['temp_doc_id'].astype(str) +\n",
    "        '_chunk_' + df_repeated['chunk_id'].astype(str) +\n",
    "        '_' + df_repeated['repetition'].astype(str)\n",
    "    )\n",
    "    \n",
    "    batch_loc = f\"{data_loc}{folder_name}/batch_sentence_preprocessed/\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(batch_loc, exist_ok=True)\n",
    "    \n",
    "    save_batch_jsonl(df_repeated, batch_loc)\n",
    "\n",
    "    print(f\"Corpus: {folder_name} Batch Preprocessing Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
