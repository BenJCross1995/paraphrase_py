{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "152bbc5d-a7ff-4e30-8f42-f567359f3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23bfa65b-a581-49fa-a12f-d9a6cd375567",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_loc = \"../../credentials.json\"\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"The Telegraph\"\n",
    "\n",
    "data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/\"\n",
    "batch_loc = f\"{data_loc}batch_sentence_preprocessed/\"\n",
    "raw_data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "\n",
    "# Location for data when sent to batch\n",
    "batch_sent_loc = f\"{data_loc}batch_sentence_sent/\"\n",
    "os.makedirs(batch_sent_loc, exist_ok=True)\n",
    "\n",
    "# Location once batch complete\n",
    "batch_complete_loc = f\"{data_loc}batch_sentence_complete/\"\n",
    "os.makedirs(batch_complete_loc, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6e3837b-aafd-47f2-9434-b978cf8eebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and converts it into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the JSONL file to read.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing the data from the JSONL file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbc4b3a0-a985-4cda-a832-d6943939b254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to process: 220\n"
     ]
    }
   ],
   "source": [
    "raw_df = read_jsonl(raw_data_loc)\n",
    "print(f\"Number of documents to process: {len(raw_df['doc_id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84a25110-d7b4-450b-a56f-20097193aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(credential_loc, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = data['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fcc66a5-b66f-42a9-8992-62f4c3ee4b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to be processed: 115\n",
      "Files complete: 105\n",
      "Total Files: 220\n"
     ]
    }
   ],
   "source": [
    "# List all .jsonl files in the batch_loc directory\n",
    "files_to_be_processed = [\n",
    "    f for f in os.listdir(batch_loc)\n",
    "    if os.path.isfile(os.path.join(batch_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "files_processed = [\n",
    "    f for f in os.listdir(batch_complete_loc)\n",
    "    if os.path.isfile(os.path.join(batch_complete_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "print(f\"Files to be processed: {len(files_to_be_processed)}\")\n",
    "print(f\"Files complete: {len(files_processed)}\")\n",
    "print(f\"Total Files: {len(files_processed) + len(files_to_be_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dff8313b-0e0b-471d-acd2-063f7446dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file with a custom description\n",
    "def process_file(file_path, description):\n",
    "    # Open the file and create a batch input file\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        batch_input_file = client.files.create(file=f, purpose=\"batch\")\n",
    "    \n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    # Create a batch job with the batch input file and custom description\n",
    "    client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": description\n",
    "        }\n",
    "    )\n",
    "    print(f\"Processed file: {file_path} with description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c17c3d7-1fe6-4b17-af00-7502c1072805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(batch_loc, batch_sent_loc, files, batch_size=5, wait_time=600):\n",
    "\n",
    "    total_files = len(files)\n",
    "    \n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = files[i:i+batch_size]\n",
    "        \n",
    "        for file_name in batch_files:\n",
    "            file_path = os.path.join(batch_loc, file_name)\n",
    "            description = os.path.splitext(file_name)[0]\n",
    "            process_file(file_path, description)\n",
    "            \n",
    "            # Move the processed file to batch_sent_loc directory\n",
    "            shutil.move(file_path, os.path.join(batch_sent_loc, file_name))\n",
    "            print(f\"Moved file: {file_name} to {batch_sent_loc}\")\n",
    "        \n",
    "        print(f\"Processed {len(batch_files)} files. Waiting for {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)  # Wait for the specified time before processing the next batch\n",
    "\n",
    "    print(\"All files processed and moved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4875f631-c2b7-464d-8aa6-357219d2d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_jsonl(data, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for _, row in data.iterrows():\n",
    "            json.dump(row.to_dict(), file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aef3f8c0-389a-4e2d-9e6a-86f79e33217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_helenbrown_text_1.jsonl with description: batch_helenbrown_text_1\n",
      "Moved file: batch_helenbrown_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_helenyemm_text_1.jsonl with description: batch_helenyemm_text_1\n",
      "Moved file: batch_helenyemm_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_helenyemm_text_3.jsonl with description: batch_helenyemm_text_3\n",
      "Moved file: batch_helenyemm_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_henrysamuel_text_2.jsonl with description: batch_henrysamuel_text_2\n",
      "Moved file: batch_henrysamuel_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_henrywinter_text_1.jsonl with description: batch_henrywinter_text_1\n",
      "Moved file: batch_henrywinter_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_henrywinter_text_2.jsonl with description: batch_henrywinter_text_2\n",
      "Moved file: batch_henrywinter_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hollywatt_text_2.jsonl with description: batch_hollywatt_text_2\n",
      "Moved file: batch_hollywatt_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_horatiaharrod_text_1.jsonl with description: batch_horatiaharrod_text_1\n",
      "Moved file: batch_horatiaharrod_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_horatiaharrod_text_2.jsonl with description: batch_horatiaharrod_text_2\n",
      "Moved file: batch_horatiaharrod_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hughmorris_text_1.jsonl with description: batch_hughmorris_text_1\n",
      "Moved file: batch_hughmorris_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hughmorris_text_3.jsonl with description: batch_hughmorris_text_3\n",
      "Moved file: batch_hughmorris_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_craigmclean_text_1.jsonl with description: batch_craigmclean_text_1\n",
      "Moved file: batch_craigmclean_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_craigmclean_text_2.jsonl with description: batch_craigmclean_text_2\n",
      "Moved file: batch_craigmclean_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_danieljohnson_text_1.jsonl with description: batch_danieljohnson_text_1\n",
      "Moved file: batch_danieljohnson_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_davidblair_text_2.jsonl with description: batch_davidblair_text_2\n",
      "Moved file: batch_davidblair_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_davidblair_text_3.jsonl with description: batch_davidblair_text_3\n",
      "Moved file: batch_davidblair_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_davidgritten_text_2.jsonl with description: batch_davidgritten_text_2\n",
      "Moved file: batch_davidgritten_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_davidgritten_text_3.jsonl with description: batch_davidgritten_text_3\n",
      "Moved file: batch_davidgritten_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_deannelson_text_1.jsonl with description: batch_deannelson_text_1\n",
      "Moved file: batch_deannelson_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_derekpringle_text_3.jsonl with description: batch_derekpringle_text_3\n",
      "Moved file: batch_derekpringle_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_dianahenry_text_1.jsonl with description: batch_dianahenry_text_1\n",
      "Moved file: batch_dianahenry_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_dianahenry_text_3.jsonl with description: batch_dianahenry_text_3\n",
      "Moved file: batch_dianahenry_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_edcumming_text_1.jsonl with description: batch_edcumming_text_1\n",
      "Moved file: batch_edcumming_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_edcumming_text_2.jsonl with description: batch_edcumming_text_2\n",
      "Moved file: batch_edcumming_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_elizabethgrice_text_1.jsonl with description: batch_elizabethgrice_text_1\n",
      "Moved file: batch_elizabethgrice_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_emilygosden_text_2.jsonl with description: batch_emilygosden_text_2\n",
      "Moved file: batch_emilygosden_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gabywood_text_1.jsonl with description: batch_gabywood_text_1\n",
      "Moved file: batch_gabywood_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gabywood_text_2.jsonl with description: batch_gabywood_text_2\n",
      "Moved file: batch_gabywood_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_garethadavies_text_3.jsonl with description: batch_garethadavies_text_3\n",
      "Moved file: batch_garethadavies_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gavinmairs_text_1.jsonl with description: batch_gavinmairs_text_1\n",
      "Moved file: batch_gavinmairs_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gavinmairs_text_2.jsonl with description: batch_gavinmairs_text_2\n",
      "Moved file: batch_gavinmairs_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_geoffreyboycott_text_1.jsonl with description: batch_geoffreyboycott_text_1\n",
      "Moved file: batch_geoffreyboycott_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_geoffreylean_text_2.jsonl with description: batch_geoffreylean_text_2\n",
      "Moved file: batch_geoffreylean_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gilesmole_text_1.jsonl with description: batch_gilesmole_text_1\n",
      "Moved file: batch_gilesmole_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gilesmole_text_3.jsonl with description: batch_gilesmole_text_3\n",
      "Moved file: batch_gilesmole_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gordonrayner_text_1.jsonl with description: batch_gordonrayner_text_1\n",
      "Moved file: batch_gordonrayner_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gordonrayner_text_3.jsonl with description: batch_gordonrayner_text_3\n",
      "Moved file: batch_gordonrayner_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_graemepaton_text_2.jsonl with description: batch_graemepaton_text_2\n",
      "Moved file: batch_graemepaton_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_graemepaton_text_3.jsonl with description: batch_graemepaton_text_3\n",
      "Moved file: batch_graemepaton_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_grahamnorton_text_1.jsonl with description: batch_grahamnorton_text_1\n",
      "Moved file: batch_grahamnorton_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_grahamnorton_text_2.jsonl with description: batch_grahamnorton_text_2\n",
      "Moved file: batch_grahamnorton_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_grahamruddick_text_1.jsonl with description: batch_grahamruddick_text_1\n",
      "Moved file: batch_grahamruddick_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_grahamruddick_text_2.jsonl with description: batch_grahamruddick_text_2\n",
      "Moved file: batch_grahamruddick_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_gregorywalton_text_1.jsonl with description: batch_gregorywalton_text_1\n",
      "Moved file: batch_gregorywalton_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hannahfurness_text_1.jsonl with description: batch_hannahfurness_text_1\n",
      "Moved file: batch_hannahfurness_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hannahstrange_text_1.jsonl with description: batch_hannahstrange_text_1\n",
      "Moved file: batch_hannahstrange_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_hannahstrange_text_3.jsonl with description: batch_hannahstrange_text_3\n",
      "Moved file: batch_hannahstrange_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_harrietalexander_text_1.jsonl with description: batch_harrietalexander_text_1\n",
      "Moved file: batch_harrietalexander_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_harrydequetteville_text_1.jsonl with description: batch_harrydequetteville_text_1\n",
      "Moved file: batch_harrydequetteville_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_harrywallop_text_1.jsonl with description: batch_harrywallop_text_1\n",
      "Moved file: batch_harrywallop_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_harrywallop_text_2.jsonl with description: batch_harrywallop_text_2\n",
      "Moved file: batch_harrywallop_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alansmith_text_3.jsonl with description: batch_alansmith_text_3\n",
      "Moved file: batch_alansmith_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alantitchmarsh_text_2.jsonl with description: batch_alantitchmarsh_text_2\n",
      "Moved file: batch_alantitchmarsh_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alantovey_text_1.jsonl with description: batch_alantovey_text_1\n",
      "Moved file: batch_alantovey_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alantovey_text_2.jsonl with description: batch_alantovey_text_2\n",
      "Moved file: batch_alantovey_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alasdairreid_text_2.jsonl with description: batch_alasdairreid_text_2\n",
      "Moved file: batch_alasdairreid_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alasdairreid_text_3.jsonl with description: batch_alasdairreid_text_3\n",
      "Moved file: batch_alasdairreid_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairgood_text_1.jsonl with description: batch_alastairgood_text_1\n",
      "Moved file: batch_alastairgood_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairgood_text_3.jsonl with description: batch_alastairgood_text_3\n",
      "Moved file: batch_alastairgood_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairsmart_text_1.jsonl with description: batch_alastairsmart_text_1\n",
      "Moved file: batch_alastairsmart_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairsmart_text_2.jsonl with description: batch_alastairsmart_text_2\n",
      "Moved file: batch_alastairsmart_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairsooke_text_2.jsonl with description: batch_alastairsooke_text_2\n",
      "Moved file: batch_alastairsooke_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alastairsooke_text_3.jsonl with description: batch_alastairsooke_text_3\n",
      "Moved file: batch_alastairsooke_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alexjames_text_1.jsonl with description: batch_alexjames_text_1\n",
      "Moved file: batch_alexjames_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alexjames_text_3.jsonl with description: batch_alexjames_text_3\n",
      "Moved file: batch_alexjames_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_aliceaudley_text_1.jsonl with description: batch_aliceaudley_text_1\n",
      "Moved file: batch_aliceaudley_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_aliceaudley_text_3.jsonl with description: batch_aliceaudley_text_3\n",
      "Moved file: batch_aliceaudley_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alicephilipson_text_2.jsonl with description: batch_alicephilipson_text_2\n",
      "Moved file: batch_alicephilipson_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alicephilipson_text_3.jsonl with description: batch_alicephilipson_text_3\n",
      "Moved file: batch_alicephilipson_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alicevincent_text_1.jsonl with description: batch_alicevincent_text_1\n",
      "Moved file: batch_alicevincent_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_alicevincent_text_3.jsonl with description: batch_alicevincent_text_3\n",
      "Moved file: batch_alicevincent_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_allisonpearson_text_1.jsonl with description: batch_allisonpearson_text_1\n",
      "Moved file: batch_allisonpearson_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_allisonpearson_text_2.jsonl with description: batch_allisonpearson_text_2\n",
      "Moved file: batch_allisonpearson_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_allisterheath_text_1.jsonl with description: batch_allisterheath_text_1\n",
      "Moved file: batch_allisterheath_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_allisterheath_text_2.jsonl with description: batch_allisterheath_text_2\n",
      "Moved file: batch_allisterheath_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_ambroseevans_pritchard_text_1.jsonl with description: batch_ambroseevans_pritchard_text_1\n",
      "Moved file: batch_ambroseevans_pritchard_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_ambroseevans_pritchard_text_2.jsonl with description: batch_ambroseevans_pritchard_text_2\n",
      "Moved file: batch_ambroseevans_pritchard_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewcritchlow_text_1.jsonl with description: batch_andrewcritchlow_text_1\n",
      "Moved file: batch_andrewcritchlow_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewcritchlow_text_3.jsonl with description: batch_andrewcritchlow_text_3\n",
      "Moved file: batch_andrewcritchlow_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewenglish_text_2.jsonl with description: batch_andrewenglish_text_2\n",
      "Moved file: batch_andrewenglish_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewenglish_text_3.jsonl with description: batch_andrewenglish_text_3\n",
      "Moved file: batch_andrewenglish_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewgilligan_text_1.jsonl with description: batch_andrewgilligan_text_1\n",
      "Moved file: batch_andrewgilligan_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewgilligan_text_3.jsonl with description: batch_andrewgilligan_text_3\n",
      "Moved file: batch_andrewgilligan_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewmarszal_text_1.jsonl with description: batch_andrewmarszal_text_1\n",
      "Moved file: batch_andrewmarszal_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_andrewmarszal_text_2.jsonl with description: batch_andrewmarszal_text_2\n",
      "Moved file: batch_andrewmarszal_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_annatyzack_text_1.jsonl with description: batch_annatyzack_text_1\n",
      "Moved file: batch_annatyzack_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_annatyzack_text_2.jsonl with description: batch_annatyzack_text_2\n",
      "Moved file: batch_annatyzack_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_annebillson_text_1.jsonl with description: batch_annebillson_text_1\n",
      "Moved file: batch_annebillson_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_annebillson_text_3.jsonl with description: batch_annebillson_text_3\n",
      "Moved file: batch_annebillson_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_anthonyperegrine_text_1.jsonl with description: batch_anthonyperegrine_text_1\n",
      "Moved file: batch_anthonyperegrine_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_anthonyperegrine_text_3.jsonl with description: batch_anthonyperegrine_text_3\n",
      "Moved file: batch_anthonyperegrine_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_auslancramb_text_1.jsonl with description: batch_auslancramb_text_1\n",
      "Moved file: batch_auslancramb_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_auslancramb_text_3.jsonl with description: batch_auslancramb_text_3\n",
      "Moved file: batch_auslancramb_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_beewilson_text_3.jsonl with description: batch_beewilson_text_3\n",
      "Moved file: batch_beewilson_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benfarmer_text_2.jsonl with description: batch_benfarmer_text_2\n",
      "Moved file: batch_benfarmer_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benfarmer_text_3.jsonl with description: batch_benfarmer_text_3\n",
      "Moved file: batch_benfarmer_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benfogle_text_2.jsonl with description: batch_benfogle_text_2\n",
      "Moved file: batch_benfogle_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benfogle_text_3.jsonl with description: batch_benfogle_text_3\n",
      "Moved file: batch_benfogle_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benlawrence_text_1.jsonl with description: batch_benlawrence_text_1\n",
      "Moved file: batch_benlawrence_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benlawrence_text_2.jsonl with description: batch_benlawrence_text_2\n",
      "Moved file: batch_benlawrence_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benmarlow_text_1.jsonl with description: batch_benmarlow_text_1\n",
      "Moved file: batch_benmarlow_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_benmarlow_text_2.jsonl with description: batch_benmarlow_text_2\n",
      "Moved file: batch_benmarlow_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_bernadettemcnulty_text_2.jsonl with description: batch_bernadettemcnulty_text_2\n",
      "Moved file: batch_bernadettemcnulty_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_beverleyturner_text_1.jsonl with description: batch_beverleyturner_text_1\n",
      "Moved file: batch_beverleyturner_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_beverleyturner_text_3.jsonl with description: batch_beverleyturner_text_3\n",
      "Moved file: batch_beverleyturner_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_borisjohnson_text_1.jsonl with description: batch_borisjohnson_text_1\n",
      "Moved file: batch_borisjohnson_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_borisjohnson_text_2.jsonl with description: batch_borisjohnson_text_2\n",
      "Moved file: batch_borisjohnson_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_brianmoore_text_2.jsonl with description: batch_brianmoore_text_2\n",
      "Moved file: batch_brianmoore_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_brianmoore_text_3.jsonl with description: batch_brianmoore_text_3\n",
      "Moved file: batch_brianmoore_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_brunowaterfield_text_2.jsonl with description: batch_brunowaterfield_text_2\n",
      "Moved file: batch_brunowaterfield_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_brunowaterfield_text_3.jsonl with description: batch_brunowaterfield_text_3\n",
      "Moved file: batch_brunowaterfield_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_bryonygordon_text_2.jsonl with description: batch_bryonygordon_text_2\n",
      "Moved file: batch_bryonygordon_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_bunnyguinness_text_2.jsonl with description: batch_bunnyguinness_text_2\n",
      "Moved file: batch_bunnyguinness_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_bunnyguinness_text_3.jsonl with description: batch_bunnyguinness_text_3\n",
      "Moved file: batch_bunnyguinness_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_cameronmacphail_text_1.jsonl with description: batch_cameronmacphail_text_1\n",
      "Moved file: batch_cameronmacphail_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_cameronmacphail_text_3.jsonl with description: batch_cameronmacphail_text_3\n",
      "Moved file: batch_cameronmacphail_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_camillaturner_text_1.jsonl with description: batch_camillaturner_text_1\n",
      "Moved file: batch_camillaturner_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_camillaturner_text_2.jsonl with description: batch_camillaturner_text_2\n",
      "Moved file: batch_camillaturner_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolinekent_text_2.jsonl with description: batch_carolinekent_text_2\n",
      "Moved file: batch_carolinekent_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolinekent_text_3.jsonl with description: batch_carolinekent_text_3\n",
      "Moved file: batch_carolinekent_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolinemcghie_text_1.jsonl with description: batch_carolinemcghie_text_1\n",
      "Moved file: batch_carolinemcghie_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolinemcghie_text_3.jsonl with description: batch_carolinemcghie_text_3\n",
      "Moved file: batch_carolinemcghie_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolynhart_text_1.jsonl with description: batch_carolynhart_text_1\n",
      "Moved file: batch_carolynhart_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_carolynhart_text_2.jsonl with description: batch_carolynhart_text_2\n",
      "Moved file: batch_carolynhart_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_catalinastogdon_text_1.jsonl with description: batch_catalinastogdon_text_1\n",
      "Moved file: batch_catalinastogdon_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_catalinastogdon_text_3.jsonl with description: batch_catalinastogdon_text_3\n",
      "Moved file: batch_catalinastogdon_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_catherinegee_text_2.jsonl with description: batch_catherinegee_text_2\n",
      "Moved file: batch_catherinegee_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlesmoore_text_1.jsonl with description: batch_charlesmoore_text_1\n",
      "Moved file: batch_charlesmoore_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlesmoore_text_2.jsonl with description: batch_charlesmoore_text_2\n",
      "Moved file: batch_charlesmoore_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlesspencer_text_1.jsonl with description: batch_charlesspencer_text_1\n",
      "Moved file: batch_charlesspencer_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlesspencer_text_2.jsonl with description: batch_charlesspencer_text_2\n",
      "Moved file: batch_charlesspencer_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlottebeugge_text_1.jsonl with description: batch_charlottebeugge_text_1\n",
      "Moved file: batch_charlottebeugge_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlottebeugge_text_3.jsonl with description: batch_charlottebeugge_text_3\n",
      "Moved file: batch_charlottebeugge_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlotteruncie_text_1.jsonl with description: batch_charlotteruncie_text_1\n",
      "Moved file: batch_charlotteruncie_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_charlotteruncie_text_3.jsonl with description: batch_charlotteruncie_text_3\n",
      "Moved file: batch_charlotteruncie_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_chrisharvey_text_1.jsonl with description: batch_chrisharvey_text_1\n",
      "Moved file: batch_chrisharvey_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_chrisharvey_text_3.jsonl with description: batch_chrisharvey_text_3\n",
      "Moved file: batch_chrisharvey_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_chrisknapman_text_1.jsonl with description: batch_chrisknapman_text_1\n",
      "Moved file: batch_chrisknapman_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_chrisknapman_text_2.jsonl with description: batch_chrisknapman_text_2\n",
      "Moved file: batch_chrisknapman_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherbooker_text_1.jsonl with description: batch_christopherbooker_text_1\n",
      "Moved file: batch_christopherbooker_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherbooker_text_3.jsonl with description: batch_christopherbooker_text_3\n",
      "Moved file: batch_christopherbooker_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherhope_text_1.jsonl with description: batch_christopherhope_text_1\n",
      "Moved file: batch_christopherhope_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherhope_text_3.jsonl with description: batch_christopherhope_text_3\n",
      "Moved file: batch_christopherhope_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherhowse_text_1.jsonl with description: batch_christopherhowse_text_1\n",
      "Moved file: batch_christopherhowse_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christophermiddleton_text_2.jsonl with description: batch_christophermiddleton_text_2\n",
      "Moved file: batch_christophermiddleton_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christophermiddleton_text_3.jsonl with description: batch_christophermiddleton_text_3\n",
      "Moved file: batch_christophermiddleton_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherwilliams_text_1.jsonl with description: batch_christopherwilliams_text_1\n",
      "Moved file: batch_christopherwilliams_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_christopherwilliams_text_3.jsonl with description: batch_christopherwilliams_text_3\n",
      "Moved file: batch_christopherwilliams_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_clairecarter_text_1.jsonl with description: batch_clairecarter_text_1\n",
      "Moved file: batch_clairecarter_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_clairecarter_text_2.jsonl with description: batch_clairecarter_text_2\n",
      "Moved file: batch_clairecarter_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_clairecohen_text_1.jsonl with description: batch_clairecohen_text_1\n",
      "Moved file: batch_clairecohen_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_clairecohen_text_3.jsonl with description: batch_clairecohen_text_3\n",
      "Moved file: batch_clairecohen_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_claireduffin_text_2.jsonl with description: batch_claireduffin_text_2\n",
      "Moved file: batch_claireduffin_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_claireduffin_text_3.jsonl with description: batch_claireduffin_text_3\n",
      "Moved file: batch_claireduffin_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_cliveaslet_text_1.jsonl with description: batch_cliveaslet_text_1\n",
      "Moved file: batch_cliveaslet_text_1.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_cliveaslet_text_3.jsonl with description: batch_cliveaslet_text_3\n",
      "Moved file: batch_cliveaslet_text_3.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 3 files. Waiting for 600 seconds...\n",
      "Processed file: /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/batch_clivejames_text_2.jsonl with description: batch_clivejames_text_2\n",
      "Moved file: batch_clivejames_text_2.jsonl to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/\n",
      "Processed 1 files. Waiting for 600 seconds...\n",
      "All files processed and moved successfully.\n"
     ]
    }
   ],
   "source": [
    "process_all_files(batch_loc, batch_sent_loc, files_to_be_processed, batch_size=3, wait_time=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e75b201d-35dd-4217-a40f-0e9c7811fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batches(client, after=None):\n",
    "    \"\"\"\n",
    "    Fetch batches from the client, optionally using an 'after' parameter to paginate.\n",
    "    \n",
    "    :param client: API client instance.\n",
    "    :param after: The ID to paginate after.\n",
    "    :return: List of batch data.\n",
    "    \"\"\"\n",
    "    response = client.batches.list(limit=100, after=after)\n",
    "    return response.data\n",
    "\n",
    "def process_batches(client):\n",
    "    \"\"\"\n",
    "    Process batches by fetching data, sorting, and paginating until no new data is retrieved.\n",
    "    \n",
    "    :param client: API client instance.\n",
    "    :return: DataFrame containing all batch data.\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    after = None\n",
    "\n",
    "    while True:\n",
    "        # Fetch the current batch of data\n",
    "        current_batches = fetch_batches(client, after)\n",
    "        if not current_batches:\n",
    "            print(\"No more batches to fetch.\")\n",
    "            break\n",
    "        \n",
    "        # Extract attributes and convert to DataFrame\n",
    "        for batch in current_batches:\n",
    "            batch_dict = {\n",
    "                \"id\": batch.id,\n",
    "                \"completion_window\": batch.completion_window,\n",
    "                \"created_at\": batch.created_at,\n",
    "                \"endpoint\": batch.endpoint,\n",
    "                \"input_file_id\": batch.input_file_id,\n",
    "                \"object\": batch.object,\n",
    "                \"status\": batch.status,\n",
    "                \"cancelled_at\": batch.cancelled_at,\n",
    "                \"cancelling_at\": batch.cancelling_at,\n",
    "                \"completed_at\": batch.completed_at,\n",
    "                \"error_file_id\": batch.error_file_id,\n",
    "                \"errors\": batch.errors,\n",
    "                \"expired_at\": batch.expired_at,\n",
    "                \"expires_at\": batch.expires_at,\n",
    "                \"failed_at\": batch.failed_at,\n",
    "                \"finalizing_at\": batch.finalizing_at,\n",
    "                \"in_progress_at\": batch.in_progress_at,\n",
    "                \"metadata_description\": batch.metadata.get('description', ''),\n",
    "                \"output_file_id\": batch.output_file_id,\n",
    "                \"request_counts_completed\": batch.request_counts.completed,\n",
    "                \"request_counts_failed\": batch.request_counts.failed,\n",
    "                \"request_counts_total\": batch.request_counts.total\n",
    "            }\n",
    "            batch_data.append(batch_dict)\n",
    "        \n",
    "        # Create a DataFrame from the current batch data\n",
    "        batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "        # Remove duplicate rows based on 'id'\n",
    "        batch_df.drop_duplicates(subset='id', keep='last', inplace=True)\n",
    "        \n",
    "        # Sort DataFrame by 'created_at' column\n",
    "        batch_df.sort_values(by='created_at', ascending=True, inplace=True)\n",
    "        \n",
    "        # Print the current DataFrame state for debugging\n",
    "        print(f\"Current DataFrame shape: {batch_df.shape}\")\n",
    "        \n",
    "        # Update the 'after' parameter with the last batch ID for pagination\n",
    "        last_batch_id = batch_df['id'].iloc[-1] if not batch_df.empty else None\n",
    "        if last_batch_id == after:\n",
    "            print(\"No new batches found.\")\n",
    "            break\n",
    "        after = last_batch_id\n",
    "    \n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3013e113-92af-4edf-a656-1b7c32f96b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batches(client, after=None):\n",
    "    \"\"\"\n",
    "    Fetch batches from the client, optionally using an 'after' parameter to paginate.\n",
    "    \n",
    "    :param client: API client instance.\n",
    "    :param after: The ID to paginate after.\n",
    "    :return: List of batch data.\n",
    "    \"\"\"\n",
    "    response = client.batches.list(limit=100, after=after)\n",
    "    return response.data\n",
    "\n",
    "def process_batches(client):\n",
    "    \"\"\"\n",
    "    Process batches by fetching data, sorting, and paginating until no new data is retrieved.\n",
    "    \n",
    "    :param client: API client instance.\n",
    "    :return: DataFrame containing all batch data.\n",
    "    \"\"\"\n",
    "    all_batches = []  # This will collect all batch dictionaries\n",
    "    after = None\n",
    "\n",
    "    while True:\n",
    "        # Fetch the current batch of data\n",
    "        current_batches = fetch_batches(client, after)\n",
    "        if not current_batches:\n",
    "            print(\"No more batches to fetch.\")\n",
    "            break\n",
    "        \n",
    "        # Extract attributes and append to the main list\n",
    "        for batch in current_batches:\n",
    "            batch_dict = {\n",
    "                \"id\": batch.id,\n",
    "                \"completion_window\": batch.completion_window,\n",
    "                \"created_at\": batch.created_at,\n",
    "                \"endpoint\": batch.endpoint,\n",
    "                \"input_file_id\": batch.input_file_id,\n",
    "                \"object\": batch.object,\n",
    "                \"status\": batch.status,\n",
    "                \"cancelled_at\": batch.cancelled_at,\n",
    "                \"cancelling_at\": batch.cancelling_at,\n",
    "                \"completed_at\": batch.completed_at,\n",
    "                \"error_file_id\": batch.error_file_id,\n",
    "                \"errors\": batch.errors,\n",
    "                \"expired_at\": batch.expired_at,\n",
    "                \"expires_at\": batch.expires_at,\n",
    "                \"failed_at\": batch.failed_at,\n",
    "                \"finalizing_at\": batch.finalizing_at,\n",
    "                \"in_progress_at\": batch.in_progress_at,\n",
    "                \"metadata_description\": batch.metadata.get('description', ''),\n",
    "                \"output_file_id\": batch.output_file_id,\n",
    "                \"request_counts_completed\": batch.request_counts.completed,\n",
    "                \"request_counts_failed\": batch.request_counts.failed,\n",
    "                \"request_counts_total\": batch.request_counts.total\n",
    "            }\n",
    "            all_batches.append(batch_dict)\n",
    "        \n",
    "        # Update the 'after' parameter with the last batch ID for pagination\n",
    "        last_batch_id = current_batches[-1].id if current_batches else None\n",
    "        if last_batch_id == after:\n",
    "            print(\"No new batches found.\")\n",
    "            break\n",
    "        after = last_batch_id\n",
    "\n",
    "    # Create a single DataFrame after collecting all batches\n",
    "    batch_df = pd.DataFrame(all_batches)\n",
    "\n",
    "    # Remove duplicate rows based on 'id'\n",
    "    batch_df.drop_duplicates(subset='id', keep='last', inplace=True)\n",
    "    \n",
    "    # Sort DataFrame by 'created_at' column\n",
    "    batch_df.sort_values(by='created_at', ascending=True, inplace=True)\n",
    "\n",
    "    return batch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "326df939-223a-4fed-a14d-320110d1e837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Batch(id='batch_6778bdaceca881908231dd60ac546e6f', completion_window='24h', created_at=1735966125, endpoint='/v1/chat/completions', input_file_id='file-PqTfyspu9P3hg7UYqqJtrv', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736052525, failed_at=1735966125, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_clivejames_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778bb51155c819090eb546005419653', completion_window='24h', created_at=1735965521, endpoint='/v1/chat/completions', input_file_id='file-SmeCQNkwfwVK2ZUYNzx8s9', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735965980, error_file_id=None, errors=None, expired_at=None, expires_at=1736051921, failed_at=None, finalizing_at=1735965951, in_progress_at=1735965522, metadata={'description': 'batch_cliveaslet_text_3'}, output_file_id='file-K3YE81JzK7MTgXD6iK6CXb', request_counts=BatchRequestCounts(completed=220, failed=0, total=220)),\n",
       " Batch(id='batch_6778bb4f1464819086be5d9bdd7a0fe0', completion_window='24h', created_at=1735965519, endpoint='/v1/chat/completions', input_file_id='file-CzWaqbEcNuQbf9k4rre5B4', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736051919, failed_at=1735965519, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_cliveaslet_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778bb4c3ca0819081007036dbdee1d5', completion_window='24h', created_at=1735965516, endpoint='/v1/chat/completions', input_file_id='file-UQGgQzHHYGs5iJZiR3bKHX', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736051916, failed_at=1735965516, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_claireduffin_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b8f1963c8190a1a9486e2b09cb43', completion_window='24h', created_at=1735964913, endpoint='/v1/chat/completions', input_file_id='file-Wdngqpr2DRRQRCbeaVXj75', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736051313, failed_at=1735964914, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_claireduffin_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b8eef6248190b4a5e7fb48f06102', completion_window='24h', created_at=1735964911, endpoint='/v1/chat/completions', input_file_id='file-U5ZrckUgRqUcE4Qn2YpU7E', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735966022, error_file_id=None, errors=None, expired_at=None, expires_at=1736051311, failed_at=None, finalizing_at=1735965980, in_progress_at=1735964912, metadata={'description': 'batch_clairecohen_text_3'}, output_file_id='file-KCgrvrNNhN4Vdxvkk4WBNJ', request_counts=BatchRequestCounts(completed=290, failed=0, total=290)),\n",
       " Batch(id='batch_6778b8eb67b08190bfda1a4c323e293f', completion_window='24h', created_at=1735964907, endpoint='/v1/chat/completions', input_file_id='file-6ZcnBw7LfYzRRj4PgGViJV', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736051307, failed_at=1735964908, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_clairecohen_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b68ef940819085ae5fdfdd60187b', completion_window='24h', created_at=1735964303, endpoint='/v1/chat/completions', input_file_id='file-KCX5e5K8DZqi5oeJrXQiCW', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736050703, failed_at=1735964304, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_clairecarter_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b68c7a448190a3c9cb6115fc8ecf', completion_window='24h', created_at=1735964300, endpoint='/v1/chat/completions', input_file_id='file-FzbmQ9bwpN3ysPciiC3wk7', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736050700, failed_at=1735964301, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_clairecarter_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b6898cf08190919b1f3caba56d78', completion_window='24h', created_at=1735964297, endpoint='/v1/chat/completions', input_file_id='file-J9EAjZcmhR8CASK4Z4aZi7', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735964915, error_file_id=None, errors=None, expired_at=None, expires_at=1736050697, failed_at=None, finalizing_at=1735964889, in_progress_at=1735964298, metadata={'description': 'batch_christopherwilliams_text_3'}, output_file_id='file-M4tUAAcYE7F5a2zSFctCV2', request_counts=BatchRequestCounts(completed=190, failed=0, total=190)),\n",
       " Batch(id='batch_6778b42e57d88190ba0e3dbeeec62b98', completion_window='24h', created_at=1735963694, endpoint='/v1/chat/completions', input_file_id='file-GSBRAw4Bzg5SpGUsjai7Ng', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735964163, error_file_id=None, errors=None, expired_at=None, expires_at=1736050094, failed_at=None, finalizing_at=1735964144, in_progress_at=1735963695, metadata={'description': 'batch_christopherwilliams_text_1'}, output_file_id='file-UX4YaHJX9YMNtZFKVhGHHU', request_counts=BatchRequestCounts(completed=130, failed=0, total=130)),\n",
       " Batch(id='batch_6778b42c5a188190860743fd814908c8', completion_window='24h', created_at=1735963692, endpoint='/v1/chat/completions', input_file_id='file-GkxVUhM1tKseXDQV4Uvnmi', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736050092, failed_at=1735963693, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christophermiddleton_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b4294f448190be99e7e029825675', completion_window='24h', created_at=1735963689, endpoint='/v1/chat/completions', input_file_id='file-JiyhLL3498v5Ec7U5xEVCg', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736050089, failed_at=1735963690, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christophermiddleton_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b1ce4c48819089e56ae231efefb5', completion_window='24h', created_at=1735963086, endpoint='/v1/chat/completions', input_file_id='file-QecaM9HzzqMyV9PqWYLDij', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736049486, failed_at=1735963087, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christopherhowse_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b1cbba38819081ca91d330a2f241', completion_window='24h', created_at=1735963083, endpoint='/v1/chat/completions', input_file_id='file-RJuQDw2rE9trE5qaQ7WxYA', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736049483, failed_at=1735963084, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christopherhope_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778b1c8591c819097b062373e8a2d6d', completion_window='24h', created_at=1735963080, endpoint='/v1/chat/completions', input_file_id='file-7TEAfA29LbWM6qQqpY2dpv', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736049480, failed_at=1735963081, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christopherhope_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778af6dcee481909f281992c039e8f0', completion_window='24h', created_at=1735962477, endpoint='/v1/chat/completions', input_file_id='file-EsR3N9u22t3jv9kcWJLVux', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048877, failed_at=1735962478, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christopherbooker_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778af6b35388190b7dc16a7b45ff36e', completion_window='24h', created_at=1735962475, endpoint='/v1/chat/completions', input_file_id='file-CKHYER21rmL5s3eJ1gphvJ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048875, failed_at=1735962476, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_christopherbooker_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778af68766c819099f93218c5e1ca12', completion_window='24h', created_at=1735962472, endpoint='/v1/chat/completions', input_file_id='file-FhrqSs8FBcixvM312MXtdp', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048872, failed_at=1735962473, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_chrisknapman_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778ad0cf33c819088abced349a1f7db', completion_window='24h', created_at=1735961869, endpoint='/v1/chat/completions', input_file_id='file-11EGbs4UWt3L8VtUF6pE4c', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048269, failed_at=1735961870, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_chrisknapman_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778ad0a925c819094ff5d6b9987b5c8', completion_window='24h', created_at=1735961866, endpoint='/v1/chat/completions', input_file_id='file-8b7bB85xPZ7sgXf36mPPRz', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048266, failed_at=1735961867, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_chrisharvey_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778ad06bb508190aa94fd4e98de7284', completion_window='24h', created_at=1735961862, endpoint='/v1/chat/completions', input_file_id='file-5ynujvKHjX3rQa7AoEtE8r', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736048262, failed_at=1735961863, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_chrisharvey_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778aaaa5c108190b4b41f4545f3745c', completion_window='24h', created_at=1735961258, endpoint='/v1/chat/completions', input_file_id='file-62ofqEPwa3Xa5fVZT3kuCJ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047658, failed_at=1735961259, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlotteruncie_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778aaa7a2b881908c224d4b381a23ad', completion_window='24h', created_at=1735961255, endpoint='/v1/chat/completions', input_file_id='file-5mJiono8BTyGf23RTs2TQp', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047655, failed_at=1735961256, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlotteruncie_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778aaa541988190a27eddfb0674e3a8', completion_window='24h', created_at=1735961253, endpoint='/v1/chat/completions', input_file_id='file-WXezuDoeZLqY1iGX2Tc5CZ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047653, failed_at=1735961254, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlottebeugge_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a8496ad88190b811ffdf30e54854', completion_window='24h', created_at=1735960649, endpoint='/v1/chat/completions', input_file_id='file-4Pjvy7Eyyf1N9eb16d2JEu', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047049, failed_at=1735960650, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlottebeugge_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a846cf7c81909b8c882b0e56d9f7', completion_window='24h', created_at=1735960647, endpoint='/v1/chat/completions', input_file_id='file-KzPFT7u9G9DzCHYHekpVAP', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047047, failed_at=1735960648, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlesspencer_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a843d9848190bea9340bca053ea2', completion_window='24h', created_at=1735960643, endpoint='/v1/chat/completions', input_file_id='file-EH4gM6uFYZTMjgV8EH7Tag', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736047043, failed_at=1735960649, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlesspencer_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a5e8da3c8190bbc4f31165dba0c1', completion_window='24h', created_at=1735960040, endpoint='/v1/chat/completions', input_file_id='file-1gmY9Wa5XVEtqqHkhvCPzk', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736046440, failed_at=1735960041, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlesmoore_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a5e695d48190b38200dcc13288cb', completion_window='24h', created_at=1735960038, endpoint='/v1/chat/completions', input_file_id='file-WEuksBFuaVj16PpSN1Fy2E', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736046438, failed_at=1735960040, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_charlesmoore_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a5e2ff408190879af57327dac5ae', completion_window='24h', created_at=1735960035, endpoint='/v1/chat/completions', input_file_id='file-FLKntF7mpeXtRmoXZ81vV4', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736046435, failed_at=1735960035, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_catherinegee_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a387114881909f4a52575b6c6c70', completion_window='24h', created_at=1735959431, endpoint='/v1/chat/completions', input_file_id='file-D4R2UZcDUVms3CPcq2MRiu', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045831, failed_at=1735959432, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_catalinastogdon_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a383e1648190a89da96c295da412', completion_window='24h', created_at=1735959427, endpoint='/v1/chat/completions', input_file_id='file-7Uhv6VSYqXDGs6PT2Wr8uk', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045827, failed_at=1735959428, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_catalinastogdon_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a381aeec8190a7240ac10b04d3fb', completion_window='24h', created_at=1735959425, endpoint='/v1/chat/completions', input_file_id='file-TmhhzBVhL5FFUW89caSKHP', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045825, failed_at=1735959426, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolynhart_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a12616108190a12b344c43e84f4d', completion_window='24h', created_at=1735958822, endpoint='/v1/chat/completions', input_file_id='file-RLBHynenaw8bi4X525FgyL', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045222, failed_at=1735958823, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolynhart_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a1232a008190aedef7b1b47dc973', completion_window='24h', created_at=1735958819, endpoint='/v1/chat/completions', input_file_id='file-Bo445SnAZjaha5b4mSmyEv', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045219, failed_at=1735958819, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolinemcghie_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778a120c5cc819082695eb010ba25bd', completion_window='24h', created_at=1735958816, endpoint='/v1/chat/completions', input_file_id='file-576H3AidhLuStbZL6wEy1f', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736045216, failed_at=1735958817, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolinemcghie_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789ec5a4b08190b5a7dcc9bf8b9537', completion_window='24h', created_at=1735958213, endpoint='/v1/chat/completions', input_file_id='file-JFvminSCiKqtrF1fWscRjG', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736044613, failed_at=1735958214, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolinekent_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789ec3a3fc81908966bacf5c977232', completion_window='24h', created_at=1735958211, endpoint='/v1/chat/completions', input_file_id='file-WvnVjvXry2Gd9ciE9PHuX2', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736044611, failed_at=1735958212, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_carolinekent_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789ec06180819082cc11e842e09f02', completion_window='24h', created_at=1735958208, endpoint='/v1/chat/completions', input_file_id='file-1vyy2md7rmrfQwdDVbc4e8', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736044608, failed_at=1735958209, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_camillaturner_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789c6535448190b1da636429a737bd', completion_window='24h', created_at=1735957605, endpoint='/v1/chat/completions', input_file_id='file-6x64fJNmNy8KNVGdHn9n6g', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736044005, failed_at=1735957605, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_camillaturner_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789c62d6888190b2fadb45d5304485', completion_window='24h', created_at=1735957603, endpoint='/v1/chat/completions', input_file_id='file-ApvLLtywXHPgZ2HV1HmzKw', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736044003, failed_at=1735957664, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_cameronmacphail_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789c5daf70819099ab56d59f346e4f', completion_window='24h', created_at=1735957597, endpoint='/v1/chat/completions', input_file_id='file-X9RbfYKHAq3DQi54Z3rxcQ', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735959131, error_file_id=None, errors=None, expired_at=None, expires_at=1736043997, failed_at=None, finalizing_at=1735958978, in_progress_at=1735957659, metadata={'description': 'batch_cameronmacphail_text_1'}, output_file_id='file-1175iJrjAtiW4bPTk9FdKe', request_counts=BatchRequestCounts(completed=1050, failed=0, total=1050)),\n",
       " Batch(id='batch_67789a00a6888190b3ff07756920e655', completion_window='24h', created_at=1735956992, endpoint='/v1/chat/completions', input_file_id='file-4szE6eGp45dDhbDs1GdGax', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736043392, failed_at=1735956994, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_bunnyguinness_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677899fd8a908190b8028b1d8557315b', completion_window='24h', created_at=1735956989, endpoint='/v1/chat/completions', input_file_id='file-EpzzD1M1MbM1whJwXGnacs', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736043389, failed_at=1735956990, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_bunnyguinness_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677899f9b718819089a3e61fa4b1db3c', completion_window='24h', created_at=1735956985, endpoint='/v1/chat/completions', input_file_id='file-PaejrmAEU11Tyyt4L3mDmi', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736043385, failed_at=1735956986, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_bryonygordon_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778979e89dc819083260b5664419022', completion_window='24h', created_at=1735956382, endpoint='/v1/chat/completions', input_file_id='file-Utg63vtiiicMrqGsczotg5', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736042782, failed_at=1735956383, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_brunowaterfield_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778979c792481908f16ae17d426c27d', completion_window='24h', created_at=1735956380, endpoint='/v1/chat/completions', input_file_id='file-Xpc8Ex8apowtZB4c4CbT8K', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735956546, error_file_id=None, errors=None, expired_at=None, expires_at=1736042780, failed_at=None, finalizing_at=1735956525, in_progress_at=1735956381, metadata={'description': 'batch_brunowaterfield_text_2'}, output_file_id='file-F4jFnNTuyvUBkP9KwihQbT', request_counts=BatchRequestCounts(completed=150, failed=0, total=150)),\n",
       " Batch(id='batch_67789799d99c81909a978bbe29a33e5e', completion_window='24h', created_at=1735956377, endpoint='/v1/chat/completions', input_file_id='file-QeSYqfi3zcUEJDW9BPT1Qz', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736042777, failed_at=1735956378, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_brianmoore_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778953e70548190a917ae41ce1d63eb', completion_window='24h', created_at=1735955774, endpoint='/v1/chat/completions', input_file_id='file-5tKrPpxJBypaNaTSeBipm6', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736042174, failed_at=1735955775, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_brianmoore_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778953b67d88190ac151256ec6f3c2c', completion_window='24h', created_at=1735955771, endpoint='/v1/chat/completions', input_file_id='file-1FgGYFi9E9FRaYn5h5SHSr', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736042171, failed_at=1735955772, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_borisjohnson_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778953875c8819080fb5ea6e7fd53d2', completion_window='24h', created_at=1735955768, endpoint='/v1/chat/completions', input_file_id='file-KQXoPDLSPWQKpAXEiKZGNY', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735956763, error_file_id=None, errors=None, expired_at=None, expires_at=1736042168, failed_at=None, finalizing_at=1735956714, in_progress_at=1735955769, metadata={'description': 'batch_borisjohnson_text_1'}, output_file_id='file-3K7NC9LYJVTQvcmB8AATH8', request_counts=BatchRequestCounts(completed=350, failed=0, total=350)),\n",
       " Batch(id='batch_677892dcf7908190a11b0dd7860b36e8', completion_window='24h', created_at=1735955165, endpoint='/v1/chat/completions', input_file_id='file-72MtRre9PhDGdoWMhPV21u', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736041565, failed_at=1735955165, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_beverleyturner_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677892dada7c81909c81bf6a5fe400d9', completion_window='24h', created_at=1735955162, endpoint='/v1/chat/completions', input_file_id='file-8hE7g31y2g6GWFnEZsGAg3', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736041562, failed_at=1735955164, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_beverleyturner_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677892d6a5a48190997ed9c355a34bc0', completion_window='24h', created_at=1735955158, endpoint='/v1/chat/completions', input_file_id='file-JP4eyH3cBQMetws5LsSCoU', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736041558, failed_at=1735955159, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_bernadettemcnulty_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778907b0954819087baee5c6a3207b2', completion_window='24h', created_at=1735954555, endpoint='/v1/chat/completions', input_file_id='file-M3EvpyevoYfEkZqnXkvrKw', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040955, failed_at=1735954555, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benmarlow_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67789078f46881908170704653786fcf', completion_window='24h', created_at=1735954553, endpoint='/v1/chat/completions', input_file_id='file-GPJDg6Qedw7Kn7mk528G2i', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040953, failed_at=1735954553, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benmarlow_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677890760c848190a21524211e35e541', completion_window='24h', created_at=1735954550, endpoint='/v1/chat/completions', input_file_id='file-RVfo1BbFuRrkHHkXTV1NTt', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040950, failed_at=1735954550, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benlawrence_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788e1b7c288190aec68d875a8d60c4', completion_window='24h', created_at=1735953947, endpoint='/v1/chat/completions', input_file_id='file-HjbBPfYtvwcH4am54DWREQ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040347, failed_at=1735953948, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benlawrence_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788e1903848190988d23be9e805778', completion_window='24h', created_at=1735953945, endpoint='/v1/chat/completions', input_file_id='file-YN1GD7rkcqh3GdT6rGfE9a', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040345, failed_at=1735953946, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benfogle_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788e157fb481909e916fa299a3586c', completion_window='24h', created_at=1735953941, endpoint='/v1/chat/completions', input_file_id='file-QQTvtfAJW34NXJVw6iSZYP', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736040341, failed_at=1735953942, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benfogle_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788bba56908190a56358a00ba83fff', completion_window='24h', created_at=1735953338, endpoint='/v1/chat/completions', input_file_id='file-GRFo3uWxeNXZwFjvKXaEjE', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736039738, failed_at=1735953339, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benfarmer_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788bb81e9c81908b4497c452b6a9eb', completion_window='24h', created_at=1735953336, endpoint='/v1/chat/completions', input_file_id='file-49stG1f7g4YNrLiEazQnDW', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736039736, failed_at=1735953336, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_benfarmer_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788bb5280c819096fd2ebd39c5c1ac', completion_window='24h', created_at=1735953333, endpoint='/v1/chat/completions', input_file_id='file-PV58RvTNj4nNFHEG5ipUvR', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736039733, failed_at=1735953333, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_beewilson_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788959dee88190b48089f1ec4d51fc', completion_window='24h', created_at=1735952729, endpoint='/v1/chat/completions', input_file_id='file-CvUzmY2PiQburkXkUipwN3', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736039129, failed_at=1735952730, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_auslancramb_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677889574f3c8190bb4e3118e7d4e674', completion_window='24h', created_at=1735952727, endpoint='/v1/chat/completions', input_file_id='file-Fq8joY5zWMo82rahzD4PZ1', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735952917, error_file_id=None, errors=None, expired_at=None, expires_at=1736039127, failed_at=None, finalizing_at=1735952897, in_progress_at=1735952728, metadata={'description': 'batch_auslancramb_text_1'}, output_file_id='file-9NKhEgWU2UWfLTmqpyQB4D', request_counts=BatchRequestCounts(completed=150, failed=0, total=150)),\n",
       " Batch(id='batch_677889540570819092483bdcedf6b0a9', completion_window='24h', created_at=1735952724, endpoint='/v1/chat/completions', input_file_id='file-ECR72D6vu46THkTN53GRgt', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735953438, error_file_id=None, errors=None, expired_at=None, expires_at=1736039124, failed_at=None, finalizing_at=1735953380, in_progress_at=1735952785, metadata={'description': 'batch_anthonyperegrine_text_3'}, output_file_id='file-GReg5TBdbMkrGSkjxTC8Si', request_counts=BatchRequestCounts(completed=440, failed=0, total=440)),\n",
       " Batch(id='batch_677886f8d0208190aef269b396b33abc', completion_window='24h', created_at=1735952120, endpoint='/v1/chat/completions', input_file_id='file-XdoiEfHQpocFKKc5XotrvX', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736038520, failed_at=1735952122, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_anthonyperegrine_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677886f662208190917ef20acdc17d01', completion_window='24h', created_at=1735952118, endpoint='/v1/chat/completions', input_file_id='file-2ALM7rMGzNfeDceWNr4Q6y', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735953906, error_file_id=None, errors=None, expired_at=None, expires_at=1736038518, failed_at=None, finalizing_at=1735953875, in_progress_at=1735952119, metadata={'description': 'batch_annebillson_text_3'}, output_file_id='file-UxVunzd6JRn75aWuuU9CG3', request_counts=BatchRequestCounts(completed=190, failed=0, total=190)),\n",
       " Batch(id='batch_677886f3b20481909efc47d408da1b47', completion_window='24h', created_at=1735952115, endpoint='/v1/chat/completions', input_file_id='file-XknHCDByDqBuZzoDJ7VgQ7', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735952410, error_file_id=None, errors=None, expired_at=None, expires_at=1736038515, failed_at=None, finalizing_at=1735952383, in_progress_at=1735952117, metadata={'description': 'batch_annebillson_text_1'}, output_file_id='file-74jJ1XuCBKcTPSeGjGDs4k', request_counts=BatchRequestCounts(completed=200, failed=0, total=200)),\n",
       " Batch(id='batch_67788498f5648190be0ef238cf7eb470', completion_window='24h', created_at=1735951513, endpoint='/v1/chat/completions', input_file_id='file-QXSfRtMWizMsj13HcBuTKt', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735953839, error_file_id=None, errors=None, expired_at=None, expires_at=1736037913, failed_at=None, finalizing_at=1735953790, in_progress_at=1735951514, metadata={'description': 'batch_annatyzack_text_2'}, output_file_id='file-NmRSuuMX9b3XoaDpug5bJ8', request_counts=BatchRequestCounts(completed=300, failed=0, total=300)),\n",
       " Batch(id='batch_67788495f9048190b3b56302cbfcf6a7', completion_window='24h', created_at=1735951510, endpoint='/v1/chat/completions', input_file_id='file-F65LqEb97rLAvV7QDF8hy9', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736037910, failed_at=1735951511, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_annatyzack_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778849284cc819091e59d8122264a0b', completion_window='24h', created_at=1735951506, endpoint='/v1/chat/completions', input_file_id='file-RTfj3JbCKYWTeMTQsBLebV', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736037906, failed_at=1735951507, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewmarszal_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788237fc68819086226a3a81e41a2f', completion_window='24h', created_at=1735950904, endpoint='/v1/chat/completions', input_file_id='file-K63h3Bm4v9Y1UzYPRjPBnQ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736037304, failed_at=1735950904, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewmarszal_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788235541881908f82c003c96075b7', completion_window='24h', created_at=1735950901, endpoint='/v1/chat/completions', input_file_id='file-WKw5nNXajdt1vX5vELDx2x', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736037301, failed_at=1735950902, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewgilligan_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67788233516c8190a9e231d25a7f2bbd', completion_window='24h', created_at=1735950899, endpoint='/v1/chat/completions', input_file_id='file-ETFY4GFncQ417H6vduVkW5', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736037299, failed_at=1735950900, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewgilligan_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787fd8b534819085aa15716b946a71', completion_window='24h', created_at=1735950296, endpoint='/v1/chat/completions', input_file_id='file-7MhXzNiavTjKmBqXbqU7nh', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736036696, failed_at=1735950297, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewenglish_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787fd577d08190b0ff9ee99c33ca32', completion_window='24h', created_at=1735950293, endpoint='/v1/chat/completions', input_file_id='file-5ZH5xLyaHaUWKxAYruFfQ4', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735951469, error_file_id=None, errors=None, expired_at=None, expires_at=1736036693, failed_at=None, finalizing_at=1735951439, in_progress_at=1735950294, metadata={'description': 'batch_andrewenglish_text_2'}, output_file_id='file-5yhRxhLvQV6QNgy1wzrBXs', request_counts=BatchRequestCounts(completed=200, failed=0, total=200)),\n",
       " Batch(id='batch_67787fd1b44481909a8cb6986586fa71', completion_window='24h', created_at=1735950289, endpoint='/v1/chat/completions', input_file_id='file-GbFBYE74kLUdZjHgEzRvSh', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736036689, failed_at=1735950290, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewcritchlow_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787d7661d48190b58f25a82c3a284f', completion_window='24h', created_at=1735949686, endpoint='/v1/chat/completions', input_file_id='file-FuXX61eH9KQwC1kfawr2u8', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736036086, failed_at=1735949687, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_andrewcritchlow_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787d740afc8190ab433c0d9806d572', completion_window='24h', created_at=1735949684, endpoint='/v1/chat/completions', input_file_id='file-LNDqMrQK8mo2bVWFHkKzxq', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736036084, failed_at=1735949684, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_ambroseevans_pritchard_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787d71de548190b10300f0f4c32b25', completion_window='24h', created_at=1735949681, endpoint='/v1/chat/completions', input_file_id='file-P3nXqH1k9VmE9PDDDo53zv', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736036081, failed_at=1735949683, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_ambroseevans_pritchard_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787b1683a08190af2cfc397ba2d535', completion_window='24h', created_at=1735949078, endpoint='/v1/chat/completions', input_file_id='file-XJkWXMgGGnuhLmm8XvCJF3', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736035478, failed_at=1735949079, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_allisterheath_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787b147b488190a8c41dae30783108', completion_window='24h', created_at=1735949076, endpoint='/v1/chat/completions', input_file_id='file-4YjbybDK6VKyhNjbn2uhWK', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736035476, failed_at=1735949077, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_allisterheath_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787b115d0481909b5ac87c7dfadd02', completion_window='24h', created_at=1735949073, endpoint='/v1/chat/completions', input_file_id='file-ECukZSeumsQp1CYQnXrRec', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736035473, failed_at=1735949074, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_allisonpearson_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677878b524448190b2b43085d2b5dfa0', completion_window='24h', created_at=1735948469, endpoint='/v1/chat/completions', input_file_id='file-YMWQ9i93Xr8L5rRjzEcciq', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736034869, failed_at=1735948469, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_allisonpearson_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677878b1bc34819084fa65e6ef682d23', completion_window='24h', created_at=1735948465, endpoint='/v1/chat/completions', input_file_id='file-Sur9EkAtjWR7e2ZiMmiuLw', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736034865, failed_at=1735948466, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alicevincent_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677878ae9084819090c2fd2691e4c14a', completion_window='24h', created_at=1735948462, endpoint='/v1/chat/completions', input_file_id='file-Du5d7k7PNGULmfU9B8bRxY', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736034862, failed_at=1735948463, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alicevincent_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787652c5ac8190aaa7adcf93d8433b', completion_window='24h', created_at=1735947858, endpoint='/v1/chat/completions', input_file_id='file-L6TEaYffrrbZ86fWGsLW1h', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736034258, failed_at=1735947859, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alicephilipson_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778765070a48190a0669a69de7febd1', completion_window='24h', created_at=1735947856, endpoint='/v1/chat/completions', input_file_id='file-GgKZhD4iAQX6HPPZE8S7Ux', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735948810, error_file_id=None, errors=None, expired_at=None, expires_at=1736034256, failed_at=None, finalizing_at=1735948785, in_progress_at=1735947857, metadata={'description': 'batch_alicephilipson_text_2'}, output_file_id='file-CBNBBNFYgWWtTosroS8k9f', request_counts=BatchRequestCounts(completed=160, failed=0, total=160)),\n",
       " Batch(id='batch_6778764e48588190b3acb0a9c3fa0b44', completion_window='24h', created_at=1735947854, endpoint='/v1/chat/completions', input_file_id='file-1V3vvssBCYDUSoVCKorLco', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735947985, error_file_id=None, errors=None, expired_at=None, expires_at=1736034254, failed_at=None, finalizing_at=1735947940, in_progress_at=1735947855, metadata={'description': 'batch_aliceaudley_text_3'}, output_file_id='file-GsizfNigVjqAzNiHHH3T18', request_counts=BatchRequestCounts(completed=340, failed=0, total=340)),\n",
       " Batch(id='batch_677873f2596c8190a6246b42207ac04e', completion_window='24h', created_at=1735947250, endpoint='/v1/chat/completions', input_file_id='file-5bSX7HVcM9icxgytWeG6AG', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033650, failed_at=1735947251, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_aliceaudley_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677873ef92a48190906a3105c77528c0', completion_window='24h', created_at=1735947247, endpoint='/v1/chat/completions', input_file_id='file-NjNJnhLPKX3UsdAt5t2vXD', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033647, failed_at=1735947248, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alexjames_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677873ec9ac88190a4dc83cff7a64891', completion_window='24h', created_at=1735947244, endpoint='/v1/chat/completions', input_file_id='file-SZDQfJ4sfyD9q3v8YL7NLf', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033644, failed_at=1735947245, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alexjames_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67787191376c8190839e77bd39715e49', completion_window='24h', created_at=1735946641, endpoint='/v1/chat/completions', input_file_id='file-W1AqhfWRCwiP9gC68rLCzX', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033041, failed_at=1735946641, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alastairsooke_text_3'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778718e8690819083e90b9aa2b03f4f', completion_window='24h', created_at=1735946638, endpoint='/v1/chat/completions', input_file_id='file-CWFkPvFVLHAgFF276HJnx6', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033038, failed_at=1735946639, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alastairsooke_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_6778718b08c8819088a048de61938043', completion_window='24h', created_at=1735946635, endpoint='/v1/chat/completions', input_file_id='file-WNdp1di2hxGu9skf6iKPgu', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736033035, failed_at=1735946636, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alastairsmart_text_2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_67786f2fed2c819081334f1e3c33f511', completion_window='24h', created_at=1735946032, endpoint='/v1/chat/completions', input_file_id='file-XFMfspEJ64svmAhWaprUBd', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735946341, error_file_id=None, errors=None, expired_at=None, expires_at=1736032432, failed_at=None, finalizing_at=1735946314, in_progress_at=1735946033, metadata={'description': 'batch_alastairsmart_text_1'}, output_file_id='file-X7TUTrQQV1qV5U3uXcAYDb', request_counts=BatchRequestCounts(completed=200, failed=0, total=200)),\n",
       " Batch(id='batch_67786f2df9c08190be74693d879e73dd', completion_window='24h', created_at=1735946030, endpoint='/v1/chat/completions', input_file_id='file-S6NBYwkFKFWq6YhuW1uRJF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1735947367, error_file_id=None, errors=None, expired_at=None, expires_at=1736032430, failed_at=None, finalizing_at=1735947342, in_progress_at=1735946031, metadata={'description': 'batch_alastairgood_text_3'}, output_file_id='file-PaeLy66xQhNKVwrDB5nZHe', request_counts=BatchRequestCounts(completed=220, failed=0, total=220)),\n",
       " Batch(id='batch_67786f2b76d88190bd6eda6b6757599e', completion_window='24h', created_at=1735946027, endpoint='/v1/chat/completions', input_file_id='file-K6aiJBQYxTyvHRVy7hKLQZ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-Sb2daX4hNaYB1SUdvsFFH7EZ. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736032427, failed_at=1735946028, finalizing_at=None, in_progress_at=None, metadata={'description': 'batch_alastairgood_text_1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_batches(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c313c75-28f6-46ca-a0ca-94f3d2bb8a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more batches to fetch.\n"
     ]
    }
   ],
   "source": [
    "batch_df = process_batches(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8e45039-0ea2-4a57-82c0-73f2a77527a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>completion_window</th>\n",
       "      <th>created_at</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>input_file_id</th>\n",
       "      <th>object</th>\n",
       "      <th>status</th>\n",
       "      <th>cancelled_at</th>\n",
       "      <th>cancelling_at</th>\n",
       "      <th>completed_at</th>\n",
       "      <th>...</th>\n",
       "      <th>expired_at</th>\n",
       "      <th>expires_at</th>\n",
       "      <th>failed_at</th>\n",
       "      <th>finalizing_at</th>\n",
       "      <th>in_progress_at</th>\n",
       "      <th>metadata_description</th>\n",
       "      <th>output_file_id</th>\n",
       "      <th>request_counts_completed</th>\n",
       "      <th>request_counts_failed</th>\n",
       "      <th>request_counts_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>batch_676825247ad48190a6f08d4e76e89017</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878500</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-6fi5kfDUeao21Js4JR8b6e</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964900</td>\n",
       "      <td>1.734878e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AdrianBridge</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>batch_676825270fcc8190a8ac2038f6faebcd</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878503</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-5iFGuwEexjn54QSuodQjeq</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964903</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AislinnLaing</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>batch_6768252a23488190af4daffcc4955131</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878506</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-PnkDD9DL3zB2MGjG9RoTDq</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964906</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanHansen</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>batch_6768252dc88481908aa81f3aee15f22c</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878509</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-Apr6U4sjd4U2cBftVJz7DN</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964909</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanSmith</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>batch_676825305b348190a3153dbb01be772c</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878512</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-SQX8GXzoqpKy2wWswojwxk</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964912</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanTitchmarsh</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_6778b8f1963c8190a1a9486e2b09cb43</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735964913</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-Wdngqpr2DRRQRCbeaVXj75</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051313</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_claireduffin_text_2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_6778bb4c3ca0819081007036dbdee1d5</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965516</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-UQGgQzHHYGs5iJZiR3bKHX</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051916</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_claireduffin_text_3</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_6778bb4f1464819086be5d9bdd7a0fe0</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965519</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-CzWaqbEcNuQbf9k4rre5B4</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051919</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_cliveaslet_text_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_6778bb51155c819090eb546005419653</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965521</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-SmeCQNkwfwVK2ZUYNzx8s9</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>batch_cliveaslet_text_3</td>\n",
       "      <td>file-K3YE81JzK7MTgXD6iK6CXb</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_6778bdaceca881908231dd60ac546e6f</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735966125</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-PqTfyspu9P3hg7UYqqJtrv</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736052525</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_clivejames_text_2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id completion_window  created_at  \\\n",
       "570  batch_676825247ad48190a6f08d4e76e89017               24h  1734878500   \n",
       "569  batch_676825270fcc8190a8ac2038f6faebcd               24h  1734878503   \n",
       "568  batch_6768252a23488190af4daffcc4955131               24h  1734878506   \n",
       "567  batch_6768252dc88481908aa81f3aee15f22c               24h  1734878509   \n",
       "566  batch_676825305b348190a3153dbb01be772c               24h  1734878512   \n",
       "..                                      ...               ...         ...   \n",
       "4    batch_6778b8f1963c8190a1a9486e2b09cb43               24h  1735964913   \n",
       "3    batch_6778bb4c3ca0819081007036dbdee1d5               24h  1735965516   \n",
       "2    batch_6778bb4f1464819086be5d9bdd7a0fe0               24h  1735965519   \n",
       "1    batch_6778bb51155c819090eb546005419653               24h  1735965521   \n",
       "0    batch_6778bdaceca881908231dd60ac546e6f               24h  1735966125   \n",
       "\n",
       "                 endpoint                input_file_id object     status  \\\n",
       "570  /v1/chat/completions  file-6fi5kfDUeao21Js4JR8b6e  batch     failed   \n",
       "569  /v1/chat/completions  file-5iFGuwEexjn54QSuodQjeq  batch     failed   \n",
       "568  /v1/chat/completions  file-PnkDD9DL3zB2MGjG9RoTDq  batch     failed   \n",
       "567  /v1/chat/completions  file-Apr6U4sjd4U2cBftVJz7DN  batch     failed   \n",
       "566  /v1/chat/completions  file-SQX8GXzoqpKy2wWswojwxk  batch     failed   \n",
       "..                    ...                          ...    ...        ...   \n",
       "4    /v1/chat/completions  file-Wdngqpr2DRRQRCbeaVXj75  batch     failed   \n",
       "3    /v1/chat/completions  file-UQGgQzHHYGs5iJZiR3bKHX  batch     failed   \n",
       "2    /v1/chat/completions  file-CzWaqbEcNuQbf9k4rre5B4  batch     failed   \n",
       "1    /v1/chat/completions  file-SmeCQNkwfwVK2ZUYNzx8s9  batch  completed   \n",
       "0    /v1/chat/completions  file-PqTfyspu9P3hg7UYqqJtrv  batch     failed   \n",
       "\n",
       "    cancelled_at cancelling_at  completed_at  ... expired_at  expires_at  \\\n",
       "570         None          None           NaN  ...       None  1734964900   \n",
       "569         None          None           NaN  ...       None  1734964903   \n",
       "568         None          None           NaN  ...       None  1734964906   \n",
       "567         None          None           NaN  ...       None  1734964909   \n",
       "566         None          None           NaN  ...       None  1734964912   \n",
       "..           ...           ...           ...  ...        ...         ...   \n",
       "4           None          None           NaN  ...       None  1736051313   \n",
       "3           None          None           NaN  ...       None  1736051916   \n",
       "2           None          None           NaN  ...       None  1736051919   \n",
       "1           None          None  1.735966e+09  ...       None  1736051921   \n",
       "0           None          None           NaN  ...       None  1736052525   \n",
       "\n",
       "        failed_at  finalizing_at  in_progress_at  \\\n",
       "570  1.734878e+09            NaN             NaN   \n",
       "569  1.734879e+09            NaN             NaN   \n",
       "568  1.734879e+09            NaN             NaN   \n",
       "567  1.734879e+09            NaN             NaN   \n",
       "566  1.734879e+09            NaN             NaN   \n",
       "..            ...            ...             ...   \n",
       "4    1.735965e+09            NaN             NaN   \n",
       "3    1.735966e+09            NaN             NaN   \n",
       "2    1.735966e+09            NaN             NaN   \n",
       "1             NaN   1.735966e+09    1.735966e+09   \n",
       "0    1.735966e+09            NaN             NaN   \n",
       "\n",
       "                   metadata_description               output_file_id  \\\n",
       "570    batch_The Telegraph_AdrianBridge                         None   \n",
       "569    batch_The Telegraph_AislinnLaing                         None   \n",
       "568      batch_The Telegraph_AlanHansen                         None   \n",
       "567       batch_The Telegraph_AlanSmith                         None   \n",
       "566  batch_The Telegraph_AlanTitchmarsh                         None   \n",
       "..                                  ...                          ...   \n",
       "4             batch_claireduffin_text_2                         None   \n",
       "3             batch_claireduffin_text_3                         None   \n",
       "2               batch_cliveaslet_text_1                         None   \n",
       "1               batch_cliveaslet_text_3  file-K3YE81JzK7MTgXD6iK6CXb   \n",
       "0               batch_clivejames_text_2                         None   \n",
       "\n",
       "    request_counts_completed request_counts_failed  request_counts_total  \n",
       "570                        0                     0                     0  \n",
       "569                        0                     0                     0  \n",
       "568                        0                     0                     0  \n",
       "567                        0                     0                     0  \n",
       "566                        0                     0                     0  \n",
       "..                       ...                   ...                   ...  \n",
       "4                          0                     0                     0  \n",
       "3                          0                     0                     0  \n",
       "2                          0                     0                     0  \n",
       "1                        220                     0                   220  \n",
       "0                          0                     0                     0  \n",
       "\n",
       "[571 rows x 22 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7c48418-87e9-4996-9b58-7b4c036beb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df = batch_df[batch_df['status']== 'failed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99c418a2-099e-4255-af44-b4c37f9c4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_df = batch_df[batch_df['status']== 'completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e88cca88-0d7b-448a-8a92-91ebe451ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_progress_df = batch_df[batch_df['status']== 'in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c5506b2-a6e7-4a47-a8a6-034864077d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: 466 - Completed: 105 - In Progress 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed: {len(failed_df)} - Completed: {len(completed_df)} - In Progress {len(in_progress_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f64e761c-e41d-474d-b29f-fa19cb7e1d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>completion_window</th>\n",
       "      <th>created_at</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>input_file_id</th>\n",
       "      <th>object</th>\n",
       "      <th>status</th>\n",
       "      <th>cancelled_at</th>\n",
       "      <th>cancelling_at</th>\n",
       "      <th>completed_at</th>\n",
       "      <th>...</th>\n",
       "      <th>expired_at</th>\n",
       "      <th>expires_at</th>\n",
       "      <th>failed_at</th>\n",
       "      <th>finalizing_at</th>\n",
       "      <th>in_progress_at</th>\n",
       "      <th>metadata_description</th>\n",
       "      <th>output_file_id</th>\n",
       "      <th>request_counts_completed</th>\n",
       "      <th>request_counts_failed</th>\n",
       "      <th>request_counts_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, completion_window, created_at, endpoint, input_file_id, object, status, cancelled_at, cancelling_at, completed_at, error_file_id, errors, expired_at, expires_at, failed_at, finalizing_at, in_progress_at, metadata_description, output_file_id, request_counts_completed, request_counts_failed, request_counts_total]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_progress_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25abd6dd-cecc-4570-8b2a-9e7077c78b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>completion_window</th>\n",
       "      <th>created_at</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>input_file_id</th>\n",
       "      <th>object</th>\n",
       "      <th>status</th>\n",
       "      <th>cancelled_at</th>\n",
       "      <th>cancelling_at</th>\n",
       "      <th>completed_at</th>\n",
       "      <th>...</th>\n",
       "      <th>expired_at</th>\n",
       "      <th>expires_at</th>\n",
       "      <th>failed_at</th>\n",
       "      <th>finalizing_at</th>\n",
       "      <th>in_progress_at</th>\n",
       "      <th>metadata_description</th>\n",
       "      <th>output_file_id</th>\n",
       "      <th>request_counts_completed</th>\n",
       "      <th>request_counts_failed</th>\n",
       "      <th>request_counts_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>batch_676875ee0be48190bffc2e78934bffee</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734899182</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-8Qep8LF4HmtkJRkskshbL1</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734985582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>1.734899e+09</td>\n",
       "      <td>batch_adrianbridge_text_1</td>\n",
       "      <td>file-19qKyHbN4tuMgXfHQi5s9w</td>\n",
       "      <td>380</td>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>batch_676875f1296c81909ddeb532ce4de20e</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734899185</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-XKGGSKR7MuabVoLoTDAnJh</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734985585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>1.734899e+09</td>\n",
       "      <td>batch_adrianbridge_text_2</td>\n",
       "      <td>file-9hdr8gKLqkwHPN4SmDAsf9</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>batch_676875f3ba5c8190bea3ac9d42fd3cdb</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734899187</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-AJVucrCogsLi9q2c8Wu5Dn</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.734902e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734985587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.734902e+09</td>\n",
       "      <td>1.734899e+09</td>\n",
       "      <td>batch_aislinnlaing_text_1</td>\n",
       "      <td>file-MCGCkkYk46DAbgucQezq2e</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>batch_676875f524e88190a721fb66be28f4b1</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734899189</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-RHEBxSMqnYS8aMPb2af8rH</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734985589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>1.734899e+09</td>\n",
       "      <td>batch_aislinnlaing_text_2</td>\n",
       "      <td>file-1GEm5w318zftkda4uk5HZo</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>batch_676875f824308190ad32fdfb2068147e</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734899192</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-EAxdwcvGhMEMeDHSu4w2nC</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734985592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.734909e+09</td>\n",
       "      <td>1.734899e+09</td>\n",
       "      <td>batch_alanhansen_text_1</td>\n",
       "      <td>file-4SYJYZySQxbDGrjs5y3ywj</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>batch_67789c5daf70819099ab56d59f346e4f</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735957597</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-X9RbfYKHAq3DQi54Z3rxcQ</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735959e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736043997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735959e+09</td>\n",
       "      <td>1.735958e+09</td>\n",
       "      <td>batch_cameronmacphail_text_1</td>\n",
       "      <td>file-1175iJrjAtiW4bPTk9FdKe</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>batch_6778b42e57d88190ba0e3dbeeec62b98</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735963694</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-GSBRAw4Bzg5SpGUsjai7Ng</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735964e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736050094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735964e+09</td>\n",
       "      <td>1.735964e+09</td>\n",
       "      <td>batch_christopherwilliams_text_1</td>\n",
       "      <td>file-UX4YaHJX9YMNtZFKVhGHHU</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>batch_6778b6898cf08190919b1f3caba56d78</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735964297</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-J9EAjZcmhR8CASK4Z4aZi7</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736050697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>1.735964e+09</td>\n",
       "      <td>batch_christopherwilliams_text_3</td>\n",
       "      <td>file-M4tUAAcYE7F5a2zSFctCV2</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>batch_6778b8eef6248190b4a5e7fb48f06102</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735964911</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-U5ZrckUgRqUcE4Qn2YpU7E</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>batch_clairecohen_text_3</td>\n",
       "      <td>file-KCgrvrNNhN4Vdxvkk4WBNJ</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batch_6778bb51155c819090eb546005419653</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965521</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-SmeCQNkwfwVK2ZUYNzx8s9</td>\n",
       "      <td>batch</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>batch_cliveaslet_text_3</td>\n",
       "      <td>file-K3YE81JzK7MTgXD6iK6CXb</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id completion_window  created_at  \\\n",
       "460  batch_676875ee0be48190bffc2e78934bffee               24h  1734899182   \n",
       "459  batch_676875f1296c81909ddeb532ce4de20e               24h  1734899185   \n",
       "458  batch_676875f3ba5c8190bea3ac9d42fd3cdb               24h  1734899187   \n",
       "457  batch_676875f524e88190a721fb66be28f4b1               24h  1734899189   \n",
       "456  batch_676875f824308190ad32fdfb2068147e               24h  1734899192   \n",
       "..                                      ...               ...         ...   \n",
       "42   batch_67789c5daf70819099ab56d59f346e4f               24h  1735957597   \n",
       "10   batch_6778b42e57d88190ba0e3dbeeec62b98               24h  1735963694   \n",
       "9    batch_6778b6898cf08190919b1f3caba56d78               24h  1735964297   \n",
       "5    batch_6778b8eef6248190b4a5e7fb48f06102               24h  1735964911   \n",
       "1    batch_6778bb51155c819090eb546005419653               24h  1735965521   \n",
       "\n",
       "                 endpoint                input_file_id object     status  \\\n",
       "460  /v1/chat/completions  file-8Qep8LF4HmtkJRkskshbL1  batch  completed   \n",
       "459  /v1/chat/completions  file-XKGGSKR7MuabVoLoTDAnJh  batch  completed   \n",
       "458  /v1/chat/completions  file-AJVucrCogsLi9q2c8Wu5Dn  batch  completed   \n",
       "457  /v1/chat/completions  file-RHEBxSMqnYS8aMPb2af8rH  batch  completed   \n",
       "456  /v1/chat/completions  file-EAxdwcvGhMEMeDHSu4w2nC  batch  completed   \n",
       "..                    ...                          ...    ...        ...   \n",
       "42   /v1/chat/completions  file-X9RbfYKHAq3DQi54Z3rxcQ  batch  completed   \n",
       "10   /v1/chat/completions  file-GSBRAw4Bzg5SpGUsjai7Ng  batch  completed   \n",
       "9    /v1/chat/completions  file-J9EAjZcmhR8CASK4Z4aZi7  batch  completed   \n",
       "5    /v1/chat/completions  file-U5ZrckUgRqUcE4Qn2YpU7E  batch  completed   \n",
       "1    /v1/chat/completions  file-SmeCQNkwfwVK2ZUYNzx8s9  batch  completed   \n",
       "\n",
       "    cancelled_at cancelling_at  completed_at  ... expired_at  expires_at  \\\n",
       "460         None          None  1.734909e+09  ...       None  1734985582   \n",
       "459         None          None  1.734909e+09  ...       None  1734985585   \n",
       "458         None          None  1.734902e+09  ...       None  1734985587   \n",
       "457         None          None  1.734909e+09  ...       None  1734985589   \n",
       "456         None          None  1.734909e+09  ...       None  1734985592   \n",
       "..           ...           ...           ...  ...        ...         ...   \n",
       "42          None          None  1.735959e+09  ...       None  1736043997   \n",
       "10          None          None  1.735964e+09  ...       None  1736050094   \n",
       "9           None          None  1.735965e+09  ...       None  1736050697   \n",
       "5           None          None  1.735966e+09  ...       None  1736051311   \n",
       "1           None          None  1.735966e+09  ...       None  1736051921   \n",
       "\n",
       "    failed_at  finalizing_at  in_progress_at  \\\n",
       "460       NaN   1.734909e+09    1.734899e+09   \n",
       "459       NaN   1.734909e+09    1.734899e+09   \n",
       "458       NaN   1.734902e+09    1.734899e+09   \n",
       "457       NaN   1.734909e+09    1.734899e+09   \n",
       "456       NaN   1.734909e+09    1.734899e+09   \n",
       "..        ...            ...             ...   \n",
       "42        NaN   1.735959e+09    1.735958e+09   \n",
       "10        NaN   1.735964e+09    1.735964e+09   \n",
       "9         NaN   1.735965e+09    1.735964e+09   \n",
       "5         NaN   1.735966e+09    1.735965e+09   \n",
       "1         NaN   1.735966e+09    1.735966e+09   \n",
       "\n",
       "                 metadata_description               output_file_id  \\\n",
       "460         batch_adrianbridge_text_1  file-19qKyHbN4tuMgXfHQi5s9w   \n",
       "459         batch_adrianbridge_text_2  file-9hdr8gKLqkwHPN4SmDAsf9   \n",
       "458         batch_aislinnlaing_text_1  file-MCGCkkYk46DAbgucQezq2e   \n",
       "457         batch_aislinnlaing_text_2  file-1GEm5w318zftkda4uk5HZo   \n",
       "456           batch_alanhansen_text_1  file-4SYJYZySQxbDGrjs5y3ywj   \n",
       "..                                ...                          ...   \n",
       "42       batch_cameronmacphail_text_1  file-1175iJrjAtiW4bPTk9FdKe   \n",
       "10   batch_christopherwilliams_text_1  file-UX4YaHJX9YMNtZFKVhGHHU   \n",
       "9    batch_christopherwilliams_text_3  file-M4tUAAcYE7F5a2zSFctCV2   \n",
       "5            batch_clairecohen_text_3  file-KCgrvrNNhN4Vdxvkk4WBNJ   \n",
       "1             batch_cliveaslet_text_3  file-K3YE81JzK7MTgXD6iK6CXb   \n",
       "\n",
       "    request_counts_completed request_counts_failed  request_counts_total  \n",
       "460                      380                     0                   380  \n",
       "459                      230                     0                   230  \n",
       "458                      150                     0                   150  \n",
       "457                      190                     0                   190  \n",
       "456                      310                     0                   310  \n",
       "..                       ...                   ...                   ...  \n",
       "42                      1050                     0                  1050  \n",
       "10                       130                     0                   130  \n",
       "9                        190                     0                   190  \n",
       "5                        290                     0                   290  \n",
       "1                        220                     0                   220  \n",
       "\n",
       "[105 rows x 22 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a79b042-4aef-45f4-9e73-49f58f16ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'completion_window', 'created_at', 'endpoint', 'input_file_id',\n",
       "       'object', 'status', 'cancelled_at', 'cancelling_at', 'completed_at',\n",
       "       'error_file_id', 'errors', 'expired_at', 'expires_at', 'failed_at',\n",
       "       'finalizing_at', 'in_progress_at', 'metadata_description',\n",
       "       'output_file_id', 'request_counts_completed', 'request_counts_failed',\n",
       "       'request_counts_total'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_df[completed_df['output_file_id'] == 'file-EVpH09R0q5PPlCo217pR5mXL'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4aeca0a9-c361-45cc-b58c-e6397f3b8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process files\n",
    "def process_batch_files(batch_complete_loc, batch_sent_loc, completed_df):\n",
    "    # Define the function to extract the content from the response\n",
    "    def extract_content(response):\n",
    "        try:\n",
    "            content = response['body']['choices'][0]['message']['content']\n",
    "            return content\n",
    "        except (KeyError, TypeError):\n",
    "            return None\n",
    "\n",
    "    # Get the list of files already in the batch_complete_loc\n",
    "    existing_files = [f.replace('.jsonl', '') for f in os.listdir(batch_complete_loc) if f.endswith('.jsonl')]\n",
    "\n",
    "    # Filter out rows in completed_df where metadata_description matches existing files\n",
    "    df_to_process = completed_df[~completed_df['metadata_description'].isin(existing_files)]\n",
    "\n",
    "    # Loop through the rows in the filtered DataFrame\n",
    "    for index, row in df_to_process.iterrows():\n",
    "        metadata_description = row['metadata_description']\n",
    "        output_file_id = row['output_file_id']\n",
    "        print(output_file_id)\n",
    "\n",
    "        # Call the API to get the file content\n",
    "        file_response = client.files.content(output_file_id)\n",
    "        jsonl_io = StringIO(file_response.text)\n",
    "        df = pd.read_json(jsonl_io, lines=True)\n",
    "\n",
    "        # Apply the function to extract the 'content' from the 'response' column\n",
    "        df['response'] = df['response'].apply(extract_content)\n",
    "\n",
    "        # Select only the required columns\n",
    "        df = df[['id', 'custom_id', 'response']]\n",
    "\n",
    "        # Save the DataFrame as a jsonl file in batch_complete_loc\n",
    "        output_filepath = os.path.join(batch_complete_loc, f\"{metadata_description}.jsonl\")\n",
    "        df.to_json(output_filepath, orient='records', lines=True)\n",
    "\n",
    "        # Move the file from batch_sent_loc to batch_complete_loc\n",
    "        sent_filepath = os.path.join(batch_sent_loc, f\"{metadata_description}.jsonl\")\n",
    "        if os.path.exists(sent_filepath):\n",
    "            os.remove(sent_filepath)\n",
    "            print(f\"File {sent_filepath} moved to {batch_complete_loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3bc6340-48e9-4919-8f38-625356042177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-3QZLvYNSTsAGM7EtGWA1Uh\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_helenbrown_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-LT2zpgoXBnxAWhkF1Dsj2q\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_helenyemm_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-Vij52HASdnWPmd6p8eUnVu\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_helenyemm_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-32DZphWVK5hqJgxLAQyUEF\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_henrysamuel_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-6yrizdbb8vKV1sCvX8tirc\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_henrywinter_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-2hN8yHiLiMTbvizG85gD2i\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_henrywinter_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-GaJNURU5pdGoS3TPZTn2eG\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_hollywatt_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-6UorSfkJ5iG1ycp5TNQWRm\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_horatiaharrod_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-BEMiBMrUHZLYccogzh6brS\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_hughmorris_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-XKKkb9w1jUndhFVigAJkRT\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_hughmorris_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-KqvtbQypJD6x3NGb1Qqr9R\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_craigmclean_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-EPKXhKQRywYTjv5tPR8o9o\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_danieljohnson_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-NYbv2QhDTaepMnQQzFYmcR\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_davidblair_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-RY5PWs7vZRAX8LqFXmbLmp\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_davidgritten_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-X1LWvzQb9mUuspkGTvr8u6\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_dianahenry_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-QC8ccc1LtdV1xUf7dYV8ec\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_elizabethgrice_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-WnF5noYGQphVCcqwakSrih\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_gabywood_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-3xAA7tV8CmLjd56NnDhx68\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_garethadavies_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-UydoYQTiAhLG19Ehbu1SRf\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_gavinmairs_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-WqyAMvDinDRZStvds8Zzo9\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_gilesmole_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-1xZBXbqm3HDoWMbqzvHFkY\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_grahamruddick_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-Epck6eivo2c1p7Pa15Qtxn\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_gregorywalton_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-QU9rn312Gh72yzmy9vi58Q\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_hannahstrange_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-2ZjPpnaMaYBxSVq1rptEY8\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_harrywallop_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-Af4tvKWfEYxyvFpBfXZibd\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_alasdairreid_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-PaeLy66xQhNKVwrDB5nZHe\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_alastairgood_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-X7TUTrQQV1qV5U3uXcAYDb\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_alastairsmart_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-GsizfNigVjqAzNiHHH3T18\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_aliceaudley_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-CBNBBNFYgWWtTosroS8k9f\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_alicephilipson_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-5yhRxhLvQV6QNgy1wzrBXs\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_andrewenglish_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-NmRSuuMX9b3XoaDpug5bJ8\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_annatyzack_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-74jJ1XuCBKcTPSeGjGDs4k\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_annebillson_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-UxVunzd6JRn75aWuuU9CG3\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_annebillson_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-GReg5TBdbMkrGSkjxTC8Si\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_anthonyperegrine_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-9NKhEgWU2UWfLTmqpyQB4D\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_auslancramb_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-3K7NC9LYJVTQvcmB8AATH8\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_borisjohnson_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-F4jFnNTuyvUBkP9KwihQbT\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_brunowaterfield_text_2.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-1175iJrjAtiW4bPTk9FdKe\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_cameronmacphail_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-UX4YaHJX9YMNtZFKVhGHHU\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_christopherwilliams_text_1.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-M4tUAAcYE7F5a2zSFctCV2\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_christopherwilliams_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-KCgrvrNNhN4Vdxvkk4WBNJ\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_clairecohen_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n",
      "file-K3YE81JzK7MTgXD6iK6CXb\n",
      "File /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/batch_cliveaslet_text_3.jsonl moved to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_complete/\n"
     ]
    }
   ],
   "source": [
    "process_batch_files(batch_complete_loc, batch_sent_loc, completed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "191b45d3-ccca-4b08-b101-fd76a6792f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>completion_window</th>\n",
       "      <th>created_at</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>input_file_id</th>\n",
       "      <th>object</th>\n",
       "      <th>status</th>\n",
       "      <th>cancelled_at</th>\n",
       "      <th>cancelling_at</th>\n",
       "      <th>completed_at</th>\n",
       "      <th>...</th>\n",
       "      <th>expired_at</th>\n",
       "      <th>expires_at</th>\n",
       "      <th>failed_at</th>\n",
       "      <th>finalizing_at</th>\n",
       "      <th>in_progress_at</th>\n",
       "      <th>metadata_description</th>\n",
       "      <th>output_file_id</th>\n",
       "      <th>request_counts_completed</th>\n",
       "      <th>request_counts_failed</th>\n",
       "      <th>request_counts_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>batch_676825247ad48190a6f08d4e76e89017</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878500</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-6fi5kfDUeao21Js4JR8b6e</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964900</td>\n",
       "      <td>1.734878e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AdrianBridge</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>batch_676825270fcc8190a8ac2038f6faebcd</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878503</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-5iFGuwEexjn54QSuodQjeq</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964903</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AislinnLaing</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>batch_6768252a23488190af4daffcc4955131</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878506</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-PnkDD9DL3zB2MGjG9RoTDq</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964906</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanHansen</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>batch_6768252dc88481908aa81f3aee15f22c</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878509</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-Apr6U4sjd4U2cBftVJz7DN</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964909</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanSmith</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>batch_676825305b348190a3153dbb01be772c</td>\n",
       "      <td>24h</td>\n",
       "      <td>1734878512</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-SQX8GXzoqpKy2wWswojwxk</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1734964912</td>\n",
       "      <td>1.734879e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_The Telegraph_AlanTitchmarsh</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>batch_6778b8eb67b08190bfda1a4c323e293f</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735964907</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-6ZcnBw7LfYzRRj4PgGViJV</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051307</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_clairecohen_text_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batch_6778b8f1963c8190a1a9486e2b09cb43</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735964913</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-Wdngqpr2DRRQRCbeaVXj75</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051313</td>\n",
       "      <td>1.735965e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_claireduffin_text_2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batch_6778bb4c3ca0819081007036dbdee1d5</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965516</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-UQGgQzHHYGs5iJZiR3bKHX</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051916</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_claireduffin_text_3</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batch_6778bb4f1464819086be5d9bdd7a0fe0</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735965519</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-CzWaqbEcNuQbf9k4rre5B4</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736051919</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_cliveaslet_text_1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batch_6778bdaceca881908231dd60ac546e6f</td>\n",
       "      <td>24h</td>\n",
       "      <td>1735966125</td>\n",
       "      <td>/v1/chat/completions</td>\n",
       "      <td>file-PqTfyspu9P3hg7UYqqJtrv</td>\n",
       "      <td>batch</td>\n",
       "      <td>failed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1736052525</td>\n",
       "      <td>1.735966e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>batch_clivejames_text_2</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id completion_window  created_at  \\\n",
       "570  batch_676825247ad48190a6f08d4e76e89017               24h  1734878500   \n",
       "569  batch_676825270fcc8190a8ac2038f6faebcd               24h  1734878503   \n",
       "568  batch_6768252a23488190af4daffcc4955131               24h  1734878506   \n",
       "567  batch_6768252dc88481908aa81f3aee15f22c               24h  1734878509   \n",
       "566  batch_676825305b348190a3153dbb01be772c               24h  1734878512   \n",
       "..                                      ...               ...         ...   \n",
       "6    batch_6778b8eb67b08190bfda1a4c323e293f               24h  1735964907   \n",
       "4    batch_6778b8f1963c8190a1a9486e2b09cb43               24h  1735964913   \n",
       "3    batch_6778bb4c3ca0819081007036dbdee1d5               24h  1735965516   \n",
       "2    batch_6778bb4f1464819086be5d9bdd7a0fe0               24h  1735965519   \n",
       "0    batch_6778bdaceca881908231dd60ac546e6f               24h  1735966125   \n",
       "\n",
       "                 endpoint                input_file_id object  status  \\\n",
       "570  /v1/chat/completions  file-6fi5kfDUeao21Js4JR8b6e  batch  failed   \n",
       "569  /v1/chat/completions  file-5iFGuwEexjn54QSuodQjeq  batch  failed   \n",
       "568  /v1/chat/completions  file-PnkDD9DL3zB2MGjG9RoTDq  batch  failed   \n",
       "567  /v1/chat/completions  file-Apr6U4sjd4U2cBftVJz7DN  batch  failed   \n",
       "566  /v1/chat/completions  file-SQX8GXzoqpKy2wWswojwxk  batch  failed   \n",
       "..                    ...                          ...    ...     ...   \n",
       "6    /v1/chat/completions  file-6ZcnBw7LfYzRRj4PgGViJV  batch  failed   \n",
       "4    /v1/chat/completions  file-Wdngqpr2DRRQRCbeaVXj75  batch  failed   \n",
       "3    /v1/chat/completions  file-UQGgQzHHYGs5iJZiR3bKHX  batch  failed   \n",
       "2    /v1/chat/completions  file-CzWaqbEcNuQbf9k4rre5B4  batch  failed   \n",
       "0    /v1/chat/completions  file-PqTfyspu9P3hg7UYqqJtrv  batch  failed   \n",
       "\n",
       "    cancelled_at cancelling_at  completed_at  ... expired_at  expires_at  \\\n",
       "570         None          None           NaN  ...       None  1734964900   \n",
       "569         None          None           NaN  ...       None  1734964903   \n",
       "568         None          None           NaN  ...       None  1734964906   \n",
       "567         None          None           NaN  ...       None  1734964909   \n",
       "566         None          None           NaN  ...       None  1734964912   \n",
       "..           ...           ...           ...  ...        ...         ...   \n",
       "6           None          None           NaN  ...       None  1736051307   \n",
       "4           None          None           NaN  ...       None  1736051313   \n",
       "3           None          None           NaN  ...       None  1736051916   \n",
       "2           None          None           NaN  ...       None  1736051919   \n",
       "0           None          None           NaN  ...       None  1736052525   \n",
       "\n",
       "        failed_at  finalizing_at  in_progress_at  \\\n",
       "570  1.734878e+09            NaN             NaN   \n",
       "569  1.734879e+09            NaN             NaN   \n",
       "568  1.734879e+09            NaN             NaN   \n",
       "567  1.734879e+09            NaN             NaN   \n",
       "566  1.734879e+09            NaN             NaN   \n",
       "..            ...            ...             ...   \n",
       "6    1.735965e+09            NaN             NaN   \n",
       "4    1.735965e+09            NaN             NaN   \n",
       "3    1.735966e+09            NaN             NaN   \n",
       "2    1.735966e+09            NaN             NaN   \n",
       "0    1.735966e+09            NaN             NaN   \n",
       "\n",
       "                   metadata_description  output_file_id  \\\n",
       "570    batch_The Telegraph_AdrianBridge            None   \n",
       "569    batch_The Telegraph_AislinnLaing            None   \n",
       "568      batch_The Telegraph_AlanHansen            None   \n",
       "567       batch_The Telegraph_AlanSmith            None   \n",
       "566  batch_The Telegraph_AlanTitchmarsh            None   \n",
       "..                                  ...             ...   \n",
       "6              batch_clairecohen_text_1            None   \n",
       "4             batch_claireduffin_text_2            None   \n",
       "3             batch_claireduffin_text_3            None   \n",
       "2               batch_cliveaslet_text_1            None   \n",
       "0               batch_clivejames_text_2            None   \n",
       "\n",
       "    request_counts_completed request_counts_failed  request_counts_total  \n",
       "570                        0                     0                     0  \n",
       "569                        0                     0                     0  \n",
       "568                        0                     0                     0  \n",
       "567                        0                     0                     0  \n",
       "566                        0                     0                     0  \n",
       "..                       ...                   ...                   ...  \n",
       "6                          0                     0                     0  \n",
       "4                          0                     0                     0  \n",
       "3                          0                     0                     0  \n",
       "2                          0                     0                     0  \n",
       "0                          0                     0                     0  \n",
       "\n",
       "[466 rows x 22 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd8e82bd-3b7a-4a8c-b25b-ea61bfc1cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to handle failed files\n",
    "def handle_failed_files(batch_loc, batch_sent, failed_df):\n",
    "    # Loop through the rows in the failed_df DataFrame\n",
    "    for index, row in failed_df.iterrows():\n",
    "        metadata_description = row['metadata_description']\n",
    "        filename = f\"{metadata_description}.jsonl\"\n",
    "        \n",
    "        # Define the source and destination file paths\n",
    "        sent_filepath = os.path.join(batch_sent, filename)\n",
    "        loc_filepath = os.path.join(batch_loc, filename)\n",
    "        \n",
    "        # Check if the file exists in batch_sent\n",
    "        if os.path.exists(sent_filepath):\n",
    "            # Move the file to batch_loc\n",
    "            shutil.move(sent_filepath, loc_filepath)\n",
    "            print(f\"File {filename} moved from {batch_sent} to {batch_loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2e76b8-b28a-45e7-8245-0fbd85212ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File batch_alansmith_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alantitchmarsh_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alantovey_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alantovey_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alasdairreid_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alastairgood_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alastairsmart_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alastairsooke_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alastairsooke_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alexjames_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alexjames_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_aliceaudley_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alicephilipson_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alicevincent_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_alicevincent_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_allisonpearson_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_allisonpearson_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_allisterheath_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_allisterheath_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_ambroseevans_pritchard_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_ambroseevans_pritchard_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewcritchlow_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewcritchlow_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewenglish_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewgilligan_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewgilligan_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewmarszal_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_andrewmarszal_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_annatyzack_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_anthonyperegrine_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_auslancramb_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_beewilson_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benfarmer_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benfarmer_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benfogle_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benfogle_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benlawrence_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benlawrence_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benmarlow_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_benmarlow_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_bernadettemcnulty_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_beverleyturner_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_beverleyturner_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_borisjohnson_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_brianmoore_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_brianmoore_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_brunowaterfield_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_bryonygordon_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_bunnyguinness_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_bunnyguinness_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_cameronmacphail_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_camillaturner_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_camillaturner_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolinekent_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolinekent_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolinemcghie_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolinemcghie_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolynhart_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_carolynhart_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_catalinastogdon_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_catalinastogdon_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_catherinegee_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlesmoore_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlesmoore_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlesspencer_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlesspencer_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlottebeugge_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlottebeugge_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlotteruncie_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_charlotteruncie_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_chrisharvey_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_chrisharvey_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_chrisknapman_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_chrisknapman_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christopherbooker_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christopherbooker_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christopherhope_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christopherhope_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christopherhowse_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christophermiddleton_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_christophermiddleton_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_clairecarter_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_clairecarter_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_clairecohen_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_claireduffin_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_claireduffin_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_cliveaslet_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_clivejames_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_craigmclean_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_davidblair_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_davidgritten_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_deannelson_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_derekpringle_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_dianahenry_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_edcumming_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_edcumming_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_emilygosden_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_gabywood_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_gavinmairs_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_geoffreyboycott_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_geoffreylean_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_gilesmole_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_gordonrayner_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_gordonrayner_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_graemepaton_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_graemepaton_text_3.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_grahamnorton_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_grahamnorton_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_grahamruddick_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_hannahfurness_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_hannahstrange_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_harrietalexander_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_harrydequetteville_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_harrywallop_text_1.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n",
      "File batch_horatiaharrod_text_2.jsonl moved from /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_sent/ to /Volumes/BCross/datasets/author_verification/training/The Telegraph/batch_sentence_preprocessed/\n"
     ]
    }
   ],
   "source": [
    "handle_failed_files(batch_loc, batch_sent_loc, failed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e6c83-8d2a-495b-9949-21dc7c77a359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
