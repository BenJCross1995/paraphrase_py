{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1645378-31c9-4c10-a4f8-d251feaaa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "# from parascore import ParaScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83405597-8844-4bbe-933e-7442cb631c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_loc = \"../../credentials.json\"\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"Wiki\"\n",
    "\n",
    "data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/\"\n",
    "\n",
    "raw_data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "\n",
    "save_loc = f\"{data_loc}full_doc_paraphrase/\"\n",
    "os.makedirs(save_loc, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb961d-9a4b-4062-b20e-561e6a01e6a3",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f71179d-4c9f-458c-a6d6-7ce892c91724",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(credential_loc, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = data['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# scorer = ParaScorer(lang=\"en\", model_type='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf091903-fa1b-4c4b-9ac0-7f97a3d98cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your role is to function as an advanced paraphrasing assistant. Your task is to generate a fully paraphrased version of a given document that preserves its original meaning, tone, genre, and style, while exhibiting significantly heightened lexical diversity and structural transformation. The aim is to produce a document that reflects a broad, globally influenced language profile for authorship verification research.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1. **Preserve Core Meaning & Intent:**  \n",
    "   - Ensure that the paraphrased text maintains the original document’s logical flow, factual accuracy, and overall message.  \n",
    "   - Retain the tone, style, and genre to match the source content precisely.\n",
    "\n",
    "2. **Maximize Lexical Diversity:**  \n",
    "   - Use an extensive range of synonyms, idiomatic expressions, and alternative phrasings to replace common expressions.  \n",
    "   - Avoid repetitive language; introduce varied vocabulary throughout the document to ensure a fresh linguistic perspective.\n",
    "\n",
    "3. **Transform Structural Elements:**  \n",
    "   - Reorganize sentences and paragraphs: invert sentence structures, vary sentence lengths, and use different clause orders.  \n",
    "   - Experiment with alternative grammatical constructions and narrative flows without compromising clarity or meaning.\n",
    "\n",
    "4. **Preserve Critical Terms & Proper Nouns:**  \n",
    "   - Do not alter technical terms, names, or key references unless explicitly instructed.  \n",
    "   - Ensure these elements remain intact to maintain the document's integrity.\n",
    "\n",
    "5. **Ensure Naturalness & Cohesion:**  \n",
    "   - Despite extensive lexical and structural changes, the paraphrased document must remain coherent, natural, and easily understandable.  \n",
    "   - Strive for a balanced output that is both distinct in language and faithful to the original content.\n",
    "\n",
    "6. **Output Format:**  \n",
    "   - Provide only the paraphrased document without any extra commentary or explanations.  \n",
    "   - The output must be structured in JSON format as follows:  \n",
    "\n",
    "     {\"new_document\": <paraphrased_document>}\n",
    "\n",
    "Instructions:\n",
    "- Prioritize high lexical variation and significant syntactic reordering.\n",
    "- Create a paraphrase that is distinct in wording and structure from the source while fully retaining its meaning, tone, and intent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376846e-d24a-4d9f-9070-7184b8b0ceaa",
   "metadata": {},
   "source": [
    "### Data Prep Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458c35c7-f379-415c-af73-ac421526fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "def write_jsonl(data, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for _, row in data.iterrows():\n",
    "            json.dump(row.to_dict(), file)\n",
    "            file.write('\\n')\n",
    "            \n",
    "def create_temp_doc_id(input_text):\n",
    "    # Extract everything between the brackets\n",
    "    match = re.search(r'\\[(.*?)\\]', input_text)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        # Replace all punctuation and spaces with \"_\"\n",
    "        cleaned_text = re.sub(r'[^\\w]', '_', extracted_text)\n",
    "        # Replace multiple underscores with a single \"_\"\n",
    "        final_text = re.sub(r'_{2,}', '_', cleaned_text)\n",
    "        return final_text.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6eda8a-e81f-4772-b985-2e3dabbe5f39",
   "metadata": {},
   "source": [
    "### Paraphrase Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c47a17-36e5-4c94-9f98-08e048435da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_call(text, system_prompt, client, n=10, model=\"gpt-4o-mini\", temperature=0.7, top_p=0.9, **kwargs):\n",
    "    \"\"\"\n",
    "    Calls the LLM with the specified hyperparameters.\n",
    "    Returns the completion response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        n=n,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        **kwargs\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def paraphrase_dataframe(df, system_prompt, client, n=10, m=1, **llm_params):\n",
    "    \"\"\"\n",
    "    Iterates over the DataFrame rows and for each text calls the paraphrase LLM m times,\n",
    "    with n completions each time. For every generated paraphrase, it creates a new row that\n",
    "    contains the original data, the paraphrased text, and all provided hyperparameter settings.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        for _ in range(m):\n",
    "            response = paraphrase_call(text, system_prompt, client, n=n, **llm_params)\n",
    "            # Process each completion (choice) from the LLM response.\n",
    "            for choice in response.choices:\n",
    "                paraphrased_text = json.loads(choice.message.content)['new_document']\n",
    "                new_row = row.copy()\n",
    "                new_row['paraphrased_text'] = paraphrased_text\n",
    "                # Save every hyperparameter in the row.\n",
    "                for param, value in llm_params.items():\n",
    "                    new_row[param] = value\n",
    "                expanded_rows.append(new_row)\n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "def grid_search_paraphrases(df, system_prompt, client, param_grid, sample_size=10, m=1, n_completions=10):\n",
    "    \"\"\"\n",
    "    Selects a random subset of rows from the DataFrame (sample_size) and then iterates over\n",
    "    all hyperparameter combinations from param_grid. For each combination, it calls paraphrase_dataframe,\n",
    "    ensuring that each generated paraphrase row includes the hyperparameter values used.\n",
    "    All results are concatenated into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: Original DataFrame (must contain a 'text' column).\n",
    "      - system_prompt: The system prompt for the LLM.\n",
    "      - client: The LLM client instance.\n",
    "      - param_grid: A dictionary where keys are hyperparameter names and values are lists of choices.\n",
    "      - sample_size: Number of random rows to sample.\n",
    "      - m: Number of repeats per row.\n",
    "      - n_completions: Number of completions per LLM call.\n",
    "      \n",
    "    Returns:\n",
    "      - A DataFrame that includes original text, paraphrased_text, and columns for each hyperparameter.\n",
    "    \"\"\"\n",
    "    # Sample a subset of rows for quick evaluation.\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    results = []\n",
    "    # Generate all combinations of hyperparameter values.\n",
    "    keys = list(param_grid.keys())\n",
    "    for values in itertools.product(*param_grid.values()):\n",
    "        param_dict = dict(zip(keys, values))\n",
    "        # Generate paraphrases with the current hyperparameter combination.\n",
    "        result_df = paraphrase_dataframe(sample_df, system_prompt, client, n=n_completions, m=m, **param_dict)\n",
    "        results.append(result_df)\n",
    "    # Concatenate all results; every row will include hyperparameter columns.\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    # Optional: ensure all hyperparameter keys appear as columns (fill missing ones with None).\n",
    "    for key in keys:\n",
    "        if key not in final_df.columns:\n",
    "            final_df[key] = None\n",
    "    return final_df\n",
    "\n",
    "def compute_parascore(row):\n",
    "    \"\"\"\n",
    "    Compute the parascore for a given row.\n",
    "    The function calls the scorer.score method with:\n",
    "      - cands: a list containing the paraphrased_text from the row,\n",
    "      - refs: a list containing the original text from the row,\n",
    "    and returns the third element (index 2) of the resulting score.\n",
    "    \"\"\"\n",
    "    # Create lists as required: first value is paraphrased_text and second is text.\n",
    "    score = scorer.score(\n",
    "        cands=[row[\"paraphrased_text\"]],\n",
    "        refs=[row[\"text\"]],\n",
    "        batch_size=16\n",
    "    )\n",
    "    return score[2].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72deda70-2b25-421f-9d4a-ab18c677cb77",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8f76b9-3528-4f9c-81d7-aab9048ba341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(raw_data_loc)\n",
    "\n",
    "# Rename doc_id to orig_doc_id first\n",
    "df.rename(columns={'doc_id': 'orig_doc_id'}, inplace=True)\n",
    "\n",
    "# Create the new doc_id column directly\n",
    "df['doc_id'] = df['orig_doc_id'].apply(create_temp_doc_id)\n",
    "# Move the new doc_id column to the front\n",
    "cols = ['doc_id'] + [col for col in df.columns if col not in ['doc_id', 'text']] + ['text']\n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a3a6c2-e1c7-4002-949f-2ec5f8ba31f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>orig_doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>known [142.196.88.228 - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article that is being referred to via the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142_196_88_228_text_3</td>\n",
       "      <td>known [142.196.88.228 - Text-3].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>However, it is worth noting that year after ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142_196_88_228_text_4</td>\n",
       "      <td>known [142.196.88.228 - Text-4].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>Specially in post 9/11.No jobs, no housing and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_man_in_black_text_1</td>\n",
       "      <td>known [A_Man_In_Black - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>A_Man_In_Black</td>\n",
       "      <td>known</td>\n",
       "      <td>Nobody's seen fit to comment on these organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_man_in_black_text_2</td>\n",
       "      <td>known [A_Man_In_Black - Text-2].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>A_Man_In_Black</td>\n",
       "      <td>known</td>\n",
       "      <td>Meaning not DMM/Arzon/whatever, or Amazon.co.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>headleydown_text_3</td>\n",
       "      <td>known [HeadleyDown - Text-3].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>known</td>\n",
       "      <td>You have just made undiscussed and unagreed ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>headleydown_text_5</td>\n",
       "      <td>known [HeadleyDown - Text-5].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>known</td>\n",
       "      <td>It is fairly easy to spot problems and there a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>hipocrite_text_2</td>\n",
       "      <td>known [Hipocrite - Text-2].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>known</td>\n",
       "      <td>It appears that reliable sources are using the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>hipocrite_text_3</td>\n",
       "      <td>known [Hipocrite - Text-3].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>known</td>\n",
       "      <td>I would look to other political movements with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>hipocrite_text_5</td>\n",
       "      <td>known [Hipocrite - Text-5].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>known</td>\n",
       "      <td>It is not accurate, and as such disrupts the e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    doc_id                          orig_doc_id corpus  \\\n",
       "0    142_196_88_228_text_1  known [142.196.88.228 - Text-1].txt   Wiki   \n",
       "1    142_196_88_228_text_3  known [142.196.88.228 - Text-3].txt   Wiki   \n",
       "2    142_196_88_228_text_4  known [142.196.88.228 - Text-4].txt   Wiki   \n",
       "3    a_man_in_black_text_1  known [A_Man_In_Black - Text-1].txt   Wiki   \n",
       "4    a_man_in_black_text_2  known [A_Man_In_Black - Text-2].txt   Wiki   \n",
       "..                     ...                                  ...    ...   \n",
       "220     headleydown_text_3     known [HeadleyDown - Text-3].txt   Wiki   \n",
       "221     headleydown_text_5     known [HeadleyDown - Text-5].txt   Wiki   \n",
       "222       hipocrite_text_2       known [Hipocrite - Text-2].txt   Wiki   \n",
       "223       hipocrite_text_3       known [Hipocrite - Text-3].txt   Wiki   \n",
       "224       hipocrite_text_5       known [Hipocrite - Text-5].txt   Wiki   \n",
       "\n",
       "             author texttype  \\\n",
       "0    142.196.88.228    known   \n",
       "1    142.196.88.228    known   \n",
       "2    142.196.88.228    known   \n",
       "3    A_Man_In_Black    known   \n",
       "4    A_Man_In_Black    known   \n",
       "..              ...      ...   \n",
       "220     HeadleyDown    known   \n",
       "221     HeadleyDown    known   \n",
       "222       Hipocrite    known   \n",
       "223       Hipocrite    known   \n",
       "224       Hipocrite    known   \n",
       "\n",
       "                                                  text  \n",
       "0    The article that is being referred to via the ...  \n",
       "1    However, it is worth noting that year after ye...  \n",
       "2    Specially in post 9/11.No jobs, no housing and...  \n",
       "3    Nobody's seen fit to comment on these organiza...  \n",
       "4    Meaning not DMM/Arzon/whatever, or Amazon.co.j...  \n",
       "..                                                 ...  \n",
       "220  You have just made undiscussed and unagreed ch...  \n",
       "221  It is fairly easy to spot problems and there a...  \n",
       "222  It appears that reliable sources are using the...  \n",
       "223  I would look to other political movements with...  \n",
       "224  It is not accurate, and as such disrupts the e...  \n",
       "\n",
       "[225 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b880908-2353-449d-a85e-4bd9ea4bbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Loop through unique doc_id values in the dataframe\n",
    "for doc_id in df['doc_id'].unique():\n",
    "    # Optionally, filter the dataframe rows for this doc_id\n",
    "    doc_df = df[df['doc_id'] == doc_id]\n",
    "    \n",
    "    # Build the file path using the doc_id\n",
    "    file_path = os.path.join(output_directory, f\"{doc_id}.jsonl\")\n",
    "    \n",
    "    # Write the data to a JSONL file using your function\n",
    "    write_jsonl(file_path, doc_df.to_dict(orient='records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd29bd-c3d8-4953-97c4-7bdd428ead2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(raw_data_loc)\n",
    "\n",
    "# Rename doc_id to orig_doc_id first\n",
    "df.rename(columns={'doc_id': 'orig_doc_id'}, inplace=True)\n",
    "\n",
    "# Create the new doc_id column directly\n",
    "df['doc_id'] = df['orig_doc_id'].apply(create_temp_doc_id)\n",
    "df['tokens'] = df['text'].apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# Move the new doc_id column to the front\n",
    "cols = ['doc_id'] + [col for col in df.columns if col not in ['doc_id', 'text']] + ['text']\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "df = df.sort_values(by='tokens', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ea300-c90e-4cc8-9d5d-5ce7c8c1869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['doc_id']\n",
    "\n",
    "completed_files = [\n",
    "    os.path.splitext(f)[0]  # Removes the file extension\n",
    "    for f in os.listdir(save_loc)\n",
    "    if os.path.isfile(os.path.join(save_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "files_to_be_processed = list(set(docs) - set(completed_files))\n",
    "files_to_be_processed = sorted(files_to_be_processed)\n",
    "\n",
    "print(f\"Number of documents to process in raw data: {len(docs)}\")\n",
    "print(f\"Files Complete: {len(completed_files)}\")\n",
    "print(f\"Files to be Processed: {len(files_to_be_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbee5b-7a28-4fd2-9e0e-7f513a8e7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['doc_id'].isin(completed_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0eaa8-dc5e-4079-b2ba-81f7e6d40df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd99f8-fa81-44ad-b7db-591b9c8fbf35",
   "metadata": {},
   "source": [
    "### Gridsearch to find top parameters\n",
    "\n",
    "This was used originally to generate optimal parameters across the Enron dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd494cd3-7c6d-4270-b599-43d9e7e4278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"temperature\": [0.7, 0.8, 0.9],\n",
    "#     'top_p': [0.8, 0.9, 1.0],\n",
    "#     # \"frequency_penalty\": [0.0, 0.5, 1.0],\n",
    "# }\n",
    "\n",
    "# gridsearch_results = grid_search_paraphrases(df, system_prompt, client, param_grid, sample_size=10, m=1, n_completions=10)\n",
    "# gridsearch_results[\"parascore\"] = gridsearch_results.apply(compute_parascore, axis=1)\n",
    "# gridsearch_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145911fb-5552-4146-b9d7-1c24a6401702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Group by the hyperparameter columns and compute the mean parascore\n",
    "# grouped = gridsearch_results.groupby([\"temperature\", \"top_p\"])[\"parascore\"].mean()\n",
    "# print(\"Grouped Mean Parascores:\")\n",
    "# print(grouped)\n",
    "\n",
    "# # Determine the combination with the highest mean parascore\n",
    "# best_params = grouped.idxmax()  # This returns a tuple, e.g., (0.8, 0.9)\n",
    "# best_score = grouped.loc[best_params]\n",
    "\n",
    "# print(\"\\nBest hyperparameter combination (by average score):\")\n",
    "# print(f\"Temperature = {best_params[0]}\")\n",
    "# print(f\"Top p = {best_params[1]}\")\n",
    "# print(f\"Average Parascore = {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7326c98c-7675-46ca-bb41-0ea27a20791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected values are temperature = 0.7, top_p = 0.8 with average ParaScore = 0.7910261923074722"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510f83f-78ae-4647-8493-86f24b2dffdf",
   "metadata": {},
   "source": [
    "### Function to process dataframe by document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f068bb-48be-467c-a119-3956d29c11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_doc_id(df, system_prompt, client, save_loc, n=10, m=1, max_fails=3, **llm_params):\n",
    "    \"\"\"\n",
    "    For each unique doc_id in the DataFrame:\n",
    "      1. Print a header: \"Document: <doc_id>, Doc <i> out of <total_docs>\"\n",
    "      2. Filter the DataFrame to include only rows with that doc_id.\n",
    "      3. Loop m times, printing for each iteration: \"Iteration: <current iteration> out of <m>\".\n",
    "      4. Call paraphrase_dataframe (with m=1 per iteration), handling failures gracefully.\n",
    "      5. If an iteration fails, move to the next. If max_fails in a row occur, move to the next document.\n",
    "      6. Concatenate the successful results and save them to a JSONL file with the doc_id appended to save_loc.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: Input DataFrame containing at least 'doc_id' and 'text' columns.\n",
    "      - system_prompt: The system prompt for the LLM.\n",
    "      - client: The LLM client instance.\n",
    "      - save_loc: Base save location (filename prefix); the doc_id is appended.\n",
    "      - n: Number of completions per LLM call.\n",
    "      - m: Number of iterations per doc_id.\n",
    "      - max_fails: Maximum allowed consecutive failures before skipping the document.\n",
    "      - **llm_params: Additional hyperparameters for the LLM call.\n",
    "    \"\"\"\n",
    "    unique_doc_ids = df['doc_id'].unique()\n",
    "    num_docs = len(unique_doc_ids)\n",
    "    \n",
    "    for idx, doc_id in enumerate(unique_doc_ids, start=1):\n",
    "        print(f\"Document: {doc_id} - Doc {idx} out of {num_docs}\")\n",
    "        \n",
    "        filtered_df = df[df['doc_id'] == doc_id]\n",
    "        iter_dfs = []\n",
    "        fail_count = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            if fail_count >= max_fails:\n",
    "                print(f\"Skipping document {doc_id} due to {fail_count} consecutive failures.\")\n",
    "                break\n",
    "            \n",
    "            print(f\"    Iteration: {i+1} out of {m}\")\n",
    "            try:\n",
    "                iter_df = paraphrase_dataframe(filtered_df, system_prompt, client, n=n, m=1, **llm_params)\n",
    "                iter_dfs.append(iter_df)\n",
    "                fail_count = 0  # Reset failure count on success\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in iteration {i+1} for document {doc_id}: {e}\")\n",
    "                fail_count += 1\n",
    "                continue  # Skip to the next iteration\n",
    "        \n",
    "        if iter_dfs:\n",
    "            result_df = pd.concat(iter_dfs, ignore_index=True)\n",
    "            file_path = f\"{save_loc}{doc_id}.jsonl\"\n",
    "            write_jsonl(result_df, file_path)\n",
    "            print(f\"Saved results for document {doc_id} to {file_path}\")\n",
    "        else:\n",
    "            print(f\"No successful iterations for document {doc_id}, skipping save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98848b-f926-4780-b6bd-51ddf5aaf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_by_doc_id(df, system_prompt, client, save_loc, m=100, n=10, max_fails=5, temperature=0.7, top_p=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
