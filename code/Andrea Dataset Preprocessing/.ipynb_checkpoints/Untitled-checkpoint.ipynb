{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ce36-d18f-4d77-8835-71a90e0202c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3131a6c0-2818-4dcf-88e6-1f8de4bc661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cce6f74-77c4-4aad-8315-f1721c3dbda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdb5cb1-4a63-4a22-bd8d-7cf837162cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_probabilities(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Given a text, model, and tokenizer, return a dictionary with:\n",
    "    - List of all conditional probabilities.\n",
    "    - A dictionary mapping each token to its conditional probability.\n",
    "    - A list of full probability distributions for each token.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "        model (PreTrainedModel): The language model to use for predictions.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the conditional probabilities as a list,\n",
    "              a dictionary of token probabilities, and a list of full distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Initialize the dictionary to store the results\n",
    "    conditional_probs = {\n",
    "        \"conditional_probabilities\": [],  # List of conditional probabilities\n",
    "        \"token_probabilities\": {},       # Dictionary of token-to-probability mappings\n",
    "        \"full_distributions\": []         # List of full probability distributions for each token\n",
    "    }\n",
    "\n",
    "    # Compute the probability for the first token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Compute the probability of the first token given no prior context\n",
    "    first_token_prob = F.softmax(logits[:, 0, :], dim=-1).max().item()\n",
    "    \n",
    "    # Store the first token's probability (usually context-independent)\n",
    "    first_token = tokenizer.decode([input_ids[0, 0]])\n",
    "    conditional_probs[\"conditional_probabilities\"].append(first_token_prob)\n",
    "    conditional_probs[\"token_probabilities\"][first_token] = first_token_prob\n",
    "\n",
    "    # Store the full probability distribution for the first token\n",
    "    full_distribution_first_token = F.softmax(logits[:, 0, :], dim=-1).squeeze().cpu().numpy()\n",
    "    conditional_probs[\"full_distributions\"].append(full_distribution_first_token)\n",
    "\n",
    "    # Iterate through each token in the sequence starting from the second token\n",
    "    for i in range(1, input_ids.size(1)):\n",
    "        prefix = input_ids[:, :i]  # Context: all tokens before the current one\n",
    "        next_token_id = input_ids[0, i]  # Current token to predict\n",
    "\n",
    "        # Get logits for the context\n",
    "        with torch.no_grad():\n",
    "            outputs = model(prefix)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Compute probabilities for the next token\n",
    "        log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # Log probs for last step\n",
    "        prob_distribution = log_probs.exp()  # Convert log probs to probabilities\n",
    "\n",
    "        # Extract the current token's probability\n",
    "        next_token_prob = prob_distribution[0, next_token_id].item()\n",
    "\n",
    "        # Decode the full distribution for readability\n",
    "        full_distribution = prob_distribution.squeeze().cpu().numpy()\n",
    "\n",
    "        # Store the conditional probability and the full distribution\n",
    "        token = tokenizer.decode([next_token_id])\n",
    "        conditional_probs[\"conditional_probabilities\"].append(next_token_prob)\n",
    "        conditional_probs[\"token_probabilities\"][token] = next_token_prob\n",
    "        conditional_probs[\"full_distributions\"].append(full_distribution)\n",
    "\n",
    "    # Return the dictionary with both the list and the dictionary of probabilities\n",
    "    return conditional_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e747a615-6121-4eae-82a3-eaba8a3b1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_probabilities(text, model, tokenizer, \n",
    "                                   include_conditional_probabilities=True,\n",
    "                                   include_token_probabilities=True,\n",
    "                                   include_full_distributions=True):\n",
    "    \"\"\"\n",
    "    Given a text, model, and tokenizer, return a dictionary with:\n",
    "    - List of all conditional probabilities (optional).\n",
    "    - A dictionary mapping each token to its conditional probability (optional).\n",
    "    - A list of full probability distributions for each token (optional).\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to analyze.\n",
    "        model (PreTrainedModel): The language model to use for predictions.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
    "        include_conditional_probabilities (bool): Whether to include the list of conditional probabilities.\n",
    "        include_token_probabilities (bool): Whether to include the dictionary of token probabilities.\n",
    "        include_full_distributions (bool): Whether to include the list of full probability distributions.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the conditional probabilities, token-to-probability mappings,\n",
    "              and full distributions based on the flags provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Initialize the dictionary to store the results\n",
    "    conditional_probs = {}\n",
    "\n",
    "    # Compute the probability for the first token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Compute the probability of the first token given no prior context\n",
    "    first_token_prob = F.softmax(logits[:, 0, :], dim=-1).max().item()\n",
    "    \n",
    "    # Store the first token's probability (usually context-independent)\n",
    "    first_token = tokenizer.decode([input_ids[0, 0]])\n",
    "\n",
    "    # Add components to the dictionary if requested\n",
    "    if include_conditional_probabilities:\n",
    "        conditional_probs[\"conditional_probabilities\"] = [first_token_prob]\n",
    "    \n",
    "    if include_token_probabilities:\n",
    "        conditional_probs[\"token_probabilities\"] = {first_token: first_token_prob}\n",
    "\n",
    "    if include_full_distributions:\n",
    "        full_distribution_first_token = F.softmax(logits[:, 0, :], dim=-1).squeeze().cpu().numpy()\n",
    "        conditional_probs[\"full_distributions\"] = [full_distribution_first_token]\n",
    "\n",
    "    # Iterate through each token in the sequence starting from the second token\n",
    "    for i in range(1, input_ids.size(1)):\n",
    "        prefix = input_ids[:, :i]  # Context: all tokens before the current one\n",
    "        next_token_id = input_ids[0, i]  # Current token to predict\n",
    "\n",
    "        # Get logits for the context\n",
    "        with torch.no_grad():\n",
    "            outputs = model(prefix)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Compute probabilities for the next token\n",
    "        log_probs = F.log_softmax(logits[:, -1, :], dim=-1)  # Log probs for last step\n",
    "        prob_distribution = log_probs.exp()  # Convert log probs to probabilities\n",
    "\n",
    "        # Extract the current token's probability\n",
    "        next_token_prob = prob_distribution[0, next_token_id].item()\n",
    "\n",
    "        # Decode the full distribution for readability\n",
    "        full_distribution = prob_distribution.squeeze().cpu().numpy()\n",
    "\n",
    "        # Store the conditional probability and the full distribution if requested\n",
    "        token = tokenizer.decode([next_token_id])\n",
    "\n",
    "        if include_conditional_probabilities:\n",
    "            conditional_probs[\"conditional_probabilities\"].append(next_token_prob)\n",
    "        \n",
    "        if include_token_probabilities:\n",
    "            conditional_probs[\"token_probabilities\"][token] = next_token_prob\n",
    "        \n",
    "        if include_full_distributions:\n",
    "            conditional_probs[\"full_distributions\"].append(full_distribution)\n",
    "\n",
    "    # Return the dictionary with the requested components\n",
    "    return conditional_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccf3f6a-2f03-4db1-8508-24a7205d4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Call the function to get the conditional probabilities\n",
    "conditional_probs = get_conditional_probabilities(text, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81ea524-290f-43c7-b46d-34114314046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(location, exact_name=None):\n",
    "    \"\"\"\n",
    "    Lists all files in the specified location, optionally filtering by file type.\n",
    "\n",
    "    Parameters:\n",
    "    - location (str): The directory to search in.\n",
    "    - file_type (str, optional): The file extension to filter by (e.g., \".jsonl\").\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of full file paths that match the file type.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store file paths\n",
    "    file_list = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file_name in files:\n",
    "            # Match exact file name if specified\n",
    "            if exact_name and file_name == exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "            # If no exact_name is provided, include all files\n",
    "            elif not exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f6721e-9817-40fb-83f7-8711eccf8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and converts it into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the JSONL file to read.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing the data from the JSONL file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d613170-7bdd-4a6f-8f66-ce3ad02d0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "test_or_training = \"test\"\n",
    "\n",
    "base_file_type_loc = f\"{base_loc}/{test_or_training}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3ab3e1-6cd1-46b2-9de0-907636ad8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list_files(base_file_type_loc, \"known_raw.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66fbd3ec-3ce7-4114-83ee-e6e8eea4d316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/BCross/datasets/author_verification/test/StackExchange/known_raw.jsonl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32e8fab-605d-4a75-81c1-d3cdb78e993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc0c0b1-e226-48ae-86d7-b590b1f2e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>corpus</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>known [271958 stats] [ 7.39 kb].txt</td>\n",
       "      <td>Your classifier gives you a probability for ea...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>271958</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>known [2736153 stats] [ 17.67 kb].txt</td>\n",
       "      <td>Moving average is what you get when you are UR...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>2736153</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>known [2852150 stats] [ 12.08 kb].txt</td>\n",
       "      <td>Both logit and probit models provide statistic...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>2852150</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>known [2875509 stats] [ 10.69 kb].txt</td>\n",
       "      <td>This is the extent of the knowledge I am famil...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>2875509</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>known [298433 stats] [ 19.97 kb].txt</td>\n",
       "      <td>URL has some good free online tutorials for mu...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>298433</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>known [9437855 stats] [ 19.9 kb].txt</td>\n",
       "      <td>The problem would be simple when the dataset d...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>9437855</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>known [9466756 stats] [ 11.95 kb].txt</td>\n",
       "      <td>1) If we want to make an assumption about the ...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>9466756</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>known [947356 stats] [ 3.05 kb].txt</td>\n",
       "      <td>As already pointed out by HANDLE at the end of...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>947356</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>known [9722956 stats] [ 19.93 kb].txt</td>\n",
       "      <td>You are talking about Bayesian analysis, not B...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>9722956</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>known [980225 stats] [ 19.76 kb].txt</td>\n",
       "      <td>We take the log-likelihood because each case i...</td>\n",
       "      <td>StackExchange</td>\n",
       "      <td>980225</td>\n",
       "      <td>known</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    doc_id  \\\n",
       "0      known [271958 stats] [ 7.39 kb].txt   \n",
       "1    known [2736153 stats] [ 17.67 kb].txt   \n",
       "2    known [2852150 stats] [ 12.08 kb].txt   \n",
       "3    known [2875509 stats] [ 10.69 kb].txt   \n",
       "4     known [298433 stats] [ 19.97 kb].txt   \n",
       "..                                     ...   \n",
       "109   known [9437855 stats] [ 19.9 kb].txt   \n",
       "110  known [9466756 stats] [ 11.95 kb].txt   \n",
       "111    known [947356 stats] [ 3.05 kb].txt   \n",
       "112  known [9722956 stats] [ 19.93 kb].txt   \n",
       "113   known [980225 stats] [ 19.76 kb].txt   \n",
       "\n",
       "                                                  text         corpus  \\\n",
       "0    Your classifier gives you a probability for ea...  StackExchange   \n",
       "1    Moving average is what you get when you are UR...  StackExchange   \n",
       "2    Both logit and probit models provide statistic...  StackExchange   \n",
       "3    This is the extent of the knowledge I am famil...  StackExchange   \n",
       "4    URL has some good free online tutorials for mu...  StackExchange   \n",
       "..                                                 ...            ...   \n",
       "109  The problem would be simple when the dataset d...  StackExchange   \n",
       "110  1) If we want to make an assumption about the ...  StackExchange   \n",
       "111  As already pointed out by HANDLE at the end of...  StackExchange   \n",
       "112  You are talking about Bayesian analysis, not B...  StackExchange   \n",
       "113  We take the log-likelihood because each case i...  StackExchange   \n",
       "\n",
       "      author texttype  \n",
       "0     271958    known  \n",
       "1    2736153    known  \n",
       "2    2852150    known  \n",
       "3    2875509    known  \n",
       "4     298433    known  \n",
       "..       ...      ...  \n",
       "109  9437855    known  \n",
       "110  9466756    known  \n",
       "111   947356    known  \n",
       "112  9722956    known  \n",
       "113   980225    known  \n",
       "\n",
       "[114 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf8a1c0-d8ac-4b6a-a863-76ab6b011379",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84a9118a-23d1-4ecc-b28e-6c222cc256f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your classifier gives you a probability for each class. as a result. Hence, in a classification problem with MATH accuracy. I am currently developing a mathematical symbol classifier (see URL for my bachelors thesis in computer science. I do this by VALUE -fold cross-validation: I am now wondering to how many digits I should publish the accuracy. This is one form of transfer learning. So you can transfer some of the knowledge obtained from dataset MATH. See my URL for this and more terms explained in very few words. LDA is a dimensionality reduction method, not a classifier. In SKlearn, CODE seems to be a naive bayes classifier after LDA, see docs. One quality indicator for a clustering is the silhouette coefficient: Get a distance metric MATH for two objects in your space. For example, the euclidean distance. Let MATH be the average distance of o to the second-closest cluster: MATH s You want this value to be as big as possible. Everything below 0 is bad. To answer your questions, I have to give a little bit of background information. LeNet was described in URL . Please note that this is before GPU programming (CUDA was released in DATE). This means it was practically impossible to train large networks. LeNet-5 has 7 layers. LeNet-5 was designed to work on the URL. Alexnet was described in URL . They use a CNN with ReLU activation functions and 8 layers. One interesting non-standard thing is overlapping pooling areas. The last two layer are fully connected with VALUE neurons each. They apply dropout regularization. Alexnet was designed to work on the Imagenet dataset. LeNet was not designed to work on large images. Just like Alexnet, LeNet is designed to work on a fixed-size input. There are ways to design networks for arbitrary sizes, though . However, often it is easy to adjust the first layer to make the network (in principle) work with different sized input. Having said that, Alexnet would still perform better than LeNet on big images.  regularization is VERY important when it comes to large networks. The training time is faster with ReLU units (I'm not too sure what LeNet uses). Those techniques help to prevent overfitting. Parameter MATH in SVMs. I have a project (URL where I want to classify handwritten recordings into symbols. I get my data from a crowd-sourcing approach (with lots of filtering by hand, because people give obviously wrong labels). For some of my data, I cannot say what the correct label would be. For example, a round shape could be CODE, CODE, CODE, CODE,. But I can say that a round shape is NOT CODE, CODE, CODE,. I guess I would have to implement a custom error function which gives no error for any output of the net of the possible values, but an error for anything different from 0 in the negative classes. Supervised learning needs labeled data. Unsupervised learning finds useful representations of the data from unlabeled data. Semi-supervised learning uses both, labeled and unlabeled data. I can't explain those formulas without more details about the notation, but plurality voting is a method where the class with the most votes wins. It is one form of majority voting. So every classifier gives exactly one vote. The class with most votes is the prediction of the ensemble. In many cases where you apply ranking algorithms (e.g. Google search, Amazon product recommendation) you have hundreds and thousands of results. The user only wants to watch at the top VALUE or so. So the rest is completely irrelevant. If this is true for your application, then this has direct implications on the metric: You only need to look at the top MATH ranked items and the top MATH items of the ground truth ranking. The order of those potentially MATH items might be relevant or not - but for sure the order of all other items is irrelevant. Three relevant metrics are top-k accuracy, and . The MATH depends on your application. For all of them, for the ranking-queries you evaluate, the total number of relevant items should be above MATH. For the ground truth, it might be hard to define an order. Top-n accuracy is a metric for classification. See URL. So you let the ranking algorithm predict MATH elements and see if it contains at least one relevant item. I like this very much because it is so easy to interpret. MATH comes from a business requirement , then you can say how often the users will be happy. Downside of this: If you still care about the order within the MATH items, you have to find another metric. You give them all the relevant items. Due to this, alone might be not so meaningful. If it is combined with a high , then increasing k might make sense. Local response normalization  is done pixel-wise for each channel MATH: Batch normalization also works pixel-wise: where MATH are learnable parameters which allow the net to remove the normalization. So the answer is: Local Response Normalization is not important any more, because we have something which works better and replaced LRN: Batch Normalization. A single bias node per layer. In a couple of experiments in my masters thesis , I found that the bias might be important for the first layer, but especially at the fully connected layers at the end it seems not to play a big role. Hence one can have them at the first few layers and not at the last ones. Simply train a network, plot the distribution of weights of the bias nodes and prune them if the weights seem to be too close to zero. Ok, I found the answer myself: Conditinal Random Fields  are a special case of Markov Random Fields . Source: Blake, Kohli and Rother: Markov random fields for vision and image processing. DATE. Yes. Fixing the values is the same as conditioning on them. However, you should note that there are differences in training, too. Watching many of the lectures about PGM (probabilistic graphical models) on coursera helped me a lot. The lower boundary is the VALUE -percentile, the upper boundary is the VALUE -percentile. The line in the middle is the mean. VALUE times the IQR from the VALUE -percentile border is the lower whisker (the vertical line at VALUE for human and VALUE for ape). Everything lower than that is an outlier and only represented by a dot. The same for the upper whisker. You can make such plots like this: When you want to make a recommendation for user MATH, you calculate MATH and get the scores for all of the MATH items. Feel free to answer this question for other matrix factorization  optimization problem formulations if this makes things clearer. In a very simple case, you have one training set and one test set. You train on the training set and then you test on the test set. As you have trained on the training set, the network has already seen the data and the optimization method was optimized for this data. Hence the error on the test set should be higher. If you have hyper parameters which you want to optimize, you will split your training set into a training and a development set. You train for different choices on the training set, see the error on the development set and at the end, when you think everything is fine, you test on the test set. The test set should NEVER be used for optimization. n-fold cross-validation makes better use of your data. You make MATH. Then you average the error. You should not use this for hyper parameter optimization as this will add knowledge of the test set to your system. Hyper parameter optimization should only be done on the training set. I've listed many ways of topology learning in my masters thesis, chapter 3. The big categories are:\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2792ca5-ab41-4bac-b657-822e67982451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f82f9ac3fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user/my_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_conditional_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43minclude_conditional_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43minclude_token_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43minclude_full_distributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36mget_conditional_probabilities\u001b[0;34m(text, model, tokenizer, include_conditional_probabilities, include_token_probabilities, include_full_distributions)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Get logits for the context\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Compute probabilities for the next token\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:1186\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1174\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1175\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1183\u001b[0m )\n\u001b[1;32m   1185\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1186\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1189\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = get_conditional_probabilities(text, model, tokenizer,\n",
    "                                        include_conditional_probabilities=True,\n",
    "                                        include_token_probabilities=False,\n",
    "                                        include_full_distributions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048936e-4619-4349-a718-74b6db01a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
