{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415c6827-db62-4ba2-a04f-8993399cfca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/my_env/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9d767d-c9f8-4ef5-bbc8-e4331af03d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67607fa5-c524-492c-b3bb-bb77ba3184dd",
   "metadata": {},
   "source": [
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7252422-5d96-4347-bddc-9dd3cdddda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "test_or_training = \"test\"\n",
    "\n",
    "base_file_type_loc = f\"{base_loc}/{test_or_training}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e03335-b832-4263-8be8-dfdef53f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(location, exact_name=None):\n",
    "    \"\"\"\n",
    "    Lists all files in the specified location, optionally filtering by file type.\n",
    "\n",
    "    Parameters:\n",
    "    - location (str): The directory to search in.\n",
    "    - file_type (str, optional): The file extension to filter by (e.g., \".jsonl\").\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of full file paths that match the file type.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store file paths\n",
    "    file_list = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file_name in files:\n",
    "            # Match exact file name if specified\n",
    "            if exact_name and file_name == exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "            # If no exact_name is provided, include all files\n",
    "            elif not exact_name:\n",
    "                file_list.append(os.path.join(root, file_name))\n",
    "    \n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf219e7-65c4-4477-8d7a-fb4089550626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and converts it into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the JSONL file to read.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing the data from the JSONL file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93bc9b44-5f91-415e-b496-f61cb0990d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data, output_file_path):\n",
    "    \"\"\"\n",
    "    Writes a pandas DataFrame to a JSONL file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A pandas DataFrame to save.\n",
    "    - output_file_path: Path to the output JSONL file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for _, row in data.iterrows():\n",
    "            json.dump(row.to_dict(), file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1acd81-7517-4c82-b27e-3990fc21762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add custom rule for abbreviations like Capt. and others\n",
    "# Add custom rules for abbreviations\n",
    "abbreviation_patterns = [\n",
    "    # Personal titles\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"capt.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"dr.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"mr.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"mrs.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"prof.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"rev.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"sr.\"}]},\n",
    "    {\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"jr.\"}]},\n",
    "    \n",
    "    # Common Latin abbreviations\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"e.g.\"}]},  # For example\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"i.e.\"}]},  # That is\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"etc.\"}]},  # Et cetera\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"et al.\"}]},  # And others\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"a.m.\"}]},  # Ante meridiem\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"p.m.\"}]},  # Post meridiem\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"vs.\"}]},   # Versus\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"cf.\"}]},   # Compare\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"viz.\"}]},  # Namely\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"ca.\"}]},   # Circa\n",
    "\n",
    "    # Additional abbreviations\n",
    "    # {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"no.\"}]},  # Number - Ignore \n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"vol.\"}]},  # Volume\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"pp.\"}]},   # Pages\n",
    "    {\"label\": \"ABBREVIATION\", \"pattern\": [{\"LOWER\": \"fig.\"}]},  # Figure\n",
    "]\n",
    "\n",
    "# Adding the patterns to spaCy's pipeline\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", name=\"abbreviation_ruler\")\n",
    "ruler.add_patterns(abbreviation_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5539c4-6bf3-4681-9d30-5ae3165ca6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Step 1: Perform Named Entity Recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text: f\"__ENTITY{idx}__\" for idx, ent in enumerate(doc.ents)}\n",
    "\n",
    "    # Step 2: Mask named entities\n",
    "    masked_text = text\n",
    "    for entity, placeholder in entities.items():\n",
    "        masked_text = masked_text.replace(entity, placeholder)\n",
    "\n",
    "    # Step 3: Perform Sentence Splitting using NLTK\n",
    "    sentences = sent_tokenize(masked_text)\n",
    "\n",
    "    # Step 4: Restore Named Entities\n",
    "    restored_sentences = []\n",
    "    for sentence in sentences:\n",
    "        for placeholder, entity in entities.items():\n",
    "            sentence = sentence.replace(entity, placeholder)\n",
    "        restored_sentences.append(sentence)\n",
    "\n",
    "    return restored_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52123259-1c5b-4083-aa4c-71dd1f31290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process the input text by performing Named Entity Recognition (NER), masking the entities,\n",
    "    splitting the text into sentences, and then restoring the entities with their original text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to process, including named entities.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of sentences with named entities restored to their original form.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Perform Named Entity Recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create a dictionary where the keys are the original entities and the values are placeholders\n",
    "    entities = {ent.text: f\"__ENTITY{idx}__\" for idx, ent in enumerate(doc.ents)}\n",
    "\n",
    "    # Step 2: Mask named entities in the text\n",
    "    masked_text = text\n",
    "    \n",
    "    # Sort entities by length to prevent conflicts (longer entities are replaced first)\n",
    "    sorted_entities = sorted(entities.items(), key=lambda x: -len(x[0]))\n",
    "    \n",
    "    for entity, placeholder in sorted_entities:\n",
    "        # Use regex to ensure we match whole words only (avoiding partial replacements)\n",
    "        masked_text = re.sub(r'\\b' + re.escape(entity) + r'\\b', placeholder, masked_text)\n",
    "\n",
    "    # Step 3: Perform Sentence Splitting using NLTK\n",
    "    sentences = sent_tokenize(masked_text)\n",
    "\n",
    "    # Step 3: Perform Sentence Splitting using Spacy\n",
    "    # doc = nlp(masked_text)\n",
    "    # sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    reversed_entities = {v: k for k, v in entities.items()}\n",
    "    \n",
    "    # Step 4: Restore Named Entities in sentences using regex for exact match\n",
    "    restored_sentences = []\n",
    "    for sentence in sentences:\n",
    "        restored_sentence = sentence\n",
    "        for placeholder, entity in reversed_entities.items():\n",
    "            # Replace placeholders with original entities using \\b for word boundaries\n",
    "            restored_sentence = re.sub(r'\\b' + re.escape(placeholder) + r'\\b', entity, restored_sentence)\n",
    "        restored_sentences.append(restored_sentence)\n",
    "\n",
    "    return restored_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41021786-2235-419e-9396-5bf02364eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_process_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by replacing '\\\\' with a placeholder, processes with NER and sentence splitting,\n",
    "    then restores the '\\\\' placeholders back to '\\\\'.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The original text to process.\n",
    "\n",
    "    Returns:\n",
    "    - list: The list of sentences with entities restored, including correct handling of backslashes.\n",
    "    \"\"\"\n",
    "    # Preprocess step: Replace '\\\\' with a placeholder to avoid issues during NER\n",
    "    text = text.replace(\"\\\\\", \"__BACKSLASH_PLACEHOLDER__\")\n",
    "    text = text.replace(\"(??)\", \"__REF_ERROR__\")\n",
    "\n",
    "    # Now apply the process_text function\n",
    "    processed_sentences = process_text(text)\n",
    "\n",
    "    # Restore the '\\\\' back in the processed sentences\n",
    "    processed_sentences_with_backslashes = [\n",
    "        sentence.replace('__BACKSLASH_PLACEHOLDER__', r'\\\\') for sentence in processed_sentences\n",
    "    ]\n",
    "    processed_sentences_with_backslashes = [\n",
    "        sentence.replace('__REF_ERROR__', \"(??)\") for sentence in processed_sentences\n",
    "    ]\n",
    "    \n",
    "    return processed_sentences_with_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "328b7246-b2d9-4372-b9c9-bf4c96858cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ner(df):\n",
    "    \"\"\"\n",
    "    Apply Named Entity Recognition (NER) to the specified text column of a DataFrame\n",
    "    and return a list of unique named entities.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing the text data.\n",
    "    - text_column (str): The name of the column that contains the text data.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of unique named entities found in the text column.\n",
    "    \"\"\"\n",
    "    # Function to extract entities from a given text\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)  # Perform NER using spaCy\n",
    "        return [ent.text for ent in doc.ents]  # Extract entities as a list\n",
    "\n",
    "    # Apply NER to the text column and create a new column with the list of entities\n",
    "    df['entities'] = df['text'].apply(extract_entities)\n",
    "\n",
    "    # Flatten the list of entities from all rows and get unique entities\n",
    "    all_entities = [entity for sublist in df['entities'] for entity in sublist]\n",
    "    unique_entities = list(set(all_entities))  # Remove duplicates by converting to a set\n",
    "\n",
    "    return unique_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66e1564-9226-493f-a7a2-d6b72b94e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list_files(base_file_type_loc, \"known_raw.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9a5a41-8750-4d03-8a3f-f04cdaf8f690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/BCross/datasets/author_verification/test/StackExchange/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Amazon/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/The Telegraph/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Yelp/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Wiki/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/All-the-news/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/IMDB/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Reddit/known_raw.jsonl',\n",
       " \"/Volumes/BCross/datasets/author_verification/test/Koppel's Blogs/known_raw.jsonl\",\n",
       " '/Volumes/BCross/datasets/author_verification/test/Perverted Justice/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/TripAdvisor/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/ACL/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/The Apricity/known_raw.jsonl',\n",
       " '/Volumes/BCross/datasets/author_verification/test/Enron/known_raw.jsonl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b34e455-3b92-4332-a391-8198e49ef3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File: /Volumes/BCross/datasets/author_verification/training/StackExchange/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Amazon/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/The Telegraph/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Yelp/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Wiki/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/All-the-news/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/IMDB/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Reddit/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Koppel's Blogs/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Perverted Justice/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/TripAdvisor/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/ACL/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/The Apricity/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/training/Enron/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/StackExchange/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Amazon/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/The Telegraph/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Yelp/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Wiki/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/All-the-news/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/IMDB/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Reddit/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Koppel's Blogs/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Perverted Justice/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/TripAdvisor/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/ACL/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/The Apricity/known_raw.jsonl\n",
      "    File Processed.\n",
      "Processing File: /Volumes/BCross/datasets/author_verification/test/Enron/known_raw.jsonl\n",
      "    File Processed.\n"
     ]
    }
   ],
   "source": [
    "data_types = ['training', 'test']\n",
    "base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "for d_type in data_types:\n",
    "\n",
    "    base_file_type_loc = f\"{base_loc}/{d_type}/\"\n",
    "\n",
    "    file_list = list_files(base_file_type_loc, \"known_raw.jsonl\")\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        print(f\"Processing File: {file}\")\n",
    "        \n",
    "        new_file_loc = file.replace('raw.jsonl', 'processed.jsonl')\n",
    "    \n",
    "        df = read_jsonl(file)\n",
    "        df_copy = df.copy()\n",
    "        \n",
    "        # Apply the function to the DataFrame\n",
    "        df_copy[\"sentence\"] = df_copy[\"text\"].apply(preprocess_and_process_text)\n",
    "    \n",
    "        # Expand the processed sentences into separate rows if needed\n",
    "        expanded_df = df_copy.explode(\"sentence\").reset_index(drop=True)\n",
    "    \n",
    "        # Step 3: Add chunk_id column (sequential numbering grouped by doc_id)\n",
    "        expanded_df[\"chunk_id\"] = expanded_df.groupby(\"doc_id\").cumcount() + 1\n",
    "    \n",
    "        # Rename the \"processed_sentences\" column for clarity\n",
    "        expanded_df = expanded_df.rename(columns={\"processed_sentences\": \"sentence\"})\n",
    "        \n",
    "        expanded_df = expanded_df[['corpus', 'doc_id', 'chunk_id', 'author', 'texttype', 'sentence']]\n",
    "    \n",
    "        write_jsonl(expanded_df, new_file_loc)\n",
    "    \n",
    "        print(\"    File Processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8cb6d89-9243-4f88-864e-0e8ce1bcfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = ['/Volumes/BCross/datasets/author_verification/test/IMDB/known_raw.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5755d374-05aa-4c66-88f9-2f9406d5db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through the files\n",
    "# for file in file_list:\n",
    "#     print(f\"Processing file: {file}\")\n",
    "#     try:\n",
    "#         # Load the file into a DataFrame\n",
    "#         df = read_jsonl(file)\n",
    "#         df_copy = df.copy()\n",
    "        \n",
    "#         # Apply the filter\n",
    "#         filtered_df = df_copy[df_copy['text'].str.contains(\"(??)\", na=False, regex=False)]\n",
    "        \n",
    "#         # Check the number of rows after filtering\n",
    "#         num_rows = len(filtered_df)\n",
    "#         print(f\"Number of rows after filtering: {num_rows}\")\n",
    "        \n",
    "#         if num_rows > 0:\n",
    "#             print(f\"{file} has {num_rows} rows after filtering.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e102449-2ab3-4b50-bf78-7540b9c7c745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for file in file_list:\n",
    "\n",
    "#     print(f\"Processing File: {file}\")\n",
    "    \n",
    "#     new_file_loc = file.replace('raw.jsonl', 'processed.jsonl')\n",
    "\n",
    "#     df = read_jsonl(file)\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     # Apply the function to the DataFrame\n",
    "#     df_copy[\"sentence\"] = df_copy[\"text\"].apply(preprocess_and_process_text)\n",
    "\n",
    "#     # Expand the processed sentences into separate rows if needed\n",
    "#     expanded_df = df_copy.explode(\"sentence\").reset_index(drop=True)\n",
    "\n",
    "#     # Step 3: Add chunk_id column (sequential numbering grouped by doc_id)\n",
    "#     expanded_df[\"chunk_id\"] = expanded_df.groupby(\"doc_id\").cumcount() + 1\n",
    "\n",
    "#     # Rename the \"processed_sentences\" column for clarity\n",
    "#     expanded_df = expanded_df.rename(columns={\"processed_sentences\": \"sentence\"})\n",
    "    \n",
    "#     expanded_df = expanded_df[['corpus', 'doc_id', 'chunk_id', 'author', 'texttype', 'sentence']]\n",
    "\n",
    "#     write_jsonl(expanded_df, new_file_loc)\n",
    "\n",
    "#     print(\"    File Processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
