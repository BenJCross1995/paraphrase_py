{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1645378-31c9-4c10-a4f8-d251feaaa941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "# from parascore import ParaScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83405597-8844-4bbe-933e-7442cb631c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_loc = \"../../credentials.json\"\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"Wiki\"\n",
    "\n",
    "data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/\"\n",
    "\n",
    "raw_data_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "\n",
    "save_loc = f\"{data_loc}full_doc_paraphrase/\"\n",
    "os.makedirs(save_loc, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb961d-9a4b-4062-b20e-561e6a01e6a3",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f71179d-4c9f-458c-a6d6-7ce892c91724",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(credential_loc, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = data['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "# scorer = ParaScorer(lang=\"en\", model_type='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf091903-fa1b-4c4b-9ac0-7f97a3d98cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your role is to function as an advanced paraphrasing assistant. Your task is to generate a fully paraphrased version of a given document that preserves its original meaning, tone, genre, and style, while exhibiting significantly heightened lexical diversity and structural transformation. The aim is to produce a document that reflects a broad, globally influenced language profile for authorship verification research.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1. **Preserve Core Meaning & Intent:**  \n",
    "   - Ensure that the paraphrased text maintains the original document’s logical flow, factual accuracy, and overall message.  \n",
    "   - Retain the tone, style, and genre to match the source content precisely.\n",
    "\n",
    "2. **Maximize Lexical Diversity:**  \n",
    "   - Use an extensive range of synonyms, idiomatic expressions, and alternative phrasings to replace common expressions.  \n",
    "   - Avoid repetitive language; introduce varied vocabulary throughout the document to ensure a fresh linguistic perspective.\n",
    "\n",
    "3. **Transform Structural Elements:**  \n",
    "   - Reorganize sentences and paragraphs: invert sentence structures, vary sentence lengths, and use different clause orders.  \n",
    "   - Experiment with alternative grammatical constructions and narrative flows without compromising clarity or meaning.\n",
    "\n",
    "4. **Preserve Critical Terms & Proper Nouns:**  \n",
    "   - Do not alter technical terms, names, or key references unless explicitly instructed.  \n",
    "   - Ensure these elements remain intact to maintain the document's integrity.\n",
    "\n",
    "5. **Ensure Naturalness & Cohesion:**  \n",
    "   - Despite extensive lexical and structural changes, the paraphrased document must remain coherent, natural, and easily understandable.  \n",
    "   - Strive for a balanced output that is both distinct in language and faithful to the original content.\n",
    "\n",
    "6. **Output Format:**  \n",
    "   - Provide only the paraphrased document without any extra commentary or explanations.  \n",
    "   - The output must be structured in JSON format as follows:  \n",
    "\n",
    "     {\"new_document\": <paraphrased_document>}\n",
    "\n",
    "Instructions:\n",
    "- Prioritize high lexical variation and significant syntactic reordering.\n",
    "- Create a paraphrase that is distinct in wording and structure from the source while fully retaining its meaning, tone, and intent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376846e-d24a-4d9f-9070-7184b8b0ceaa",
   "metadata": {},
   "source": [
    "### Data Prep Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458c35c7-f379-415c-af73-ac421526fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "def write_jsonl(data, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for _, row in data.iterrows():\n",
    "            json.dump(row.to_dict(), file)\n",
    "            file.write('\\n')\n",
    "            \n",
    "def create_temp_doc_id(input_text):\n",
    "    # Extract everything between the brackets\n",
    "    match = re.search(r'\\[(.*?)\\]', input_text)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        # Replace all punctuation and spaces with \"_\"\n",
    "        cleaned_text = re.sub(r'[^\\w]', '_', extracted_text)\n",
    "        # Replace multiple underscores with a single \"_\"\n",
    "        final_text = re.sub(r'_{2,}', '_', cleaned_text)\n",
    "        return final_text.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6eda8a-e81f-4772-b985-2e3dabbe5f39",
   "metadata": {},
   "source": [
    "### Paraphrase Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c47a17-36e5-4c94-9f98-08e048435da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_call(text, system_prompt, client, n=10, model=\"gpt-4o-mini\", temperature=0.7, top_p=0.9, **kwargs):\n",
    "    \"\"\"\n",
    "    Calls the LLM with the specified hyperparameters.\n",
    "    Returns the completion response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        n=n,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        **kwargs\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def paraphrase_dataframe(df, system_prompt, client, n=10, m=1, **llm_params):\n",
    "    \"\"\"\n",
    "    Iterates over the DataFrame rows and for each text calls the paraphrase LLM m times,\n",
    "    with n completions each time. For every generated paraphrase, it creates a new row that\n",
    "    contains the original data, the paraphrased text, and all provided hyperparameter settings.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        for _ in range(m):\n",
    "            response = paraphrase_call(text, system_prompt, client, n=n, **llm_params)\n",
    "            # Process each completion (choice) from the LLM response.\n",
    "            for choice in response.choices:\n",
    "                paraphrased_text = json.loads(choice.message.content)['new_document']\n",
    "                new_row = row.copy()\n",
    "                new_row['paraphrased_text'] = paraphrased_text\n",
    "                # Save every hyperparameter in the row.\n",
    "                for param, value in llm_params.items():\n",
    "                    new_row[param] = value\n",
    "                expanded_rows.append(new_row)\n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "def grid_search_paraphrases(df, system_prompt, client, param_grid, sample_size=10, m=1, n_completions=10):\n",
    "    \"\"\"\n",
    "    Selects a random subset of rows from the DataFrame (sample_size) and then iterates over\n",
    "    all hyperparameter combinations from param_grid. For each combination, it calls paraphrase_dataframe,\n",
    "    ensuring that each generated paraphrase row includes the hyperparameter values used.\n",
    "    All results are concatenated into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: Original DataFrame (must contain a 'text' column).\n",
    "      - system_prompt: The system prompt for the LLM.\n",
    "      - client: The LLM client instance.\n",
    "      - param_grid: A dictionary where keys are hyperparameter names and values are lists of choices.\n",
    "      - sample_size: Number of random rows to sample.\n",
    "      - m: Number of repeats per row.\n",
    "      - n_completions: Number of completions per LLM call.\n",
    "      \n",
    "    Returns:\n",
    "      - A DataFrame that includes original text, paraphrased_text, and columns for each hyperparameter.\n",
    "    \"\"\"\n",
    "    # Sample a subset of rows for quick evaluation.\n",
    "    sample_df = df.sample(n=sample_size, random_state=42)\n",
    "    results = []\n",
    "    # Generate all combinations of hyperparameter values.\n",
    "    keys = list(param_grid.keys())\n",
    "    for values in itertools.product(*param_grid.values()):\n",
    "        param_dict = dict(zip(keys, values))\n",
    "        # Generate paraphrases with the current hyperparameter combination.\n",
    "        result_df = paraphrase_dataframe(sample_df, system_prompt, client, n=n_completions, m=m, **param_dict)\n",
    "        results.append(result_df)\n",
    "    # Concatenate all results; every row will include hyperparameter columns.\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    # Optional: ensure all hyperparameter keys appear as columns (fill missing ones with None).\n",
    "    for key in keys:\n",
    "        if key not in final_df.columns:\n",
    "            final_df[key] = None\n",
    "    return final_df\n",
    "\n",
    "def compute_parascore(row):\n",
    "    \"\"\"\n",
    "    Compute the parascore for a given row.\n",
    "    The function calls the scorer.score method with:\n",
    "      - cands: a list containing the paraphrased_text from the row,\n",
    "      - refs: a list containing the original text from the row,\n",
    "    and returns the third element (index 2) of the resulting score.\n",
    "    \"\"\"\n",
    "    # Create lists as required: first value is paraphrased_text and second is text.\n",
    "    score = scorer.score(\n",
    "        cands=[row[\"paraphrased_text\"]],\n",
    "        refs=[row[\"text\"]],\n",
    "        batch_size=16\n",
    "    )\n",
    "    return score[2].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72deda70-2b25-421f-9d4a-ab18c677cb77",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bdd29bd-c3d8-4953-97c4-7bdd428ead2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(raw_data_loc)\n",
    "\n",
    "# Rename doc_id to orig_doc_id first\n",
    "df.rename(columns={'doc_id': 'orig_doc_id'}, inplace=True)\n",
    "\n",
    "# Create the new doc_id column directly\n",
    "df['doc_id'] = df['orig_doc_id'].apply(create_temp_doc_id)\n",
    "df['tokens'] = df['text'].apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# Move the new doc_id column to the front\n",
    "cols = ['doc_id'] + [col for col in df.columns if col not in ['doc_id', 'text']] + ['text']\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "df = df.sort_values(by='tokens', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392ea300-c90e-4cc8-9d5d-5ce7c8c1869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to process in raw data: 225\n",
      "Files Complete: 0\n",
      "Files to be Processed: 225\n"
     ]
    }
   ],
   "source": [
    "docs = df['doc_id']\n",
    "\n",
    "completed_files = [\n",
    "    os.path.splitext(f)[0]  # Removes the file extension\n",
    "    for f in os.listdir(save_loc)\n",
    "    if os.path.isfile(os.path.join(save_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "files_to_be_processed = list(set(docs) - set(completed_files))\n",
    "files_to_be_processed = sorted(files_to_be_processed)\n",
    "\n",
    "print(f\"Number of documents to process in raw data: {len(docs)}\")\n",
    "print(f\"Files Complete: {len(completed_files)}\")\n",
    "print(f\"Files to be Processed: {len(files_to_be_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bbee5b-7a28-4fd2-9e0e-7f513a8e7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['doc_id'].isin(completed_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a0eaa8-dc5e-4079-b2ba-81f7e6d40df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>orig_doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>cosmos416_text_2</td>\n",
       "      <td>known [Cosmos416 - Text-2].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Cosmos416</td>\n",
       "      <td>known</td>\n",
       "      <td>67</td>\n",
       "      <td>I have to go to my girlfriend's house for Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>bulldog123_text_4</td>\n",
       "      <td>known [Bulldog123 - Text-4].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Bulldog123</td>\n",
       "      <td>known</td>\n",
       "      <td>76</td>\n",
       "      <td>If it's the later, then move the conversation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>fakirbakir_text_2</td>\n",
       "      <td>known [Fakirbakir - Text-2].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Fakirbakir</td>\n",
       "      <td>known</td>\n",
       "      <td>84</td>\n",
       "      <td>Attila Turk's research Only ten graveyards or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>enemesis_text_1</td>\n",
       "      <td>known [Enemesis - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Enemesis</td>\n",
       "      <td>known</td>\n",
       "      <td>85</td>\n",
       "      <td>That link that siafu left sort of backs up wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>cosmos416_text_3</td>\n",
       "      <td>known [Cosmos416 - Text-3].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Cosmos416</td>\n",
       "      <td>known</td>\n",
       "      <td>86</td>\n",
       "      <td>This includes unpublished facts, arguments, sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>hamiltonstone_text_4</td>\n",
       "      <td>known [Hamiltonstone - Text-4].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hamiltonstone</td>\n",
       "      <td>known</td>\n",
       "      <td>763</td>\n",
       "      <td>Not sure whether footnote is the best solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>alienus_text_10</td>\n",
       "      <td>known [Alienus - Text-10].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Alienus</td>\n",
       "      <td>known</td>\n",
       "      <td>775</td>\n",
       "      <td>For someone who's at three reverts but hasn't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aban1313_text_2</td>\n",
       "      <td>known [Aban1313 - Text-2].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>known</td>\n",
       "      <td>777</td>\n",
       "      <td>Why all these half truths, why this desperate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>essjay_text_4</td>\n",
       "      <td>known [Essjay - Text-4].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Essjay</td>\n",
       "      <td>known</td>\n",
       "      <td>807</td>\n",
       "      <td>He would probably be the best person to ask wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>essjay_text_1</td>\n",
       "      <td>known [Essjay - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Essjay</td>\n",
       "      <td>known</td>\n",
       "      <td>840</td>\n",
       "      <td>I have received an astounding amount of suppor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   doc_id                         orig_doc_id corpus  \\\n",
       "96       cosmos416_text_2      known [Cosmos416 - Text-2].txt   Wiki   \n",
       "74      bulldog123_text_4     known [Bulldog123 - Text-4].txt   Wiki   \n",
       "160     fakirbakir_text_2     known [Fakirbakir - Text-2].txt   Wiki   \n",
       "141       enemesis_text_1       known [Enemesis - Text-1].txt   Wiki   \n",
       "97       cosmos416_text_3      known [Cosmos416 - Text-3].txt   Wiki   \n",
       "..                    ...                                 ...    ...   \n",
       "202  hamiltonstone_text_4  known [Hamiltonstone - Text-4].txt   Wiki   \n",
       "21        alienus_text_10       known [Alienus - Text-10].txt   Wiki   \n",
       "7         aban1313_text_2       known [Aban1313 - Text-2].txt   Wiki   \n",
       "155         essjay_text_4         known [Essjay - Text-4].txt   Wiki   \n",
       "153         essjay_text_1         known [Essjay - Text-1].txt   Wiki   \n",
       "\n",
       "            author texttype  tokens  \\\n",
       "96       Cosmos416    known      67   \n",
       "74      Bulldog123    known      76   \n",
       "160     Fakirbakir    known      84   \n",
       "141       Enemesis    known      85   \n",
       "97       Cosmos416    known      86   \n",
       "..             ...      ...     ...   \n",
       "202  Hamiltonstone    known     763   \n",
       "21         Alienus    known     775   \n",
       "7         Aban1313    known     777   \n",
       "155         Essjay    known     807   \n",
       "153         Essjay    known     840   \n",
       "\n",
       "                                                  text  \n",
       "96   I have to go to my girlfriend's house for Than...  \n",
       "74   If it's the later, then move the conversation ...  \n",
       "160  Attila Turk's research Only ten graveyards or ...  \n",
       "141  That link that siafu left sort of backs up wha...  \n",
       "97   This includes unpublished facts, arguments, sp...  \n",
       "..                                                 ...  \n",
       "202  Not sure whether footnote is the best solution...  \n",
       "21   For someone who's at three reverts but hasn't ...  \n",
       "7    Why all these half truths, why this desperate ...  \n",
       "155  He would probably be the best person to ask wh...  \n",
       "153  I have received an astounding amount of suppor...  \n",
       "\n",
       "[225 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd99f8-fa81-44ad-b7db-591b9c8fbf35",
   "metadata": {},
   "source": [
    "### Gridsearch to find top parameters\n",
    "\n",
    "This was used originally to generate optimal parameters across the Enron dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd494cd3-7c6d-4270-b599-43d9e7e4278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"temperature\": [0.7, 0.8, 0.9],\n",
    "#     'top_p': [0.8, 0.9, 1.0],\n",
    "#     # \"frequency_penalty\": [0.0, 0.5, 1.0],\n",
    "# }\n",
    "\n",
    "# gridsearch_results = grid_search_paraphrases(df, system_prompt, client, param_grid, sample_size=10, m=1, n_completions=10)\n",
    "# gridsearch_results[\"parascore\"] = gridsearch_results.apply(compute_parascore, axis=1)\n",
    "# gridsearch_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145911fb-5552-4146-b9d7-1c24a6401702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Group by the hyperparameter columns and compute the mean parascore\n",
    "# grouped = gridsearch_results.groupby([\"temperature\", \"top_p\"])[\"parascore\"].mean()\n",
    "# print(\"Grouped Mean Parascores:\")\n",
    "# print(grouped)\n",
    "\n",
    "# # Determine the combination with the highest mean parascore\n",
    "# best_params = grouped.idxmax()  # This returns a tuple, e.g., (0.8, 0.9)\n",
    "# best_score = grouped.loc[best_params]\n",
    "\n",
    "# print(\"\\nBest hyperparameter combination (by average score):\")\n",
    "# print(f\"Temperature = {best_params[0]}\")\n",
    "# print(f\"Top p = {best_params[1]}\")\n",
    "# print(f\"Average Parascore = {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7326c98c-7675-46ca-bb41-0ea27a20791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected values are temperature = 0.7, top_p = 0.8 with average ParaScore = 0.7910261923074722"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510f83f-78ae-4647-8493-86f24b2dffdf",
   "metadata": {},
   "source": [
    "### Function to process dataframe by document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0f068bb-48be-467c-a119-3956d29c11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_by_doc_id(df, system_prompt, client, save_loc, n=10, m=1, max_fails=3, **llm_params):\n",
    "    \"\"\"\n",
    "    For each unique doc_id in the DataFrame:\n",
    "      1. Print a header: \"Document: <doc_id>, Doc <i> out of <total_docs>\"\n",
    "      2. Filter the DataFrame to include only rows with that doc_id.\n",
    "      3. Loop m times, printing for each iteration: \"Iteration: <current iteration> out of <m>\".\n",
    "      4. Call paraphrase_dataframe (with m=1 per iteration), handling failures gracefully.\n",
    "      5. If an iteration fails, move to the next. If max_fails in a row occur, move to the next document.\n",
    "      6. Concatenate the successful results and save them to a JSONL file with the doc_id appended to save_loc.\n",
    "    \n",
    "    Parameters:\n",
    "      - df: Input DataFrame containing at least 'doc_id' and 'text' columns.\n",
    "      - system_prompt: The system prompt for the LLM.\n",
    "      - client: The LLM client instance.\n",
    "      - save_loc: Base save location (filename prefix); the doc_id is appended.\n",
    "      - n: Number of completions per LLM call.\n",
    "      - m: Number of iterations per doc_id.\n",
    "      - max_fails: Maximum allowed consecutive failures before skipping the document.\n",
    "      - **llm_params: Additional hyperparameters for the LLM call.\n",
    "    \"\"\"\n",
    "    unique_doc_ids = df['doc_id'].unique()\n",
    "    num_docs = len(unique_doc_ids)\n",
    "    \n",
    "    for idx, doc_id in enumerate(unique_doc_ids, start=1):\n",
    "        print(f\"Document: {doc_id} - Doc {idx} out of {num_docs}\")\n",
    "        \n",
    "        filtered_df = df[df['doc_id'] == doc_id]\n",
    "        iter_dfs = []\n",
    "        fail_count = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            if fail_count >= max_fails:\n",
    "                print(f\"Skipping document {doc_id} due to {fail_count} consecutive failures.\")\n",
    "                break\n",
    "            \n",
    "            print(f\"    Iteration: {i+1} out of {m}\")\n",
    "            try:\n",
    "                iter_df = paraphrase_dataframe(filtered_df, system_prompt, client, n=n, m=1, **llm_params)\n",
    "                iter_dfs.append(iter_df)\n",
    "                fail_count = 0  # Reset failure count on success\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in iteration {i+1} for document {doc_id}: {e}\")\n",
    "                fail_count += 1\n",
    "                continue  # Skip to the next iteration\n",
    "        \n",
    "        if iter_dfs:\n",
    "            result_df = pd.concat(iter_dfs, ignore_index=True)\n",
    "            file_path = f\"{save_loc}{doc_id}.jsonl\"\n",
    "            write_jsonl(result_df, file_path)\n",
    "            print(f\"Saved results for document {doc_id} to {file_path}\")\n",
    "        else:\n",
    "            print(f\"No successful iterations for document {doc_id}, skipping save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98848b-f926-4780-b6bd-51ddf5aaf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_by_doc_id(df, system_prompt, client, save_loc, m=100, n=10, max_fails=5, temperature=0.7, top_p=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
