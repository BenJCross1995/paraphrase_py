{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e6aa1dc4-745d-454b-a505-780a6fd80aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "3ba1209d-493e-4d0e-8377-5e7d62e85486",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_loc = \"../../credentials.json\"\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"Enron\"\n",
    "\n",
    "base_loc = f\"/Volumes/BCross/datasets/author_verification/\"\n",
    "data_loc = f\"{base_loc}{data_type}/{corpus}/\"\n",
    "\n",
    "raw_data_loc = f\"{data_loc}known_raw.jsonl\"\n",
    "processed_data_loc = f\"{data_loc}known_processed.jsonl\"\n",
    "batch_complete_loc = f\"{data_loc}batch_sentence_complete/\"\n",
    "\n",
    "# ParaScore save location\n",
    "post_process_loc = f\"{data_loc}batch_postprocessed/\"\n",
    "os.makedirs(post_process_loc, exist_ok=True)\n",
    "\n",
    "# Phone number for WhatsApp notifications\n",
    "phone_number = \"+447756976114\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6d9d8-5152-4e02-b2ee-c1a3f0918513",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ba2605b5-1dcb-47eb-a4bd-28ae4359988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            parsed_line = json.loads(line)\n",
    "            # If the line is a single-element list, extract the first element\n",
    "            if isinstance(parsed_line, list) and len(parsed_line) == 1:\n",
    "                data.append(parsed_line[0])\n",
    "            else:\n",
    "                data.append(parsed_line)\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "def write_jsonl(data, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for _, row in data.iterrows():\n",
    "            json.dump(row.to_dict(), file)\n",
    "            file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "25758043-d1ca-41f7-bd72-32c0ee397fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents to process in raw data: 112\n",
      "Files Complete in Batch location: 112\n",
      "Files Post Processed: 0\n",
      "Files to be Processed: 112\n"
     ]
    }
   ],
   "source": [
    "raw_df = read_jsonl(raw_data_loc)\n",
    "\n",
    "batch_completed_files = [\n",
    "    f for f in os.listdir(batch_complete_loc)\n",
    "    if os.path.isfile(os.path.join(batch_complete_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "files_processed = [\n",
    "    f for f in os.listdir(post_process_loc)\n",
    "    if os.path.isfile(os.path.join(post_process_loc, f)) and f.endswith('.jsonl')\n",
    "]\n",
    "\n",
    "# Replace \"batch_\" with \"doc_\" in each element of files_processed\n",
    "files_processed = [file.replace(\"doc_\", \"batch_\") for file in files_processed]\n",
    "\n",
    "files_to_be_processed = list(set(batch_completed_files) - set(files_processed))\n",
    "\n",
    "print(f\"Number of documents to process in raw data: {len(raw_df['doc_id'])}\")\n",
    "print(f\"Files Complete in Batch location: {len(batch_completed_files)}\")\n",
    "print(f\"Files Post Processed: {len(files_processed)}\")\n",
    "print(f\"Files to be Processed: {len(files_to_be_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "14b84203-93a2-409a-ba99-43453bc6a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_custon_id(custon_id):\n",
    "    parts = custon_id.split('_')\n",
    "    doc_id = parts[2] + \"_\" + parts[3] + \"_\" + parts[4]\n",
    "    chunk_id = parts[-2]\n",
    "    repetition = parts[-1]\n",
    "    return pd.Series([doc_id, chunk_id, repetition])\n",
    "\n",
    "def parse_response(response_str):\n",
    "    # Convert the JSON string to a Python dictionary\n",
    "    response_dict = json.loads(response_str)\n",
    "    \n",
    "    # Extract the 'original' sentence\n",
    "    original_sentence = response_dict.get('original', '')\n",
    "    \n",
    "    # Extract other keys and add them to the list with 'repetition_i' format\n",
    "    rephrased = []\n",
    "    for key, value in response_dict.items():\n",
    "        if key != 'original':\n",
    "            rephrased.append(value)\n",
    "    \n",
    "    return original_sentence, rephrased\n",
    "\n",
    "def process_dataframe(df):\n",
    "\n",
    "    if 'custom_id' not in df.columns:\n",
    "        df['doc_id'] = df['doc_id'].str.replace(\"batch_\", \"\")\n",
    "\n",
    "        df = df[['doc_id', 'chunk_id', 'original', 'rephrased']]\n",
    "\n",
    "    else:\n",
    "        df[['doc_id', 'chunk_id', 'repetition']] = df['custom_id'].apply(split_custon_id)\n",
    "        \n",
    "        # Apply the parse_response function to each row of the dataframe\n",
    "        df['original_sentence'], df['rephrased'] = zip(*df['response'].apply(parse_response))\n",
    "\n",
    "        df = df[['doc_id', 'chunk_id', 'original_sentence', 'rephrased']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combine_and_unique(group):\n",
    "    # Combine all lists in the 'rephrased' column into one list\n",
    "    combined_list = sum(group['rephrased'], [])\n",
    "    \n",
    "    # Remove duplicates by converting to a set and back to a list\n",
    "    unique_list = list(set(combined_list))\n",
    "    \n",
    "    # Return a DataFrame where each unique rephrased sentence is a new row\n",
    "    return pd.DataFrame({\n",
    "        'doc_id': group['doc_id'].iloc[0],\n",
    "        'chunk_id': group['chunk_id'].iloc[0],\n",
    "        'original': group['original_sentence'].iloc[0],\n",
    "        'rephrased': unique_list\n",
    "    })\n",
    "\n",
    "def process_rephrased_sentences(df):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "        # Group by 'doc_id' and 'chunk_id' and apply the combine_and_unique function\n",
    "        result_df = df.groupby(['doc_id', 'chunk_id']).apply(combine_and_unique).reset_index(drop=True)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d4e85894-1102-4c3c-98e4-29f571aeb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_list(files, base_read_loc, base_write_loc):\n",
    "\n",
    "    files = sorted(files)\n",
    "    total_files = len(files)  # Total number of files to process\n",
    "    for i, file in enumerate(files, start=1):  # Add index to track file number\n",
    "        complete_combine_step = False\n",
    "        try:\n",
    "            print(f\"Processing file {i} of {total_files}: {file}\")  # Tracker message\n",
    "            \n",
    "            # Step 1: Read the JSONL file\n",
    "            response = read_jsonl(f\"{base_read_loc}{file}\")\n",
    "\n",
    "            if 'custom_id' in response.columns:\n",
    "                complete_combine_step = True\n",
    "            \n",
    "            # Step 2: Process the DataFrame\n",
    "            processed_dataframe = process_dataframe(response)\n",
    "\n",
    "            if complete_combine_step:\n",
    "                # Step 3: Split the list into separate rows\n",
    "                final_df = process_rephrased_sentences(processed_dataframe)\n",
    "            else:\n",
    "                final_df = processed_dataframe\n",
    "            \n",
    "            # Step 4: Save the processed DataFrame to a new location\n",
    "            save_loc = f\"{base_write_loc}{file.replace('batch', 'doc')}\"\n",
    "            write_jsonl(final_df, save_loc)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing {file}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "640dc1c8-3389-42c1-8bb6-a8c5b2ff0fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 112: batch_andy_zipper_mail_1.jsonl\n",
      "    Error processing batch_andy_zipper_mail_1.jsonl: Expecting ',' delimiter: line 2 column 5047 (char 5048)\n",
      "Processing file 2 of 112: batch_andy_zipper_mail_3.jsonl\n",
      "Processing file 3 of 112: batch_andy_zipper_mail_4.jsonl\n",
      "Processing file 4 of 112: batch_andy_zipper_mail_5.jsonl\n",
      "Processing file 5 of 112: batch_barry_tycholiz_mail_1.jsonl\n",
      "Processing file 6 of 112: batch_barry_tycholiz_mail_3.jsonl\n",
      "Processing file 7 of 112: batch_barry_tycholiz_mail_4.jsonl\n",
      "Processing file 8 of 112: batch_barry_tycholiz_mail_5.jsonl\n",
      "Processing file 9 of 112: batch_benjamin_rogers_mail_2.jsonl\n",
      "Processing file 10 of 112: batch_benjamin_rogers_mail_3.jsonl\n",
      "Processing file 11 of 112: batch_benjamin_rogers_mail_4.jsonl\n",
      "Processing file 12 of 112: batch_bill_williams_mail_2.jsonl\n",
      "Processing file 13 of 112: batch_bill_williams_mail_3.jsonl\n",
      "Processing file 14 of 112: batch_bill_williams_mail_4.jsonl\n",
      "Processing file 15 of 112: batch_cara_semperger_mail_1.jsonl\n",
      "Processing file 16 of 112: batch_cara_semperger_mail_2.jsonl\n",
      "Processing file 17 of 112: batch_cara_semperger_mail_3.jsonl\n",
      "Processing file 18 of 112: batch_cara_semperger_mail_5.jsonl\n",
      "Processing file 19 of 112: batch_carol_clair_mail_1.jsonl\n",
      "Processing file 20 of 112: batch_carol_clair_mail_3.jsonl\n",
      "Processing file 21 of 112: batch_carol_clair_mail_4.jsonl\n",
      "Processing file 22 of 112: batch_carol_clair_mail_5.jsonl\n",
      "Processing file 23 of 112: batch_chris_dorland_mail_1.jsonl\n",
      "Processing file 24 of 112: batch_chris_dorland_mail_2.jsonl\n",
      "Processing file 25 of 112: batch_chris_dorland_mail_4.jsonl\n",
      "Processing file 26 of 112: batch_chris_dorland_mail_5.jsonl\n",
      "Processing file 27 of 112: batch_cooper_richey_mail_1.jsonl\n",
      "Processing file 28 of 112: batch_cooper_richey_mail_2.jsonl\n",
      "Processing file 29 of 112: batch_cooper_richey_mail_4.jsonl\n",
      "Processing file 30 of 112: batch_d_steffes_mail_1.jsonl\n",
      "Processing file 31 of 112: batch_d_steffes_mail_2.jsonl\n",
      "Processing file 32 of 112: batch_d_steffes_mail_4.jsonl\n",
      "Processing file 33 of 112: batch_d_steffes_mail_5.jsonl\n",
      "Processing file 34 of 112: batch_d_thomas_mail_1.jsonl\n",
      "    Error processing batch_d_thomas_mail_1.jsonl: Expecting ',' delimiter: line 2 column 5132 (char 5133)\n",
      "Processing file 35 of 112: batch_d_thomas_mail_2.jsonl\n",
      "    Error processing batch_d_thomas_mail_2.jsonl: Expecting ',' delimiter: line 14 column 305 (char 6575)\n",
      "Processing file 36 of 112: batch_d_thomas_mail_5.jsonl\n",
      "    Error processing batch_d_thomas_mail_5.jsonl: Unterminated string starting at: line 165 column 109 (char 24659)\n",
      "Processing file 37 of 112: batch_dan_hyvl_mail_2.jsonl\n",
      "Processing file 38 of 112: batch_dan_hyvl_mail_3.jsonl\n",
      "Processing file 39 of 112: batch_dan_hyvl_mail_4.jsonl\n",
      "Processing file 40 of 112: batch_dana_davis_mail_2.jsonl\n",
      "Processing file 41 of 112: batch_dana_davis_mail_3.jsonl\n",
      "Processing file 42 of 112: batch_dana_davis_mail_4.jsonl\n",
      "Processing file 43 of 112: batch_daren_farmer_mail_1.jsonl\n",
      "Processing file 44 of 112: batch_daren_farmer_mail_2.jsonl\n",
      "Processing file 45 of 112: batch_daren_farmer_mail_4.jsonl\n",
      "Processing file 46 of 112: batch_daren_farmer_mail_5.jsonl\n",
      "Processing file 47 of 112: batch_darrell_schoolcraft_mail_1.jsonl\n",
      "Processing file 48 of 112: batch_darrell_schoolcraft_mail_2.jsonl\n",
      "Processing file 49 of 112: batch_darrell_schoolcraft_mail_4.jsonl\n",
      "Processing file 50 of 112: batch_darron_giron_mail_2.jsonl\n",
      "Processing file 51 of 112: batch_darron_giron_mail_3.jsonl\n",
      "Processing file 52 of 112: batch_darron_giron_mail_4.jsonl\n",
      "Processing file 53 of 112: batch_darron_giron_mail_5.jsonl\n",
      "Processing file 54 of 112: batch_david_delainey_mail_1.jsonl\n",
      "Processing file 55 of 112: batch_david_delainey_mail_2.jsonl\n",
      "    Error processing batch_david_delainey_mail_2.jsonl: Unterminated string starting at: line 2 column 15 (char 16)\n",
      "Processing file 56 of 112: batch_david_delainey_mail_4.jsonl\n",
      "Processing file 57 of 112: batch_debra_perlingiere_mail_1.jsonl\n",
      "Processing file 58 of 112: batch_debra_perlingiere_mail_3.jsonl\n",
      "Processing file 59 of 112: batch_debra_perlingiere_mail_4.jsonl\n",
      "Processing file 60 of 112: batch_drew_fossum_mail_1.jsonl\n",
      "    Error processing batch_drew_fossum_mail_1.jsonl: Expecting ',' delimiter: line 2 column 5073 (char 5074)\n",
      "Processing file 61 of 112: batch_drew_fossum_mail_2.jsonl\n",
      "Processing file 62 of 112: batch_drew_fossum_mail_3.jsonl\n",
      "Processing file 63 of 112: batch_elizabeth_sager_mail_1.jsonl\n",
      "Processing file 64 of 112: batch_elizabeth_sager_mail_2.jsonl\n",
      "Processing file 65 of 112: batch_elizabeth_sager_mail_3.jsonl\n",
      "Processing file 66 of 112: batch_elizabeth_sager_mail_4.jsonl\n",
      "Processing file 67 of 112: batch_errol_mclaughlin_mail_1.jsonl\n",
      "Processing file 68 of 112: batch_errol_mclaughlin_mail_2.jsonl\n",
      "Processing file 69 of 112: batch_errol_mclaughlin_mail_4.jsonl\n",
      "Processing file 70 of 112: batch_errol_mclaughlin_mail_5.jsonl\n",
      "Processing file 71 of 112: batch_gerald_nemec_mail_1.jsonl\n",
      "Processing file 72 of 112: batch_gerald_nemec_mail_2.jsonl\n",
      "Processing file 73 of 112: batch_gerald_nemec_mail_4.jsonl\n",
      "Processing file 74 of 112: batch_gerald_nemec_mail_5.jsonl\n",
      "Processing file 75 of 112: batch_james_derrick_mail_1.jsonl\n",
      "Processing file 76 of 112: batch_james_derrick_mail_3.jsonl\n",
      "Processing file 77 of 112: batch_james_derrick_mail_4.jsonl\n",
      "Processing file 78 of 112: batch_jane_tholt_mail_1.jsonl\n",
      "Processing file 79 of 112: batch_jane_tholt_mail_2.jsonl\n",
      "Processing file 80 of 112: batch_jane_tholt_mail_3.jsonl\n",
      "Processing file 81 of 112: batch_janette_elbertson_mail_1.jsonl\n",
      "Processing file 82 of 112: batch_janette_elbertson_mail_2.jsonl\n",
      "    Error processing batch_janette_elbertson_mail_2.jsonl: Unterminated string starting at: line 19 column 20 (char 1494)\n",
      "Processing file 83 of 112: batch_janette_elbertson_mail_4.jsonl\n",
      "    Error processing batch_janette_elbertson_mail_4.jsonl: Expecting property name enclosed in double quotes: line 17 column 2 (char 1689)\n",
      "Processing file 84 of 112: batch_jeff_dasovich_mail_1.jsonl\n",
      "Processing file 85 of 112: batch_jeff_dasovich_mail_2.jsonl\n",
      "    Error processing batch_jeff_dasovich_mail_2.jsonl: Unterminated string starting at: line 8 column 3 (char 980)\n",
      "Processing file 86 of 112: batch_jeff_dasovich_mail_3.jsonl\n",
      "Processing file 87 of 112: batch_jeff_dasovich_mail_5.jsonl\n",
      "Processing file 88 of 112: batch_jeff_skilling_mail_2.jsonl\n",
      "    Error processing batch_jeff_skilling_mail_2.jsonl: Unterminated string starting at: line 8 column 19 (char 1093)\n",
      "Processing file 89 of 112: batch_jeff_skilling_mail_3.jsonl\n",
      "Processing file 90 of 112: batch_jeff_skilling_mail_4.jsonl\n",
      "Processing file 91 of 112: batch_jeff_skilling_mail_5.jsonl\n",
      "Processing file 92 of 112: batch_jeffrey_shankman_mail_2.jsonl\n",
      "Processing file 93 of 112: batch_jeffrey_shankman_mail_3.jsonl\n",
      "Processing file 94 of 112: batch_jeffrey_shankman_mail_4.jsonl\n",
      "Processing file 95 of 112: batch_jeffrey_shankman_mail_5.jsonl\n",
      "Processing file 96 of 112: batch_joannie_williamson_mail_1.jsonl\n",
      "Processing file 97 of 112: batch_joannie_williamson_mail_2.jsonl\n",
      "Processing file 98 of 112: batch_joannie_williamson_mail_3.jsonl\n",
      "Processing file 99 of 112: batch_john_arnold_mail_1.jsonl\n",
      "    Error processing batch_john_arnold_mail_1.jsonl: Invalid control character at: line 4 column 82 (char 230)\n",
      "Processing file 100 of 112: batch_john_arnold_mail_3.jsonl\n",
      "Processing file 101 of 112: batch_john_arnold_mail_4.jsonl\n",
      "Processing file 102 of 112: batch_john_arnold_mail_5.jsonl\n",
      "Processing file 103 of 112: batch_k_allen_mail_1.jsonl\n",
      "Processing file 104 of 112: batch_k_allen_mail_3.jsonl\n",
      "    Error processing batch_k_allen_mail_3.jsonl: Unterminated string starting at: line 12 column 20 (char 887)\n",
      "Processing file 105 of 112: batch_k_allen_mail_4.jsonl\n",
      "Processing file 106 of 112: batch_kam_keiser_mail_1.jsonl\n",
      "Processing file 107 of 112: batch_kam_keiser_mail_2.jsonl\n",
      "Processing file 108 of 112: batch_kam_keiser_mail_3.jsonl\n",
      "Processing file 109 of 112: batch_kam_keiser_mail_5.jsonl\n",
      "Processing file 110 of 112: batch_kate_symes_mail_1.jsonl\n",
      "Processing file 111 of 112: batch_kate_symes_mail_3.jsonl\n",
      "    Error processing batch_kate_symes_mail_3.jsonl: Unterminated string starting at: line 21 column 20 (char 3216)\n",
      "Processing file 112 of 112: batch_kate_symes_mail_4.jsonl\n",
      "    Error processing batch_kate_symes_mail_4.jsonl: Unterminated string starting at: line 17 column 20 (char 2499)\n"
     ]
    }
   ],
   "source": [
    "process_file_list(files_to_be_processed, batch_complete_loc, post_process_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "paraphrase_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
