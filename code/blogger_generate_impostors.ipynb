{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f076cc56-896b-44a4-b451-ed3d0a88d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import combine_rephrased\n",
    "import read_and_write_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93d25ad-ca32-415f-958d-8bcbd21cc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_drive_base_loc = \"/Users/user/Library/CloudStorage/GoogleDrive-benjcross1995@gmail.com/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a8f43b-476b-4ec7-87e3-62085f10ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrased preprocessed data\n",
    "# rephrased_preprocessed = f\"{g_drive_base_loc}datasets/blogger_new_algorithm/rephrased_preprocessed.jsonl\"\n",
    "rephrased_preprocessed = \"../../../datasets/blogger_new_algorithm/rephrased_preprocessed.jsonl\"\n",
    "preprocessed_df = read_and_write_docs.read_jsonl_file(rephrased_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03956e2-4783-4ed9-8d04-b82d320df974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the rephrased sentences loc\n",
    "# rephrased_sentences_loc = f\"{g_drive_base_loc}datasets/blogger_new_algorithm/rephrased_sentences/\"\n",
    "rephrased_sentences_loc = \"../../../datasets/blogger_new_algorithm/phi_rephrased\"\n",
    "\n",
    "files = [file for file in Path(rephrased_sentences_loc).iterdir() if file.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c83d6d-5e1f-47bd-94ed-617b60df602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_combine(old_df, file_paths, num_impostors=1000):\n",
    "    \"\"\"\n",
    "    Process a list of file paths, filter the old DataFrame based on doc_id, \n",
    "    and combine the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - old_df: The old DataFrame to filter.\n",
    "    - file_paths: List of file paths to process.\n",
    "    - num_impostors: Parameter for the combine function.\n",
    "    \n",
    "    Returns:\n",
    "    - result_df: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    result_dfs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "\n",
    "        # Read the new DataFrame from the file\n",
    "        new_df = read_and_write_docs.read_jsonl_file(file_path)\n",
    "        \n",
    "        # Ensure 'doc_id' is in the DataFrame\n",
    "        if 'doc_id' not in new_df.columns:\n",
    "            raise ValueError(f\"The file {file_path} does not contain a 'doc_id' column.\")\n",
    "        \n",
    "        # Get the unique doc_id from the new DataFrame\n",
    "        new_doc_id = new_df['doc_id'].unique()[0]\n",
    "        \n",
    "        # Filter the old DataFrame for the doc_id\n",
    "        filtered_old_df = old_df[old_df['doc_id'] == new_doc_id].copy()\n",
    "\n",
    "        print(f\"Sampling Doc: {new_doc_id}\")\n",
    "        # Combine the filtered old DataFrame with the new DataFrame\n",
    "        combined_df = combine_rephrased.chunk_single_rephrased(filtered_old_df, new_df, num_impostors=num_impostors)\n",
    "        \n",
    "        # Append the combined DataFrame to the list of result DataFrames\n",
    "        result_dfs.append(combined_df)\n",
    "    \n",
    "    # Concatenate all the result DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming read_and_write_docs.read_jsonl_file and combine_rephrased.chunk_single_rephrased are defined\n",
    "# result_df = process_and_combine(preprocessed_df, file_paths, read_and_write_docs.read_jsonl_file, combine_rephrased.chunk_single_rephrased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3b547a-6a97-42bd-a220-921eda0a7e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_114599.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_18516.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_213621.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_253553.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_288775.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_299096.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_349665.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_357437.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_401638.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_402496.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_415925.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_435880.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_463289.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_490831.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_501802.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_509577.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_546828.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_569896.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_585623.jsonl'),\n",
       " WindowsPath('../../../datasets/blogger_new_algorithm/phi_rephrased/doc_676573.jsonl')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26d32f4-fe5f-4fc2-8ce4-f7e0292d17e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Doc: 114599\n",
      "Sampling Doc: 18516\n",
      "Sampling Doc: 213621\n",
      "Sampling Doc: 253553\n",
      "Sampling Doc: 288775\n",
      "Sampling Doc: 299096\n",
      "Sampling Doc: 349665\n",
      "Sampling Doc: 357437\n",
      "Sampling Doc: 401638\n",
      "Sampling Doc: 402496\n",
      "Sampling Doc: 415925\n",
      "Sampling Doc: 435880\n",
      "Sampling Doc: 463289\n",
      "Sampling Doc: 490831\n",
      "Sampling Doc: 501802\n",
      "Sampling Doc: 509577\n",
      "Sampling Doc: 546828\n",
      "Sampling Doc: 569896\n",
      "Sampling Doc: 585623\n",
      "Sampling Doc: 676573\n"
     ]
    }
   ],
   "source": [
    "result = process_and_combine(preprocessed_df, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47f1404-4c1f-4802-adc0-4aadd4359dca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/user/Library/CloudStorage/GoogleDrive-benjcross1995@gmail.com/My Drive/datasets/blogger_new_algorithm/phi_impostor_paragraphs_noqual.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mread_and_write_docs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mg_drive_base_loc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mdatasets/blogger_new_algorithm/phi_impostor_paragraphs_noqual.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\paraphrase_py\\code\\read_and_write_docs.py:19\u001b[0m, in \u001b[0;36msave_as_jsonl\u001b[1;34m(data, output_file_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_as_jsonl\u001b[39m(data, output_file_path):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     21\u001b[0m             json\u001b[38;5;241m.\u001b[39mdump(row\u001b[38;5;241m.\u001b[39mto_dict(), file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/user/Library/CloudStorage/GoogleDrive-benjcross1995@gmail.com/My Drive/datasets/blogger_new_algorithm/phi_impostor_paragraphs_noqual.jsonl'"
     ]
    }
   ],
   "source": [
    "read_and_write_docs.save_as_jsonl(result, \"../../../datasets/blogger_new_algorithm/phi_impostor_paragraphs_noqual.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31630a36-40ff-463e-b51e-87635d73fc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "para_llm",
   "language": "python",
   "name": "para_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
