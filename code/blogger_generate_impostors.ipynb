{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f076cc56-896b-44a4-b451-ed3d0a88d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import combine_rephrased\n",
    "import read_and_write_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d93d25ad-ca32-415f-958d-8bcbd21cc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_drive_base_loc = \"/Users/user/Library/CloudStorage/GoogleDrive-benjcross1995@gmail.com/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a8f43b-476b-4ec7-87e3-62085f10ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrased preprocessed data\n",
    "rephrased_preprocessed = f\"{g_drive_base_loc}datasets/blogger_new_algorithm/rephrased_preprocessed.jsonl\"\n",
    "preprocessed_df = read_and_write_docs.read_jsonl_file(rephrased_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e03956e2-4783-4ed9-8d04-b82d320df974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the rephrased sentences loc\n",
    "rephrased_sentences_loc = f\"{g_drive_base_loc}datasets/blogger_new_algorithm/rephrased_sentences/\"\n",
    "\n",
    "files = [file for file in Path(rephrased_sentences_loc).iterdir() if file.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49292b11-527f-46bd-b893-9e00fa48588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = read_and_write_docs.read_jsonl_file(files[0])\n",
    "\n",
    "new_doc_id = new_df['doc_id'].unique()[0]\n",
    "\n",
    "old_df = preprocessed_df[preprocessed_df['doc_id'] == new_doc_id].copy()\n",
    "\n",
    "result = combine_rephrased.chunk_single_rephrased(old_df, new_df, num_impostors=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c83d6d-5e1f-47bd-94ed-617b60df602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_combine(old_df, file_paths, num_impostors=1000):\n",
    "    \"\"\"\n",
    "    Process a list of file paths, filter the old DataFrame based on doc_id, \n",
    "    and combine the results into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - old_df: The old DataFrame to filter.\n",
    "    - file_paths: List of file paths to process.\n",
    "    - num_impostors: Parameter for the combine function.\n",
    "    \n",
    "    Returns:\n",
    "    - result_df: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    result_dfs = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "\n",
    "        # Read the new DataFrame from the file\n",
    "        new_df = read_and_write_docs.read_jsonl_file(file_path)\n",
    "        \n",
    "        # Ensure 'doc_id' is in the DataFrame\n",
    "        if 'doc_id' not in new_df.columns:\n",
    "            raise ValueError(f\"The file {file_path} does not contain a 'doc_id' column.\")\n",
    "        \n",
    "        # Get the unique doc_id from the new DataFrame\n",
    "        new_doc_id = new_df['doc_id'].unique()[0]\n",
    "        \n",
    "        # Filter the old DataFrame for the doc_id\n",
    "        filtered_old_df = old_df[old_df['doc_id'] == new_doc_id].copy()\n",
    "\n",
    "        print(f\"Sampling Doc: {new_doc_id}\")\n",
    "        # Combine the filtered old DataFrame with the new DataFrame\n",
    "        combined_df = combine_rephrased.chunk_single_rephrased(filtered_old_df, new_df, num_impostors=num_impostors)\n",
    "        \n",
    "        # Append the combined DataFrame to the list of result DataFrames\n",
    "        result_dfs.append(combined_df)\n",
    "    \n",
    "    # Concatenate all the result DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming read_and_write_docs.read_jsonl_file and combine_rephrased.chunk_single_rephrased are defined\n",
    "# result_df = process_and_combine(preprocessed_df, file_paths, read_and_write_docs.read_jsonl_file, combine_rephrased.chunk_single_rephrased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26d32f4-fe5f-4fc2-8ce4-f7e0292d17e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Doc: 415925\n",
      "Sampling Doc: 569896\n",
      "Sampling Doc: 253553\n",
      "Sampling Doc: 401638\n",
      "Sampling Doc: 357437\n",
      "Sampling Doc: 213621\n",
      "Sampling Doc: 349665\n",
      "Sampling Doc: 501802\n",
      "Sampling Doc: 288775\n",
      "Sampling Doc: 546828\n",
      "Sampling Doc: 402496\n",
      "Sampling Doc: 435880\n",
      "Sampling Doc: 585623\n",
      "Sampling Doc: 509577\n",
      "Sampling Doc: 490831\n",
      "Sampling Doc: 299096\n",
      "Sampling Doc: 463289\n",
      "Sampling Doc: 114599\n",
      "Sampling Doc: 676573\n",
      "Sampling Doc: 18516\n"
     ]
    }
   ],
   "source": [
    "result = process_and_combine(preprocessed_df, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a47f1404-4c1f-4802-adc0-4aadd4359dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_and_write_docs.save_as_jsonl(result, f\"{g_drive_base_loc}datasets/blogger_new_algorithm/impostors.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31630a36-40ff-463e-b51e-87635d73fc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
