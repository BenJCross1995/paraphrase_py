{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04ea14f",
   "metadata": {},
   "source": [
    "# Aggreagated and Profile Raw Table Creation\n",
    "\n",
    "The purpose of this notebook is to take the corpus metadata and the known and unknown tables and create the relevant profile and aggregated tables. Aggregated here means in preparation for an aggregated AV method so we have a row for each known document in the problems. Profile means for each problem we have a single row and all known documents are concatenated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "425f81d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../../code/'\n",
      "/Users/user/Documents/GitHub/paraphrase_py/code\n"
     ]
    }
   ],
   "source": [
    "# 1) Set the location\n",
    "%cd ../../code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "098dc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_and_write_docs import read_jsonl, read_rds, write_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86af1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"training\"\n",
    "\n",
    "metadata_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "known_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/known_raw_dataframe.rds\"\n",
    "unknown_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/unknown_raw_dataframe.rds\"\n",
    "base_save_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1a2ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = read_rds(known_loc)\n",
    "unknown = read_rds(unknown_loc)\n",
    "metadata = read_rds(metadata_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d898b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the known data - will add a row for all known docs in problem\n",
    "merged = (\n",
    "    metadata\n",
    "        .merge(\n",
    "            known,\n",
    "            how=\"left\",           # keep every row from metadata\n",
    "            left_on=\"known_author\",     # column in metadata\n",
    "            right_on=\"author\",\n",
    "            suffixes=(\"\", \"_known\")  # avoids name clashes if both frames share columns\n",
    "        )\n",
    "        .drop(columns=\"known_author\")   # optional: remove the join key from `known`\n",
    ")\n",
    "\n",
    "#Â Join the unknown data - single row for each row already there\n",
    "merged = (\n",
    "    merged\n",
    "        .merge(\n",
    "            unknown,\n",
    "            how=\"left\",           # keep every row from metadata\n",
    "            left_on=\"unknown_author\",     # column in metadata\n",
    "            right_on=\"author\",\n",
    "            suffixes=(\"\", \"_unknown\")  # avoids name clashes if both frames share columns\n",
    "        )\n",
    "        .drop(columns=\"unknown_author\")   # optional: remove the join key from `known`\n",
    ")\n",
    "\n",
    "# Rename and select columns\n",
    "merged.rename(columns={\n",
    "    \"doc_id\": \"doc_id_known\",\n",
    "    \"author\": \"author_known\",\n",
    "    \"text\": \"text_known\"\n",
    "}, inplace=True)\n",
    "\n",
    "merged = merged[[\"problem\", \"corpus\", \"doc_id_known\", \"doc_id_unknown\",\n",
    "                 \"author_known\", \"author_unknown\", \"text_known\", \"text_unknown\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedf4b7",
   "metadata": {},
   "source": [
    "## Aggregated Table\n",
    "\n",
    "The aggregated table is just the merged table. It has a single row per known document, which will be compared with the unknown document.\n",
    "\n",
    "We then save the aggregated tables in their respective areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5237da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is the aggregated version, we now need the profile version\n",
    "aggregated = merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0300d340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus\n",
       "ACL                   186\n",
       "All-the-news         1776\n",
       "Amazon               6400\n",
       "Enron                 224\n",
       "IMDB                  400\n",
       "Koppel's Blogs       3600\n",
       "Perverted Justice     380\n",
       "Reddit               2400\n",
       "StackExchange         150\n",
       "The Apricity          900\n",
       "The Telegraph         440\n",
       "TripAdvisor           120\n",
       "Wiki                  450\n",
       "Yelp                 1600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated.groupby('corpus').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b2a2a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ACL aggregated dataframe\n",
      "Saving Perverted Justice aggregated dataframe\n",
      "Saving Reddit aggregated dataframe\n",
      "Saving Yelp aggregated dataframe\n",
      "Saving TripAdvisor aggregated dataframe\n",
      "Saving All-the-news aggregated dataframe\n",
      "Saving Koppel's Blogs aggregated dataframe\n",
      "Saving StackExchange aggregated dataframe\n",
      "Saving IMDB aggregated dataframe\n",
      "Saving Amazon aggregated dataframe\n",
      "Saving The Apricity aggregated dataframe\n",
      "Saving Wiki aggregated dataframe\n",
      "Saving The Telegraph aggregated dataframe\n",
      "Saving Enron aggregated dataframe\n"
     ]
    }
   ],
   "source": [
    "corpus_list = list(set(aggregated['corpus']))\n",
    "\n",
    "for corpus in corpus_list:\n",
    "    print(f\"Saving {corpus} aggregated dataframe\")\n",
    "    aggregated_filtered = aggregated[aggregated['corpus'] == corpus]\n",
    "    \n",
    "    save_loc = f\"{base_save_loc}/{corpus}/aggregated_raw.jsonl\"\n",
    "    \n",
    "    write_jsonl(aggregated_filtered, save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282065f",
   "metadata": {},
   "source": [
    "## Profile Table\n",
    "\n",
    "The profile table concatenates all of the known documents in each problem into a single string with a newline separating the text. The idea is now we have all known documents vs the single unknown document for each problem.\n",
    "\n",
    "We then save the profile tables in the respective corpus areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55a75738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns i don't want to aggregate\n",
    "group_cols = [c for c in merged.columns if c not in (\"doc_id_known\", \"text_known\")]\n",
    "\n",
    "profile = (\n",
    "    merged\n",
    "        .groupby(group_cols, as_index=False)\n",
    "        .agg({\n",
    "            \"text_known\": lambda x: \"\\n\".join(x.dropna()),   # concat with newline\n",
    "            \"doc_id_known\": list                            # collect into Python list\n",
    "        })\n",
    ")\n",
    "\n",
    "profile = profile[[\"problem\", \"corpus\", \"doc_id_known\", \"doc_id_unknown\",\n",
    "                   \"author_known\", \"author_unknown\", \"text_known\", \"text_unknown\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9a333d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus\n",
       "ACL                   186\n",
       "All-the-news         1771\n",
       "Amazon               1600\n",
       "Enron                  64\n",
       "IMDB                  400\n",
       "Koppel's Blogs       1200\n",
       "Perverted Justice     208\n",
       "Reddit                800\n",
       "StackExchange         150\n",
       "The Apricity          228\n",
       "The Telegraph         220\n",
       "TripAdvisor           120\n",
       "Wiki                  150\n",
       "Yelp                  320\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.groupby('corpus').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33c32e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Perverted Justice profile dataframe\n",
      "Saving Reddit profile dataframe\n",
      "Saving ACL profile dataframe\n",
      "Saving Yelp profile dataframe\n",
      "Saving TripAdvisor profile dataframe\n",
      "Saving All-the-news profile dataframe\n",
      "Saving Koppel's Blogs profile dataframe\n",
      "Saving StackExchange profile dataframe\n",
      "Saving IMDB profile dataframe\n",
      "Saving The Apricity profile dataframe\n",
      "Saving Wiki profile dataframe\n",
      "Saving Amazon profile dataframe\n",
      "Saving The Telegraph profile dataframe\n",
      "Saving Enron profile dataframe\n"
     ]
    }
   ],
   "source": [
    "corpus_list = list(set(profile['corpus']))\n",
    "\n",
    "for corpus in corpus_list:\n",
    "    print(f\"Saving {corpus} profile dataframe\")\n",
    "    profile_filtered = profile[profile['corpus'] == corpus]\n",
    "    \n",
    "    save_loc = f\"{base_save_loc}/{corpus}/profile_raw.jsonl\"\n",
    "    \n",
    "    write_jsonl(profile_filtered, save_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraphrase_llm",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
