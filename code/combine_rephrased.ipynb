{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "02c2f374-f350-4eda-8ab9-799c83b54ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_and_write_docs\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def filter_chunked_unknown(df):\n",
    "\n",
    "    id_list = df['id'].unique()\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    for doc_id in id_list:\n",
    "\n",
    "        # Filter the dataframe for the current id value, resetting the index\n",
    "        filtered_df = df[df['id'] == doc_id].copy().reset_index(drop=True) \n",
    "\n",
    "        # Add a couple of columns to filter on. The idea is that if we're on row 180 for a \n",
    "        # particular doc which has 190 rows. Then the number of chunks must be less than 11\n",
    "        # as 180 is inclusive. If not we say we won't keep it later on.\n",
    "        filtered_df['row_number'] = filtered_df.index + 1\n",
    "        filtered_df['num_rows'] = len(filtered_df)\n",
    "        filtered_df['end_row'] = filtered_df['row_number'] + filtered_df['chunk_count'] - 1\n",
    "        filtered_df['keep'] = filtered_df['end_row'] <= filtered_df['num_rows']\n",
    "\n",
    "        result_df = pd.concat([result_df, filtered_df], ignore_index=True)\n",
    "\n",
    "    result_df = result_df.drop(columns=['row_number', 'end_row', 'num_rows'])\n",
    "        \n",
    "    return result_df\n",
    "\n",
    "def chunk_rephrased(unknown, rephrased, num_impostors=10):\n",
    "\n",
    "    # We only want to apply the algorithm on only docs we have rephrased \n",
    "    rephrased_docs = rephrased['doc_id'].unique()\n",
    "    unknown = unknown[unknown['id'].isin(rephrased_docs)]\n",
    "    data = []\n",
    "\n",
    "    # We want to loop across the rows in the unknown df\n",
    "    for i in range(len(unknown)):\n",
    "\n",
    "        # Keep the logged data and if the data is not 'kept' then skip the row\n",
    "        # This is to make sure no rows are pulling sentences from other docs\n",
    "        keep = unknown.iloc[i, unknown.columns.get_loc('keep')]\n",
    "        if keep == False:\n",
    "            continue\n",
    "\n",
    "        # Keep the variables for each row that matter\n",
    "        doc_id = unknown.iloc[i, unknown.columns.get_loc('id')]\n",
    "        chunk_id = unknown.iloc[i, unknown.columns.get_loc('chunk_id')]\n",
    "        subchunk_id = unknown.iloc[i, unknown.columns.get_loc('subchunk_id')]\n",
    "\n",
    "        # Get the number of chunks in the new unknown text and filter the unknown\n",
    "        # data to include those chunks.\n",
    "        chunk_count = unknown.iloc[i, unknown.columns.get_loc('chunk_count')]\n",
    "        filtered_unknown = unknown.iloc[i:i+chunk_count,]\n",
    "        \n",
    "        sentence_data = []\n",
    "\n",
    "        # Loop however many times the user desires\n",
    "        for _ in range(num_impostors):\n",
    "            \n",
    "            sentences = []\n",
    "\n",
    "            # Want to loop through the rows in the filtered unknown dataframe\n",
    "            for index, row in filtered_unknown.iterrows():\n",
    "\n",
    "                # Get the variables to filter the rephrased df for current row of unknown df\n",
    "                id_value = row['id']\n",
    "                chunk_id_value = row['chunk_id']\n",
    "                subchunk_id_value = row['subchunk_id']\n",
    "                original_sentence = row['original_sentence']\n",
    "            \n",
    "                filtered_rephrased = rephrased[\n",
    "                    (rephrased['doc_id'] == id_value) & \n",
    "                    (rephrased['chunk_id'] == chunk_id_value) & \n",
    "                    (rephrased['subchunk_id'] == subchunk_id_value)\n",
    "                ]\n",
    "\n",
    "                # Ensure rephrased_list contains only strings of paraphrases and add original sentence\n",
    "                # We add the original sentence incase no rephrases were found we wont skip the chunk.\n",
    "                rephrased_list = filtered_rephrased['text'].tolist()\n",
    "                rephrased_list = [str(item) for item in rephrased_list]\n",
    "                rephrased_list.append(original_sentence)\n",
    "            \n",
    "                # Remove duplicates by converting to a set and back to a list\n",
    "                distinct_list = list(set(rephrased_list))\n",
    "\n",
    "                # select a random sentence and add to a list\n",
    "                sample_sentence = random.choice(distinct_list)\n",
    "                sentences.append(sample_sentence)\n",
    "\n",
    "            # Convert to a paragraph by joining sentences together\n",
    "            paragraph = \" \".join(sentences)\n",
    "            \n",
    "            sentence_data.append({\n",
    "                'doc_id': doc_id,\n",
    "                'chunk_id': chunk_id,\n",
    "                'subchunk_id': subchunk_id,\n",
    "                'rephrased': paragraph\n",
    "            })\n",
    "    \n",
    "        data.extend(sentence_data)\n",
    "    \n",
    "    result_df = pd.DataFrame(data)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to parse arguments and process the input file.\n",
    "    \n",
    "    Parses command line arguments, reads the input file, processes the sentences,\n",
    "    and saves the output to the specified file path.\n",
    "    \"\"\"\n",
    "    # Parse arguments from user\n",
    "    parser = argparse.ArgumentParser(description='Chunk sentences together into paragraphs from paraphrased sentences')\n",
    "    parser.add_argument('--unknown_file_path', type=str, help='Path to unknown docs jsonl file', required=True)\n",
    "    parser.add_argument('--rephrased_file_path', type=str, help='Path to rephrased docs jsonl file', required=True)\n",
    "    parser.add_argument('--output_file_path', type=str, help='Output filepath', required=True)\n",
    "    parser.add_argument('--num_impostors', type=int, default=10, help='The number of impostors for each sentence.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Pull in the unknown and rephrased docs\n",
    "    unknown = read_and_write_docs.read_jsonl_file(args.unknown_file_path)\n",
    "    rephrased = read_and_write_docs.read_jsonl_file(args.rephrased_file_path)\n",
    "\n",
    "    # Log each doc in the unknown whether to keep or remove\n",
    "    logged_unknown = filter_chunked_unknown(unknown)\n",
    "\n",
    "    # Run the function to chunk the rephrased docs\n",
    "    result = chunk_rephrased(logged_unknown, rephrased, args.num_impostors)\n",
    "\n",
    "    read_and_write_docs.save_as_jsonl(result, args.output_file_path)\n",
    "\n",
    "    print(\"Rephrasing of Sentences Complete\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
