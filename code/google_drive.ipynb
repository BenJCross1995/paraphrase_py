{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43ba7b-ec62-4275-8cec-2aeab0a4f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd613f07-3fff-4047-b741-22120dc17808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./read_and_write_docs.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd87ae9-5d46-4d70-a049-cc791bc5cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SCOPES. If modifying it, delete the token.pickle file.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb870d2-5dce-4266-9f8d-4a261ba78209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    \"\"\"Get user credentials or create new ones if needed.\"\"\"\n",
    "    creds = None\n",
    "\n",
    "    # Check if file token.pickle exists\n",
    "    if os.path.exists('token.pickle'):\n",
    "        # Read the token from the file\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "\n",
    "    # If no valid credentials are available, request the user to log in.\n",
    "    if not creds or not creds.valid:\n",
    "        # If token is expired, refresh it, else, request a new one.\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                '../google_credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        # Save the access token in token.pickle file for future usage\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    return creds\n",
    "\n",
    "\n",
    "def connect_to_drive():\n",
    "    \"\"\"Connect to the Google Drive API service.\"\"\"\n",
    "    # Get user credentials\n",
    "    creds = get_credentials()\n",
    "    # Connect to the API service\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "\n",
    "def get_file_list(service, N):\n",
    "    \"\"\"Get a list of first N files or folders from Google Drive.\"\"\"\n",
    "    # Request a list of first N files or folders with name and id from the API.\n",
    "    resource = service.files()\n",
    "    result = resource.list(pageSize=N, fields=\"files(id, kind, name, fullFileExtension, parents)\").execute()\n",
    "    # Return the result dictionary containing the information about the files\n",
    "    return result\n",
    "\n",
    "def search_file_by_name(service, file_name):\n",
    "    \"\"\"Search for a file by name and return its data.\"\"\"\n",
    "    # Define the query to search for the file by name\n",
    "    query = f\"name='{file_name}'\"\n",
    "    \n",
    "    # Request files matching the search query\n",
    "    resource = service.files()\n",
    "    result = resource.list(q=query, fields=\"files(id, kind, name, fullFileExtension, parents)\").execute()\n",
    "    \n",
    "    # Extract the list of files from the result\n",
    "    files = result.get('files', [])\n",
    "    \n",
    "    if files:\n",
    "        # Return the data of the first file found\n",
    "        return files[0]\n",
    "    else:\n",
    "        # If no file is found, return None\n",
    "        return None\n",
    "\n",
    "def get_files_in_folder(service, parent_folder_id):\n",
    "    \"\"\"Get a list of files within a specific parent folder, handling pagination.\"\"\"\n",
    "    files = []\n",
    "    page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Define the query to retrieve files within the parent folder\n",
    "        query = f\"'{parent_folder_id}' in parents\"\n",
    "        \n",
    "        # Request files matching the query\n",
    "        resource = service.files()\n",
    "        result = resource.list(\n",
    "            q=query, \n",
    "            fields=\"nextPageToken, files(id, kind, name, fullFileExtension, parents)\",\n",
    "            pageSize=1000, \n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "        \n",
    "        # Extract the list of files from the result\n",
    "        items = result.get('files', [])\n",
    "        files.extend(items)\n",
    "        \n",
    "        # Check if there are more pages\n",
    "        page_token = result.get('nextPageToken')\n",
    "        if not page_token:\n",
    "            break\n",
    "\n",
    "    return files\n",
    "\n",
    "def get_files_in_folder_recursive(service, parent_folder_id, parent_folder_name=None):\n",
    "    \"\"\"Recursively get all files and folders within a parent folder.\"\"\"\n",
    "\n",
    "    # Initialize an empty list to store file and folder data\n",
    "    files_data = []\n",
    "\n",
    "    # Initialize a stack to store folders to be processed\n",
    "    folders_to_process = [(parent_folder_id, parent_folder_name)]\n",
    "    \n",
    "    # Process folders until the stack is empty\n",
    "    while folders_to_process:\n",
    "        # Pop the last folder from the stack\n",
    "        folder_id, folder_name = folders_to_process.pop()\n",
    "        \n",
    "        # Get a list of files and folders within the current folder\n",
    "        files_in_folder = get_files_in_folder(service, folder_id)\n",
    "        \n",
    "        # Iterate through the list of files and folders\n",
    "        for file_data in files_in_folder:\n",
    "            # Append the file or folder data to the list\n",
    "            files_data.append({\n",
    "                'parent_folder_id': folder_id,\n",
    "                'parent_folder_name': folder_name,\n",
    "                'id': file_data['id'],\n",
    "                'name': file_data['name'],\n",
    "                'kind': file_data['kind'],\n",
    "                'file_extension': file_data.get('fullFileExtension', ''),\n",
    "                'is_folder': file_data.get('fullFileExtension') is None\n",
    "            })\n",
    "            \n",
    "            # If the item is a folder, add it to the stack to be processed later\n",
    "            if file_data.get('fullFileExtension') is None:\n",
    "                subfolder_id = file_data['id']\n",
    "                subfolder_name = file_data['name']\n",
    "                folders_to_process.append((subfolder_id, subfolder_name))\n",
    "\n",
    "    files_df = pd.DataFrame(files_data)\n",
    "    \n",
    "    return files_df\n",
    "\n",
    "def split_name_column(df):\n",
    "    \"\"\"Function to extract doc, chunk, and subchunk\"\"\"\n",
    "    \n",
    "    def extract_parts(name):\n",
    "        \"\"\"Extract the doc, chunk, and subchunk\"\"\"\n",
    "        \n",
    "        match = re.match(r'doc_(\\d+)_chunk_(\\d+)(?:_subchunk_(\\d+))?.jsonl', name)\n",
    "        if match:\n",
    "            doc = int(match.group(1))\n",
    "            chunk = int(match.group(2))\n",
    "            subchunk = int(match.group(3)) if match.group(3) else 0\n",
    "            return pd.Series([doc, chunk, subchunk])\n",
    "        else:\n",
    "            return pd.Series([None, None, None])\n",
    "\n",
    "    # Apply the function to the 'name' column\n",
    "    df[['doc', 'chunk', 'subchunk']] = df['name'].apply(extract_parts)\n",
    "\n",
    "    df = df.sort_values(by=['doc', 'chunk', 'subchunk'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_google_drive_jsonl(service, file_id):\n",
    "    \"\"\"Read the document with the designated file_id\"\"\"\n",
    "    \n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    \n",
    "    # Move the cursor to the beginning of the BytesIO object\n",
    "    fh.seek(0)\n",
    "    \n",
    "    # Read the file content into memory\n",
    "    jsonl_content = fh.read().decode('utf-8')\n",
    "    \n",
    "    # Split the content into individual JSON lines\n",
    "    json_lines = jsonl_content.strip().split('\\n')\n",
    "    \n",
    "    # Parse each line as a JSON object\n",
    "    json_objects = [json.loads(line) for line in json_lines]\n",
    "    \n",
    "    json_df = pd.DataFrame(json_objects)\n",
    "\n",
    "    return json_df\n",
    "\n",
    "def move_file_to_folder(service, file_id, folder_id):\n",
    "    \"\"\"Move specified file to the specified folder.\n",
    "        Args:\n",
    "            service: service client\n",
    "            file_id: Id of the file to move.\n",
    "            folder_id: Id of the folder\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the existing parents to remove\n",
    "        file = service.files().get(fileId=file_id, fields=\"parents\").execute()\n",
    "        previous_parents = \",\".join(file.get(\"parents\"))\n",
    "        # Move the file to the new folder\n",
    "        file = (\n",
    "            service.files()\n",
    "            .update(\n",
    "                fileId=file_id,\n",
    "                addParents=folder_id,\n",
    "                removeParents=previous_parents,\n",
    "                fields=\"id, parents\",\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "def upload_file(service, file_name, file_path, parent_folder_id=None):\n",
    "    \"\"\"Upload a file to Google Drive.\"\"\"\n",
    "    file_metadata = {\n",
    "        'name': file_name\n",
    "    }\n",
    "    if parent_folder_id:\n",
    "        file_metadata['parents'] = [parent_folder_id]\n",
    "\n",
    "    media = MediaFileUpload(file_path, resumable=True)\n",
    "\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id, name, parents'\n",
    "    ).execute()\n",
    "\n",
    "def delete_file(service, file_id):\n",
    "    \"\"\"Delete a file from Google Drive.\"\"\"\n",
    "    try:\n",
    "        service.files().delete(fileId=file_id).execute()\n",
    "        print(f\"    File with ID '{file_id}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    An error occurred: {e}\")\n",
    "\n",
    "def read_raw_df(service, df, error_folder_id):\n",
    "\n",
    "    df = df[df['file_extension'] == 'jsonl']\n",
    "    num_rows = df.shape[0]\n",
    "    columns = ['doc_id', 'chunk_id', 'subchunk_id', 'rephrased']\n",
    "    result_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for i in range(0, num_rows):\n",
    "        doc_id = df.iloc[i, 2]\n",
    "        doc_num = df.iloc[i, 7]\n",
    "        chunk_num = df.iloc[i, 8]\n",
    "        subchunk_num = df.iloc[i, 9]\n",
    "\n",
    "        print(df.iloc[i, 3])\n",
    "        try:\n",
    "            doc_data = read_google_drive_jsonl(service, doc_id)\n",
    "        \n",
    "            if 'subchunk_id' not in doc_data.columns:\n",
    "                # Get the location of 'chunk_id' and insert dummy subchunk\n",
    "                chunk_index = doc_data.columns.get_loc('chunk_id')\n",
    "                doc_data.insert(chunk_index + 1, 'subchunk_id', 0)\n",
    "    \n",
    "            # Filter the data to make sure we don't duplicate in iterations\n",
    "            filtered_data = doc_data[(doc_data['doc_id'] == doc_num) &\n",
    "                (doc_data['chunk_id'] == chunk_num) &\n",
    "                (doc_data['subchunk_id'] == subchunk_num)]\n",
    "\n",
    "            delete_file(service, doc_id)\n",
    "            \n",
    "        except:\n",
    "            print(\"Parsing Error\")\n",
    "            move_file_to_folder(service, doc_id, error_folder_id)\n",
    "\n",
    "        result_df = pd.concat([result_df, filtered_data], ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def update_files(main_folder_name, save_location, filename=\"rephrased.jsonl\"):\n",
    "    \"\"\"Update the files in the google drive with any new raw files.\n",
    "    \n",
    "        Args:\n",
    "            main_folder_name: The folder which contains the error, raw and processed folders\n",
    "            save_location: The local save location.\n",
    "            filename: The filename on Google Drive\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the google drive location\n",
    "    service = connect_to_drive()\n",
    "\n",
    "    # Get the files info from within the main folder\n",
    "    main_folder_id = search_file_by_name(service, main_folder_name)['id']\n",
    "    files_data = get_files_in_folder_recursive(service, main_folder_id, main_folder_name)\n",
    "    files_data = split_name_column(files_data)\n",
    "\n",
    "    # Get the folder data from within the file data\n",
    "    folders = files_data[files_data['is_folder'] == True]\n",
    "\n",
    "    # Split the data into the raw, error, and processed data\n",
    "    raw_id = folders[folders['name'] == 'raw'].iloc[0,2]\n",
    "    error_id = folders[folders['name'] == 'errors'].iloc[0,2]\n",
    "    processed_id = folders[folders['name'] == 'processed'].iloc[0,2]\n",
    "\n",
    "    # Return only the data in the raw folder\n",
    "    raw_files = files_data[(files_data['parent_folder_id'] == raw_id) &\n",
    "        (files_data['file_extension'] == 'jsonl')]\n",
    "\n",
    "    # Run the code to pull new data\n",
    "    old_data = read_jsonl_file(save_location)\n",
    "    new_data = read_raw_df(service, raw_files, error_id)\n",
    "    new_rows = new_data.shape[0]\n",
    "\n",
    "    # If we have new rows then save to local location and Google Drive\n",
    "    if new_rows > 0:\n",
    "        print(\"New Rows Added\")\n",
    "        result_df = pd.concat([old_data, new_data], ignore_index=True)\n",
    "    \n",
    "        save_as_jsonl(result_df, save_location)\n",
    "        upload_file(service, filename, save_location, parent_folder_id=processed_id)\n",
    "    else:\n",
    "        print(\"No New Data\")\n",
    "\n",
    "def delete_trash():\n",
    "    \"\"\"Delete all items in the trash permenantly\"\"\"\n",
    "    service = connect_to_drive()\n",
    "    service.files().emptyTrash().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1ff76-cb3b-4990-bd2e-7ed8a42b7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_files('guardian_phi', \"../data/guardian_phi/rephrased.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e33875-7bb2-4029-881f-9b4f8840bc86",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/guardian_phi_chunked/rephrased.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mupdate_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mguardian_phi_chunked\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/guardian_phi_chunked/rephrased.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 299\u001b[0m, in \u001b[0;36mupdate_files\u001b[1;34m(main_folder_name, save_location, filename)\u001b[0m\n\u001b[0;32m    295\u001b[0m raw_files \u001b[38;5;241m=\u001b[39m files_data[(files_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_folder_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m raw_id) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m    296\u001b[0m     (files_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_extension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjsonl\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# Run the code to pull new data\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m old_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_jsonl_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m new_data \u001b[38;5;241m=\u001b[39m read_raw_df(service, raw_files, error_id)\n\u001b[0;32m    301\u001b[0m new_rows \u001b[38;5;241m=\u001b[39m new_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\paraphrase_py\\code\\read_and_write_docs.py:7\u001b[0m, in \u001b[0;36mread_jsonl_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_jsonl_file\u001b[39m(file_path):\n\u001b[0;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/guardian_phi_chunked/rephrased.jsonl'"
     ]
    }
   ],
   "source": [
    "update_files('guardian_phi_chunked', \"../data/guardian_phi_chunked/rephrased.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
